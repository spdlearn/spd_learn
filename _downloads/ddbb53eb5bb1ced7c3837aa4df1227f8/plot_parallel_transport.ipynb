{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# SPD Learn Example\n# ==================\n#\n# First, install the required packages:\n\n!uv pip install -q spd_learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Parallel Transport on SPD Manifolds\n\nThis tutorial explains parallel transport, why it matters for domain\nadaptation, and how different Riemannian metrics affect transport behavior.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Parallel Transport?\n\nIn flat Euclidean space, we can move vectors freely - a vector at one\npoint is \"the same\" as a vector at another point. On curved manifolds\nlike the SPD manifold, this is **not** true.\n\nParallel transport moves a tangent vector along a *chosen curve* while keeping\nit \"as parallel as possible\" according to the manifold's geometry. The\ndefinition is inherently path-dependent on curved manifolds; in this tutorial\nwe use the (unique) AIRM geodesic between $P$ and $Q$ unless stated\notherwise. For textbook definitions and details, see *Optimization Algorithms\non Matrix Manifolds* and *An Introduction to Optimization on Smooth Manifolds*.\n:cite:p:`absil2008optimization,boumal2023intromanifolds`\nMathematically, if $\\gamma(t)$ is a curve on the manifold and\n$X(t)$ is a vector field along $\\gamma$, then $X$ is\nparallel if:\n\n\\begin{align}\\nabla_{\\dot{\\gamma}(t)} X = 0\\end{align}\n\nwhere $\\nabla$ is the Levi-Civita connection.\n\n**Key properties of parallel transport:**\n\n- **Linear**: $\\Gamma(aU + bV) = a\\Gamma(U) + b\\Gamma(V)$\n- **Isometry**: Preserves inner products $\\langle \\Gamma(U), \\Gamma(V) \\rangle_Q = \\langle U, V \\rangle_P$\n- **Invertible (same curve)**: Transport $P \\to Q \\to P$ recovers the original vector\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\n\nfrom spd_learn.functional import (\n    airm_distance,\n    parallel_transport_airm,\n    parallel_transport_lem,\n    parallel_transport_log_cholesky,\n    pole_ladder,\n    schild_ladder,\n    transport_tangent_vector,\n)\n\n\n# For reproducibility\ntorch.manual_seed(42)\n\n\ndef make_spd(n: int, batch_size: int | None = None) -> torch.Tensor:\n    \"\"\"Create a random SPD matrix.\"\"\"\n    if batch_size is None:\n        A = torch.randn(n, n)\n    else:\n        A = torch.randn(batch_size, n, n)\n    eye = torch.eye(n, device=A.device, dtype=A.dtype)\n    return A @ A.transpose(-2, -1) + eye\n\n\ndef make_symmetric(n: int, batch_size: int | None = None) -> torch.Tensor:\n    \"\"\"Create a random symmetric matrix (tangent vector).\"\"\"\n    if batch_size is None:\n        V = torch.randn(n, n)\n    else:\n        V = torch.randn(batch_size, n, n)\n    return (V + V.transpose(-2, -1)) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geometric Intuition\n\nTo understand parallel transport, imagine a tangent vector as an arrow\nattached to a point on a curved surface. As you slide the base of the\narrow along a path, the arrow rotates to stay \"parallel\" to itself\nrelative to the surface's curvature.\n\n```text\nTangent space at P          Tangent space at Q\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      V          \u2502         \u2502      \u0393(V)       \u2502\n\u2502     \u2197           \u2502  \u2500\u2500\u2500\u25ba   \u2502       \u2197         \u2502\n\u2502    P            \u2502 transport\u2502      Q          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                          \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               SPD Manifold\n              (curved space)\n```\n**Key insight**: On flat spaces (Euclidean), vectors don't change during\ntransport. On curved manifolds like SPD with AIRM, the vector *rotates*\nas it moves. LEM and Log-Cholesky flatten the manifold, so transport\nbecomes trivial (identity).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AIRM Parallel Transport\n\nUnder the Affine-Invariant Riemannian Metric (AIRM), parallel transport\nalong the AIRM geodesic has a closed-form solution. :cite:p:`pennec2006riemannian,bhatia2007positive`\n\n\\begin{align}\\Gamma_{P \\to Q}(V) = E V E^T\\end{align}\n\nwhere $E = (QP^{-1})^{1/2}$ is the **principal square root** of\n$QP^{-1}$.\n\nSince $QP^{-1}$ is generally non-symmetric, SPD Learn computes $E$\nusing a numerically stable equivalent formula that only involves symmetric\nmatrix square roots. :cite:p:`pennec2006riemannian`\n\n\\begin{align}E = Q^{1/2} (Q^{-1/2} P Q^{-1/2})^{-1/2} Q^{-1/2}\\end{align}\n\nThis uses only symmetric matrix square roots, which are well-defined\nand numerically stable for SPD matrices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create two SPD matrices (source and target points)\nn = 3\nP = make_spd(n)\nQ = make_spd(n)\n\n# Create a tangent vector at P\nV0 = make_symmetric(n)\n\n# Transport V from T_P to T_Q along the AIRM geodesic\nV_transported = parallel_transport_airm(V0, P, Q)\n\nprint(\"Source point P:\")\nprint(P)\nprint(\"\\nTarget point Q:\")\nprint(Q)\nprint(\"\\nOriginal tangent vector V at P:\")\nprint(V0)\nprint(\"\\nTransported tangent vector at Q:\")\nprint(V_transported)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inner Product Preservation (Isometry)\n\nThe AIRM inner product on tangent vectors is: :cite:p:`pennec2006riemannian`\n\n\\begin{align}\\langle U, V \\rangle_P = \\text{tr}(P^{-1} U P^{-1} V)\\end{align}\n\nParallel transport preserves this inner product:\n$\\langle \\Gamma(U), \\Gamma(V) \\rangle_Q = \\langle U, V \\rangle_P$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def airm_inner_product(U, V, P):\n    \"\"\"Compute the AIRM inner product at P (stable solve for SPD matrices).\"\"\"\n    chol = torch.linalg.cholesky(P)\n    p_inv_u = torch.cholesky_solve(U, chol)\n    p_inv_v = torch.cholesky_solve(V, chol)\n    return torch.trace(p_inv_u @ p_inv_v)\n\n\n# Create two tangent vectors\nU = make_symmetric(n)\nV_second = make_symmetric(n)\n\n# Transport both vectors\nU_transported = parallel_transport_airm(U, P, Q)\nV_second_transported = parallel_transport_airm(V_second, P, Q)\n\n# Compute inner products before and after transport\ninner_before = airm_inner_product(U, V_second, P)\ninner_after = airm_inner_product(U_transported, V_second_transported, Q)\n\nprint(f\"Inner product at P: {inner_before:.6f}\")\nprint(f\"Inner product at Q: {inner_after:.6f}\")\nprint(f\"Difference: {abs(inner_before - inner_after):.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Roundtrip Consistency\n\nTransport $P \\to Q \\to P$ along the *same geodesic* (reversed) should\nrecover the original vector. This is a fundamental property of parallel\ntransport along a fixed curve. :cite:p:`absil2008optimization`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Transport V0 from P to Q\nV_at_Q = parallel_transport_airm(V0, P, Q)\n\n# Transport back from Q to P\nV_recovered = parallel_transport_airm(V_at_Q, Q, P)\n\n# Check that we recovered the original vector\nprint(\"Original V:\")\nprint(V0)\nprint(\"\\nRecovered V after roundtrip:\")\nprint(V_recovered)\nprint(f\"\\nReconstruction error: {torch.norm(V0 - V_recovered):.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why LEM and Log-Cholesky Have Identity Transport\n\nUnder the Log-Euclidean Metric (LEM), the SPD manifold becomes **flat**\nvia the matrix logarithm diffeomorphism. In that log-domain (flat space),\nparallel transport is the identity under the canonical identification of\ntangent spaces. :cite:p:`arsigny2007geometric`\n\n\\begin{align}\\Gamma_{P \\to Q}^{LEM}(V) = V\\end{align}\n\nThe same applies to the Log-Cholesky metric, which uses the Cholesky\ndecomposition to create a flat geometry in the Cholesky-log coordinates.\n:cite:p:`lin2019riemannian`\n\nThis is computationally efficient (O(1) transport) but means these\nmetrics don't capture the same geometric structure as AIRM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# LEM transport is identity\nV_lem = parallel_transport_lem(V0, P, Q)\nprint(\"LEM transport:\")\nprint(f\"  Original V == Transported V: {torch.allclose(V0, V_lem)}\")\n\n# Log-Cholesky transport is also identity\nV_chol = parallel_transport_log_cholesky(V0, P, Q)\nprint(\"Log-Cholesky transport:\")\nprint(f\"  Original V == Transported V: {torch.allclose(V0, V_chol)}\")\n\n# Compare with AIRM (non-trivial transport)\nV_airm = parallel_transport_airm(V0, P, Q)\nprint(\"AIRM transport:\")\nprint(f\"  Original V == Transported V: {torch.allclose(V0, V_airm)}\")\nprint(f\"  Transport difference norm: {torch.norm(V0 - V_airm):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Transport Methods\n\nSPD Learn provides several transport methods with different trade-offs:\n\n+----------------+------------------+-----------+---------------------------+\n| Method         | Formula          | Complexity| Notes                     |\n+================+==================+===========+===========================+\n| AIRM           | $EVE^T$    | O(n\u00b3)     | Exact, preserves geometry |\n+----------------+------------------+-----------+---------------------------+\n| LEM            | $V$        | O(1)      | Identity (flat geometry)  |\n+----------------+------------------+-----------+---------------------------+\n| Log-Cholesky   | $V$        | O(1)      | Identity (flat geometry)  |\n+----------------+------------------+-----------+---------------------------+\n| Schild's ladder| Iterative        | O(k\u00b7n\u00b3)   | ~O(1/k\u00b2) (small steps)    |\n+----------------+------------------+-----------+---------------------------+\n| Pole ladder    | Single iteration | O(n\u00b3)     | O(h\u00b2) (small distance)    |\n+----------------+------------------+-----------+---------------------------+\n\nThe ``transport_tangent_vector`` function provides a unified interface:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Transport using different metrics\nV_airm = transport_tangent_vector(V0, P, Q, metric=\"airm\")\nV_lem = transport_tangent_vector(V0, P, Q, metric=\"lem\")\nV_chol = transport_tangent_vector(V0, P, Q, metric=\"log_cholesky\")\n\nprint(\"Transport results by metric:\")\nprint(f\"  AIRM vs LEM difference: {torch.norm(V_airm - V_lem):.4f}\")\nprint(f\"  LEM vs Log-Cholesky difference: {torch.norm(V_lem - V_chol):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Approximations: Schild's and Pole Ladder\n\nWhen closed-form transport is unavailable, numerical methods approximate\ntransport using geodesics:\n\n**Schild's Ladder**: Iterative parallelogram construction along the geodesic.\nEach step uses geodesic midpoints to approximate parallel translation.\nFor sufficiently small step sizes, the approximation error scales like\nO(1/k\u00b2) in the number of steps. :cite:p:`lorenzi2014efficient`\n\n**Pole Ladder**: A more efficient variant using a single reflection through\nthe geodesic midpoint. For small geodesic distances, the local error is\nO(h\u00b2) where h is the distance between P and Q. :cite:p:`lorenzi2014efficient`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compare Schild's ladder with different step counts\nV_schild_5 = schild_ladder(V0, P, Q, n_steps=5)\nV_schild_10 = schild_ladder(V0, P, Q, n_steps=10)\nV_schild_20 = schild_ladder(V0, P, Q, n_steps=20)\n\n# Pole ladder (single step)\nV_pole = pole_ladder(V0, P, Q)\n\n# Compare to exact AIRM transport\nV_exact = parallel_transport_airm(V0, P, Q)\n\nprint(\"Approximation errors (compared to exact AIRM):\")\nprint(f\"  Schild's ladder (5 steps):  {torch.norm(V_exact - V_schild_5):.6f}\")\nprint(f\"  Schild's ladder (10 steps): {torch.norm(V_exact - V_schild_10):.6f}\")\nprint(f\"  Schild's ladder (20 steps): {torch.norm(V_exact - V_schild_20):.6f}\")\nprint(f\"  Pole ladder:                {torch.norm(V_exact - V_pole):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Convergence of Schild's Ladder\n\nLet's see how Schild's ladder converges to the exact solution as we\nincrease the number of steps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "steps = [1, 2, 5, 10, 20, 50, 100]\nerrors = []\n\nfor n_steps in steps:\n    V_approx = schild_ladder(V0, P, Q, n_steps=n_steps)\n    error = torch.norm(V_exact - V_approx).item()\n    errors.append(error)\n\nplt.figure(figsize=(8, 5))\nplt.loglog(steps, errors, \"o-\", linewidth=2, markersize=8)\nplt.xlabel(\"Number of Steps\", fontsize=12)\nplt.ylabel(\"Approximation Error\", fontsize=12)\nplt.title(\"Schild's Ladder Convergence\", fontsize=14)\nplt.grid(True, alpha=0.3)\n\n# Add reference line for O(1/k\u00b2) convergence (anchored at the largest k)\nreference = [errors[-1] * (steps[-1] / s) ** 2 for s in steps]\nplt.loglog(steps, reference, \"--\", alpha=0.5, label=r\"$O(1/k^2)$ reference\")\nplt.legend()\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pole Ladder Error vs. Geodesic Distance\n\nPole ladder's error depends on the geodesic distance between P and Q.\nFor nearby points, it's very accurate; for distant points, error grows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "distances = []\npole_errors = []\n\n# Generate pairs with varying distances\nfor scale in [0.1, 0.3, 0.5, 1.0, 2.0, 3.0]:\n    P_test = make_spd(n)\n    # Create Q at controlled distance from P\n    direction = make_symmetric(n)\n    direction = direction / torch.norm(direction) * scale\n    Q_test = P_test @ torch.linalg.matrix_exp(torch.linalg.solve(P_test, direction))\n\n    V_test = make_symmetric(n)\n    V_exact_test = parallel_transport_airm(V_test, P_test, Q_test)\n    V_pole_test = pole_ladder(V_test, P_test, Q_test)\n\n    dist = airm_distance(P_test, Q_test).item()\n    err = torch.norm(V_exact_test - V_pole_test).item()\n    distances.append(dist)\n    pole_errors.append(err)\n\nplt.figure(figsize=(8, 5))\nplt.plot(distances, pole_errors, \"s-\", linewidth=2, markersize=8, color=\"orange\")\nplt.xlabel(\"Geodesic Distance (AIRM)\", fontsize=12)\nplt.ylabel(\"Pole Ladder Error\", fontsize=12)\nplt.title(\"Pole Ladder Accuracy vs. Distance\", fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Pole ladder error increases with geodesic distance.\")\nprint(\"Use Schild's ladder (more steps) for distant points.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Timing Comparison\n\nDifferent transport methods have different computational costs.\nHere we compare wall-clock time for a realistic matrix size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\n\nn_timing = 22  # Typical EEG channel count\nn_trials = 50\n\nP_time = make_spd(n_timing)\nQ_time = make_spd(n_timing)\nV_time = make_symmetric(n_timing)\n\n\ndef benchmark(func, *args, n_runs=n_trials):\n    \"\"\"Benchmark a function and return mean time in milliseconds.\"\"\"\n    # Warmup\n    for _ in range(3):\n        func(*args)\n    # Timed runs\n    start = time.perf_counter()\n    for _ in range(n_runs):\n        func(*args)\n    elapsed = time.perf_counter() - start\n    return (elapsed / n_runs) * 1000  # Convert to ms\n\n\ntime_airm = benchmark(parallel_transport_airm, V_time, P_time, Q_time)\ntime_lem = benchmark(parallel_transport_lem, V_time, P_time, Q_time)\ntime_schild_10 = benchmark(schild_ladder, V_time, P_time, Q_time, n_trials)\ntime_pole = benchmark(pole_ladder, V_time, P_time, Q_time)\n\nprint(f\"\\nTiming comparison ({n_timing}x{n_timing} matrices, {n_trials} runs):\")\nprint(f\"  AIRM (exact):        {time_airm:.3f} ms\")\nprint(f\"  LEM (identity):      {time_lem:.3f} ms\")\nprint(f\"  Schild's (10 steps): {time_schild_10:.3f} ms\")\nprint(f\"  Pole ladder:         {time_pole:.3f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Application: Cross-Subject EEG Transfer\n\nIn Brain-Computer Interface (BCI) applications, different subjects have\ndifferent \"reference\" covariance matrices due to anatomical and\nphysiological differences. Parallel transport enables aligning tangent\nvectors from different subjects to a common reference, which is\nessential for cross-subject transfer learning.\n\n**Scenario**: Subject A has labeled training data, Subject B has no labels.\nWe want to use Subject A's classifier on Subject B's data.\n\n.. seealso::\n\n   `sphx_glr_auto_examples_plot_tsmnet_domain_adaptation.py` for a\n   complete domain adaptation example using batch normalization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Realistic EEG scenario: 22 channels (typical motor imagery setup)\nn_channels = 22\nn_trials_per_class = 30\n\n# Simulate Subject A's data (source domain)\n# Reference = geometric mean of their covariance matrices\nR_A = make_spd(n_channels)\n\n# Simulate Subject B's data (target domain) - different reference\nR_B = make_spd(n_channels)\n\n# Subject A's tangent vectors for two classes (e.g., left vs right hand)\n# In practice, these come from log_map(covariances, R_A)\nclass_1_A = [make_symmetric(n_channels) * 0.5 for _ in range(n_trials_per_class)]\nclass_2_A = [\n    make_symmetric(n_channels) * 0.5 + 0.1 * torch.eye(n_channels)\n    for _ in range(n_trials_per_class)\n]\n\n# Transport Subject A's tangent vectors to Subject B's reference\nclass_1_transported = [parallel_transport_airm(v, R_A, R_B) for v in class_1_A]\nclass_2_transported = [parallel_transport_airm(v, R_A, R_B) for v in class_2_A]\n\n# Compute class separability (simplified: distance between class means)\nmean_class_1_orig = torch.stack(class_1_A).mean(dim=0)\nmean_class_2_orig = torch.stack(class_2_A).mean(dim=0)\n\nmean_class_1_trans = torch.stack(class_1_transported).mean(dim=0)\nmean_class_2_trans = torch.stack(class_2_transported).mean(dim=0)\n\n# Parallel transport preserves the AIRM inner product, not Euclidean norm\n# The key point: relative geometry between vectors is preserved\ninner_orig = airm_inner_product(\n    mean_class_1_orig - mean_class_2_orig, mean_class_1_orig - mean_class_2_orig, R_A\n)\ninner_trans = airm_inner_product(\n    mean_class_1_trans - mean_class_2_trans,\n    mean_class_1_trans - mean_class_2_trans,\n    R_B,\n)\n\nprint(f\"Cross-subject EEG transfer ({n_channels} channels):\")\nprint(f\"  AIRM distance between subjects: {airm_distance(R_A, R_B):.4f}\")\nprint(f\"  AIRM inner product (original):    {inner_orig:.4f}\")\nprint(f\"  AIRM inner product (transported): {inner_trans:.4f}\")\nprint(f\"  Geometry preserved: {torch.isclose(inner_orig, inner_trans, rtol=1e-4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel Transport vs. Batch Normalization\n\nBoth parallel transport and SPD batch normalization address distribution\nshift, but they work differently:\n\n- **Parallel transport**: Moves tangent vectors between reference points\n  on the manifold. Use when combining tangent vectors from different\n  reference points.\n\n- **SPDBatchNormMeanVar**: Normalizes SPD matrices to a common scale by\n  centering around the geometric mean and scaling the variance. Use when\n  aligning statistical properties of SPD matrices directly.\n\nThe choice depends on your pipeline:\n\n- If you work in tangent space: use parallel transport\n- If you work with SPD matrices directly: use batch normalization\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing the Right Method\n\nHere's a decision guide for selecting the appropriate transport method:\n\n1. **Need affine invariance?** \u2192 Use AIRM transport\n\n2. **Speed critical?** \u2192 Use LEM (identity transport, O(1))\n\n3. **No closed-form available?** \u2192 Use pole ladder for small distances\n\n4. **High accuracy needed?** \u2192 Use Schild's ladder with many steps\n\n5. **Need gradients through reference points?** \u2192 Use functional AIRM\n   transport (``parallel_transport_airm``)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Gradient flow demonstration\nP_grad = make_spd(n)\nQ_grad = make_spd(n)\nV_grad = make_symmetric(n)\n\nP_grad.requires_grad_(True)\nQ_grad.requires_grad_(True)\nV_grad.requires_grad_(True)\n\n# Transport with gradient tracking\nV_out = parallel_transport_airm(V_grad, P_grad, Q_grad)\nloss = V_out.sum()\nloss.backward()\n\nprint(\"Gradient flow through parallel transport:\")\nprint(f\"  grad_V exists: {V_grad.grad is not None}\")\nprint(f\"  grad_P exists: {P_grad.grad is not None}\")\nprint(f\"  grad_Q exists: {Q_grad.grad is not None}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Pitfalls and Numerical Stability\n\nParallel transport can encounter numerical issues in certain scenarios.\nHere's how to handle them:\n\n**1. Ill-conditioned matrices (near-singular)**\n\nWhen P or Q have very small eigenvalues, matrix inversions become unstable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Example: ill-conditioned matrix\nP_illcond = torch.diag(torch.tensor([1.0, 1.0, 1e-8]))\nQ_good = make_spd(3)\nV_test = make_symmetric(3)\n\n# Check condition number\ncond_P = torch.linalg.cond(P_illcond).item()\nprint(\"\\nNumerical stability example:\")\nprint(f\"  Condition number of P: {cond_P:.2e}\")\nprint(\"  (Values > 1e10 may cause issues)\")\n\n# Solution: regularize by adding small diagonal\nepsilon = 1e-6\nP_regularized = P_illcond + epsilon * torch.eye(3)\ncond_reg = torch.linalg.cond(P_regularized).item()\nprint(f\"  After regularization: {cond_reg:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Large geodesic distances**\n\nWhen P and Q are very far apart on the manifold, numerical errors accumulate.\nUse higher precision (float64) or Schild's ladder with more steps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "P_f64 = make_spd(n).double()\nQ_f64 = make_spd(n).double()\nV_f64 = make_symmetric(n).double()\n\nV_transported_f64 = parallel_transport_airm(V_f64, P_f64, Q_f64)\nprint(\"\\nUsing float64 for better precision:\")\nprint(f\"  Input dtype: {V_f64.dtype}\")\nprint(f\"  Output dtype: {V_transported_f64.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Asymmetry in transported vectors**\n\nDue to floating-point errors, transported vectors may become slightly\nasymmetric. Re-symmetrize if needed for downstream operations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "V_transported_check = parallel_transport_airm(V0, P, Q)\nasymmetry = torch.norm(V_transported_check - V_transported_check.T).item()\nprint(\"\\nAsymmetry check:\")\nprint(f\"  Asymmetry norm: {asymmetry:.2e}\")\n\n# Re-symmetrize if needed\nV_resym = (V_transported_check + V_transported_check.T) / 2\nprint(f\"  After re-symmetrization: {torch.norm(V_resym - V_resym.T):.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this tutorial, we covered:\n\n- **Parallel transport** moves tangent vectors while preserving geometry\n- **AIRM** has non-trivial transport ($EVE^T$); LEM/Log-Cholesky\n  have identity transport due to flat geometry\n- **Numerical methods** (Schild's and pole ladder) approximate transport\n  when closed-form solutions are unavailable or expensive\n- **Cross-subject transfer** is a key application for BCI domain adaptation\n- Choose your method based on accuracy, speed, and invariance requirements\n\n## See Also\n\n**Functions:**\n\n- :func:`spd_learn.functional.parallel_transport_airm`\n- :func:`spd_learn.functional.parallel_transport_lem`\n- :func:`spd_learn.functional.schild_ladder`\n- :func:`spd_learn.functional.pole_ladder`\n- :func:`spd_learn.functional.transport_tangent_vector`\n\n**Related tutorials and examples:**\n\n- `tutorial-spd-concepts` - Foundation concepts for SPD manifolds\n- `tutorial-eeg-classification` - End-to-end EEG classification\n- `sphx_glr_auto_examples_plot_tsmnet_domain_adaptation.py` -\n  Domain adaptation using batch normalization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cleanup\nimport matplotlib.pyplot as plt\nplt.close('all')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}