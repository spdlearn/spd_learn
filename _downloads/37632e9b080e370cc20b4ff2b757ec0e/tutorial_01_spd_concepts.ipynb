{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# SPD Learn Example\n# ==================\n#\n# First, install the required packages:\n\n!uv pip install -q spd_learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Understanding Symmetric Positive Definite (SPD) Matrices\n\nThis tutorial provides a comprehensive introduction to Symmetric Positive\nDefinite (SPD) matrices and explains why Riemannian geometry is essential\nfor working with them in machine learning.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are SPD Matrices?\n\nA Symmetric Positive Definite (SPD) matrix is a square matrix $S$\nthat satisfies two properties:\n\n1. **Symmetry**: $S = S^T$\n2. **Positive Definiteness**: $x^T S x > 0$ for all non-zero\n   vectors $x$\n\nEquivalently, an SPD matrix has all positive eigenvalues.\n\nSPD matrices appear naturally in many applications:\n\n- **Covariance matrices** in statistics and signal processing\n- **Diffusion tensors** in medical imaging (DTI)\n- **Kernels** in machine learning (Gram matrices)\n- **Inertia tensors** in physics\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom spd_learn.functional import (\n    log_euclidean_distance,\n    log_euclidean_mean,\n    matrix_exp,\n    matrix_log,\n)\n\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating SPD Matrices\n\nThere are several ways to create SPD matrices. The most common is\nthrough a product $A A^T$ where $A$ has full rank.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_spd_matrix(n, eigenvalues=None, dtype=torch.float64):\n    \"\"\"Create an SPD matrix with given eigenvalues.\n\n    Parameters\n    ----------\n    n : int\n        Matrix dimension.\n    eigenvalues : array-like, optional\n        Desired eigenvalues. If None, random positive values are used.\n    dtype : torch.dtype\n        Data type for the tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        An n x n SPD matrix.\n    \"\"\"\n    # Random orthogonal matrix via QR decomposition\n    Q, _ = torch.linalg.qr(torch.randn(n, n, dtype=dtype))\n\n    if eigenvalues is None:\n        eigenvalues = torch.abs(torch.randn(n, dtype=dtype)) + 0.5  # Ensure positive\n    else:\n        eigenvalues = torch.tensor(eigenvalues, dtype=dtype)\n\n    return Q @ torch.diag(eigenvalues) @ Q.T\n\n\n# Create a simple 2x2 SPD matrix\nA = create_spd_matrix(2, eigenvalues=[2.0, 0.5])\nprint(\"SPD Matrix A:\")\nprint(A.numpy())\nprint(f\"\\nEigenvalues: {torch.linalg.eigvalsh(A).numpy()}\")\nprint(f\"Is symmetric: {torch.allclose(A, A.T)}\")\nprint(f\"All eigenvalues positive: {torch.all(torch.linalg.eigvalsh(A) > 0).item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing 2x2 SPD Matrices as Ellipses\n\nA 2x2 SPD matrix can be visualized as an ellipse. The matrix\n$S$ defines an ellipse as the set of points satisfying:\n\n\\begin{align}\\{x : x^T S^{-1} x = 1\\}\\end{align}\n\nThe eigenvectors give the principal axes, and the square roots of\neigenvalues give the semi-axis lengths.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def spd_to_ellipse(spd_matrix, n_points=100):\n    \"\"\"Convert 2x2 SPD matrix to ellipse coordinates.\"\"\"\n    eigvals, eigvecs = np.linalg.eigh(spd_matrix)\n\n    # Parametric ellipse\n    theta = np.linspace(0, 2 * np.pi, n_points)\n    circle = np.array([np.cos(theta), np.sin(theta)])\n\n    # Transform circle to ellipse\n    transform = eigvecs @ np.diag(np.sqrt(eigvals))\n    ellipse = transform @ circle\n\n    return ellipse[0], ellipse[1]\n\n\n# Create two SPD matrices with different properties\nS1 = create_spd_matrix(2, eigenvalues=[3.0, 1.0])\nS2 = create_spd_matrix(2, eigenvalues=[2.0, 2.0])\nS3 = create_spd_matrix(2, eigenvalues=[4.0, 0.5])\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\nfor ax, S, title in zip(\n    axes,\n    [S1, S2, S3],\n    [\"Anisotropic (3, 1)\", \"Isotropic (2, 2)\", \"Highly Anisotropic (4, 0.5)\"],\n):\n    # Convert to numpy for plotting\n    S_np = S.numpy()\n    x, y = spd_to_ellipse(S_np)\n    ax.fill(x, y, alpha=0.3, color=\"blue\")\n    ax.plot(x, y, \"b-\", linewidth=2)\n\n    # Draw eigenvalue axes\n    eigvals, eigvecs = np.linalg.eigh(S_np)\n    for i in range(2):\n        vec = eigvecs[:, i] * np.sqrt(eigvals[i])\n        ax.arrow(0, 0, vec[0], vec[1], head_width=0.1, color=\"red\", linewidth=2)\n\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-3, 3)\n    ax.set_aspect(\"equal\")\n    ax.grid(True, alpha=0.3)\n    ax.set_title(title, fontsize=11)\n    ax.axhline(y=0, color=\"k\", linewidth=0.5)\n    ax.axvline(x=0, color=\"k\", linewidth=0.5)\n\nplt.suptitle(\"SPD Matrices Visualized as Ellipses\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why SPD Matrices Do Not Form a Vector Space\n\nA critical insight is that SPD matrices do NOT form a vector space.\nThe set of SPD matrices, denoted $\\mathcal{S}_{++}^n$, is not\nclosed under standard linear operations:\n\n1. **Scalar multiplication**: $-1 \\cdot S$ is not SPD\n2. **Addition boundary**: Sum of SPD matrices is SPD, but differences\n   may not be\n\nMore importantly, the **Euclidean mean** of SPD matrices exhibits\nproblematic behavior known as the \"swelling effect.\"\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Swelling Effect\n\nWhen we compute the arithmetic (Euclidean) mean of SPD matrices,\nthe determinant of the mean is often larger than the geometric mean\nof the individual determinants. This violates the intuition that an\n\"average\" should be \"in the middle.\"\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def euclidean_mean(matrices):\n    \"\"\"Compute the arithmetic mean of matrices.\"\"\"\n    return torch.mean(torch.stack(matrices), dim=0)\n\n\ndef geometric_mean_det(matrices):\n    \"\"\"Compute geometric mean of determinants.\"\"\"\n    dets = torch.stack([torch.linalg.det(m) for m in matrices])\n    return torch.exp(torch.mean(torch.log(dets)))\n\n\n# Create two very different SPD matrices\nA = torch.tensor(\n    [[4.0, 0.0], [0.0, 0.25]], dtype=torch.float64\n)  # Large in x, small in y\nB = torch.tensor(\n    [[0.25, 0.0], [0.0, 4.0]], dtype=torch.float64\n)  # Small in x, large in y\n\n# Compute means\nmean_euclidean = euclidean_mean([A, B])\ndet_A = torch.linalg.det(A)\ndet_B = torch.linalg.det(B)\ndet_mean = torch.linalg.det(mean_euclidean)\ngeo_mean_det = geometric_mean_det([A, B])\n\nprint(\"Matrix A:\")\nprint(A.numpy())\nprint(f\"det(A) = {det_A.item():.3f}\")\nprint(\"\\nMatrix B:\")\nprint(B.numpy())\nprint(f\"det(B) = {det_B.item():.3f}\")\nprint(\"\\nEuclidean Mean:\")\nprint(mean_euclidean.numpy())\nprint(f\"det(Mean) = {det_mean.item():.3f}\")\nprint(f\"\\nGeometric mean of determinants: {geo_mean_det.item():.3f}\")\nprint(\n    f\"\\nSwelling ratio: det(Mean) / geo_mean = {(det_mean / geo_mean_det).item():.3f}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Swelling Effect\n\nThe Euclidean mean produces an ellipse that is \"swollen\" - its area\n(proportional to det) is larger than expected.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n\n# Convert to numpy for plotting\nA_np, B_np = A.numpy(), B.numpy()\nmean_euclidean_np = mean_euclidean.numpy()\n\n# Plot original matrices\nx_A, y_A = spd_to_ellipse(A_np)\nx_B, y_B = spd_to_ellipse(B_np)\nx_mean, y_mean = spd_to_ellipse(mean_euclidean_np)\n\nax.fill(x_A, y_A, alpha=0.3, color=\"blue\", label=f\"A (det={det_A.item():.2f})\")\nax.plot(x_A, y_A, \"b-\", linewidth=2)\n\nax.fill(x_B, y_B, alpha=0.3, color=\"green\", label=f\"B (det={det_B.item():.2f})\")\nax.plot(x_B, y_B, \"g-\", linewidth=2)\n\nax.fill(\n    x_mean,\n    y_mean,\n    alpha=0.3,\n    color=\"red\",\n    label=f\"Euclidean Mean (det={det_mean.item():.2f})\",\n)\nax.plot(x_mean, y_mean, \"r-\", linewidth=2)\n\n# Reference: what the \"ideal\" mean should look like (same determinant as geometric mean)\nideal_scale = np.sqrt(geo_mean_det.item() / det_mean.item())\nax.plot(\n    x_mean * ideal_scale,\n    y_mean * ideal_scale,\n    \"k--\",\n    linewidth=2,\n    label=f\"Expected size (det={geo_mean_det.item():.2f})\",\n)\n\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.set_aspect(\"equal\")\nax.grid(True, alpha=0.3)\nax.legend(loc=\"upper right\", fontsize=10)\nax.set_title(\n    \"The Swelling Effect: Euclidean Mean is Too Large\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The SPD Manifold: A Cone Structure\n\nThe set of SPD matrices forms an open convex cone in the space of\nsymmetric matrices. We can visualize this for 2x2 matrices:\n\nA 2x2 symmetric matrix has 3 unique elements: $[a, b, c]$ where\n\n\\begin{align}S = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}\\end{align}\n\nThe SPD condition requires:\n\n- $a > 0$ (first diagonal positive)\n- $c > 0$ (second diagonal positive)\n- $ac - b^2 > 0$ (positive determinant)\n\nThis defines a cone in 3D space.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def is_spd_2x2(a, b, c):\n    \"\"\"Check if 2x2 symmetric matrix [[a,b],[b,c]] is SPD.\"\"\"\n    return (a > 0) & (c > 0) & (a * c - b**2 > 0)\n\n\n# Create a grid\na_vals = np.linspace(0.1, 3, 30)\nb_vals = np.linspace(-2, 2, 30)\nc_vals = np.linspace(0.1, 3, 30)\n\n# Visualize the cone boundary (det = 0 surface)\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Create mesh for b vs sqrt(ac)\nA_mesh, C_mesh = np.meshgrid(a_vals, c_vals)\n# Boundary: b^2 = ac, so b = +/- sqrt(ac)\nB_boundary = np.sqrt(A_mesh * C_mesh)\n\n# Plot the cone boundary\nax.plot_surface(\n    A_mesh, B_boundary, C_mesh, alpha=0.3, color=\"blue\", label=\"Boundary (det=0)\"\n)\nax.plot_surface(A_mesh, -B_boundary, C_mesh, alpha=0.3, color=\"blue\")\n\n# Plot some SPD points inside the cone\nn_points = 50\nspd_points = []\nfor _ in range(n_points):\n    a = np.random.uniform(0.5, 2.5)\n    c = np.random.uniform(0.5, 2.5)\n    b_max = np.sqrt(a * c) * 0.9  # Inside the cone\n    b = np.random.uniform(-b_max, b_max)\n    spd_points.append([a, b, c])\n\nspd_points = np.array(spd_points)\nax.scatter(\n    spd_points[:, 0],\n    spd_points[:, 1],\n    spd_points[:, 2],\n    c=\"red\",\n    s=20,\n    label=\"SPD matrices\",\n)\n\nax.set_xlabel(\"a (diagonal)\", fontsize=10)\nax.set_ylabel(\"b (off-diagonal)\", fontsize=10)\nax.set_zlabel(\"c (diagonal)\", fontsize=10)\nax.set_title(\n    \"SPD Manifold as a Cone in 3D\\n(2x2 symmetric matrices)\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Riemannian Geometry?\n\nSince SPD matrices do not form a vector space, we need a different\nframework for computing distances and means. **Riemannian geometry**\n:cite:p:`pennec2006riemannian` provides this framework by:\n\n1. Defining a **metric** (inner product) at each point on the manifold\n2. Using **geodesics** (shortest paths) instead of straight lines\n3. Computing **exponential/logarithmic maps** to move between the\n   manifold and tangent spaces\n\nThe most common metrics for SPD matrices are:\n\n- **Affine Invariant Riemannian Metric (AIRM)**\n- **Log-Euclidean Metric**\n\nWe focus on the Log-Euclidean metric as it is computationally efficient.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix Logarithm and Exponential\n\nThe **matrix logarithm** maps an SPD matrix to a symmetric matrix\n(the tangent space at the identity). For SPD matrix $S$ with\neigendecomposition $S = U \\Lambda U^T$:\n\n\\begin{align}\\log(S) = U \\log(\\Lambda) U^T\\end{align}\n\nThe **matrix exponential** is the inverse operation:\n\n\\begin{align}\\exp(X) = U \\exp(\\Lambda) U^T\\end{align}\n\nThese operations are key to the Log-Euclidean framework.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create an SPD matrix\nS = torch.tensor([[2.0, 0.5], [0.5, 1.0]], dtype=torch.float64)\n\nprint(\"Original SPD Matrix S:\")\nprint(S.numpy())\nprint(f\"Eigenvalues: {torch.linalg.eigvalsh(S).numpy()}\")\n\n# Compute matrix logarithm\nlog_S = matrix_log.apply(S)\nprint(\"\\nMatrix Logarithm log(S):\")\nprint(log_S.numpy())\nprint(f\"Eigenvalues of log(S): {torch.linalg.eigvalsh(log_S).numpy()}\")\n\n# Verify: exp(log(S)) = S\nrecovered_S = matrix_exp.apply(log_S)\nprint(\"\\nRecovered S = exp(log(S)):\")\nprint(recovered_S.numpy())\nprint(f\"Reconstruction error: {torch.norm(recovered_S - S).item():.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log-Euclidean Distance\n\nThe **Log-Euclidean distance** :cite:p:`arsigny2007geometric` is defined as\nthe Frobenius norm of\nthe difference of matrix logarithms:\n\n\\begin{align}d_{LE}(A, B) = \\|\\log(A) - \\log(B)\\|_F\\end{align}\n\nThis distance respects the manifold structure better than the\nEuclidean distance $\\|A - B\\|_F$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create two SPD matrices\nA = torch.tensor([[2.0, 0.3], [0.3, 1.0]], dtype=torch.float64)\nB = torch.tensor([[1.5, -0.2], [-0.2, 0.8]], dtype=torch.float64)\n\n# Euclidean distance\ndist_euclidean = torch.norm(A - B, p=\"fro\")\n\n# Log-Euclidean distance\ndist_log_euclidean = log_euclidean_distance(A, B)\n\nprint(\"Matrix A:\")\nprint(A.numpy())\nprint(\"\\nMatrix B:\")\nprint(B.numpy())\nprint(f\"\\nEuclidean distance: {dist_euclidean.item():.4f}\")\nprint(f\"Log-Euclidean distance: {dist_log_euclidean.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log-Euclidean Mean\n\nThe **Log-Euclidean mean** is computed by:\n\n1. Take the matrix logarithm of each SPD matrix\n2. Compute the arithmetic mean in log-space\n3. Apply the matrix exponential to return to SPD\n\n\\begin{align}\\bar{S}_{LE} = \\exp\\left(\\frac{1}{n}\\sum_{i=1}^n \\log(S_i)\\right)\\end{align}\n\nThis mean does not suffer from the swelling effect!\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create batch of SPD matrices\nS1 = torch.tensor([[4.0, 0.0], [0.0, 0.25]], dtype=torch.float64)\nS2 = torch.tensor([[0.25, 0.0], [0.0, 4.0]], dtype=torch.float64)\n\n# Euclidean mean\nmean_euclidean = (S1 + S2) / 2\n\n# Log-Euclidean mean using weighted mean with equal weights\n# Stack matrices and create uniform weights\nS_stack = torch.stack([S1, S2], dim=0)  # (2, 2, 2)\nweights = torch.tensor([[0.5, 0.5]], dtype=torch.float64)  # (1, 2)\nmean_log_euclidean = log_euclidean_mean(weights, S_stack).squeeze(0)\n\nprint(\"SPD Matrix S1:\")\nprint(S1.numpy())\nprint(f\"det(S1) = {torch.linalg.det(S1).item():.3f}\")\n\nprint(\"\\nSPD Matrix S2:\")\nprint(S2.numpy())\nprint(f\"det(S2) = {torch.linalg.det(S2).item():.3f}\")\n\nprint(\"\\nEuclidean Mean:\")\nprint(mean_euclidean.numpy())\nprint(f\"det(Euclidean Mean) = {torch.linalg.det(mean_euclidean).item():.3f}\")\n\nprint(\"\\nLog-Euclidean Mean:\")\nprint(mean_log_euclidean.numpy())\nprint(f\"det(Log-Euclidean Mean) = {torch.linalg.det(mean_log_euclidean).item():.3f}\")\n\ngeo_mean_det = np.sqrt(torch.linalg.det(S1).item() * torch.linalg.det(S2).item())\nprint(f\"\\nGeometric mean of determinants: {geo_mean_det:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Euclidean vs Log-Euclidean Means\n\nLet us visualize the difference between the two means. The\nLog-Euclidean mean respects the manifold geometry and does not\nexhibit swelling.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Convert torch tensors to numpy for plotting\nS1_np = S1.numpy()\nS2_np = S2.numpy()\nmean_euc_np = mean_euclidean.numpy()\nmean_le_np = mean_log_euclidean.numpy()\n\n# Left plot: Euclidean mean\nax = axes[0]\nx1, y1 = spd_to_ellipse(S1_np)\nx2, y2 = spd_to_ellipse(S2_np)\nx_mean_euc, y_mean_euc = spd_to_ellipse(mean_euc_np)\n\nax.fill(x1, y1, alpha=0.3, color=\"blue\", label=\"S1\")\nax.plot(x1, y1, \"b-\", linewidth=2)\nax.fill(x2, y2, alpha=0.3, color=\"green\", label=\"S2\")\nax.plot(x2, y2, \"g-\", linewidth=2)\nax.fill(\n    x_mean_euc,\n    y_mean_euc,\n    alpha=0.3,\n    color=\"red\",\n    label=f\"Euclidean Mean (det={np.linalg.det(mean_euc_np):.2f})\",\n)\nax.plot(x_mean_euc, y_mean_euc, \"r-\", linewidth=2)\n\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.set_aspect(\"equal\")\nax.grid(True, alpha=0.3)\nax.legend(loc=\"upper right\", fontsize=9)\nax.set_title(\"Euclidean Mean\\n(Swelling Effect)\", fontsize=12, fontweight=\"bold\")\n\n# Right plot: Log-Euclidean mean\nax = axes[1]\nx_mean_le, y_mean_le = spd_to_ellipse(mean_le_np)\n\nax.fill(x1, y1, alpha=0.3, color=\"blue\", label=\"S1\")\nax.plot(x1, y1, \"b-\", linewidth=2)\nax.fill(x2, y2, alpha=0.3, color=\"green\", label=\"S2\")\nax.plot(x2, y2, \"g-\", linewidth=2)\nax.fill(\n    x_mean_le,\n    y_mean_le,\n    alpha=0.3,\n    color=\"purple\",\n    label=f\"Log-Euclidean Mean (det={np.linalg.det(mean_le_np):.2f})\",\n)\nax.plot(x_mean_le, y_mean_le, \"m-\", linewidth=2)\n\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.set_aspect(\"equal\")\nax.grid(True, alpha=0.3)\nax.legend(loc=\"upper right\", fontsize=9)\nax.set_title(\n    \"Log-Euclidean Mean\\n(Respects Manifold Geometry)\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\n\nplt.suptitle(\n    \"Comparison of Mean Computation Methods\",\n    fontsize=14,\n    fontweight=\"bold\",\n    y=1.02,\n)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpolation: Euclidean vs Geodesic\n\nAnother key difference is in interpolation. Euclidean interpolation\n(straight line) may produce matrices that are \"far\" from both\nendpoints in the manifold sense. Geodesic interpolation follows\nthe shortest path on the manifold.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def interpolate_euclidean(A, B, t):\n    \"\"\"Linear interpolation: (1-t)*A + t*B\"\"\"\n    return (1 - t) * A + t * B\n\n\ndef interpolate_log_euclidean(A, B, t):\n    \"\"\"Log-Euclidean interpolation.\"\"\"\n    log_A = matrix_log.apply(A)\n    log_B = matrix_log.apply(B)\n    log_interp = (1 - t) * log_A + t * log_B\n    return matrix_exp.apply(log_interp)\n\n\n# Create two SPD matrices\nA = torch.tensor([[3.0, 0.0], [0.0, 1.0]], dtype=torch.float64)\nB = torch.tensor([[1.0, 0.0], [0.0, 3.0]], dtype=torch.float64)\n\n# Interpolation steps\nt_values = np.linspace(0, 1, 11)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Colors for interpolation\ncolors = plt.cm.viridis(t_values)\n\n# Euclidean interpolation\nax = axes[0]\nfor t, color in zip(t_values, colors):\n    interp = interpolate_euclidean(A, B, t)\n    x, y = spd_to_ellipse(interp.numpy())\n    alpha = 0.3 if t not in [0, 1] else 0.6\n    lw = 1.5 if t not in [0, 1] else 3\n    ax.plot(x, y, color=color, linewidth=lw, alpha=alpha + 0.3)\n    ax.fill(x, y, color=color, alpha=alpha)\n\nax.set_xlim(-2.5, 2.5)\nax.set_ylim(-2.5, 2.5)\nax.set_aspect(\"equal\")\nax.grid(True, alpha=0.3)\nax.set_title(\"Euclidean Interpolation\", fontsize=12, fontweight=\"bold\")\nax.set_xlabel(\"x\", fontsize=10)\nax.set_ylabel(\"y\", fontsize=10)\n\n# Log-Euclidean interpolation\nax = axes[1]\nfor t, color in zip(t_values, colors):\n    interp = interpolate_log_euclidean(A, B, t)\n    x, y = spd_to_ellipse(interp.numpy())\n    alpha = 0.3 if t not in [0, 1] else 0.6\n    lw = 1.5 if t not in [0, 1] else 3\n    ax.plot(x, y, color=color, linewidth=lw, alpha=alpha + 0.3)\n    ax.fill(x, y, color=color, alpha=alpha)\n\nax.set_xlim(-2.5, 2.5)\nax.set_ylim(-2.5, 2.5)\nax.set_aspect(\"equal\")\nax.grid(True, alpha=0.3)\nax.set_title(\"Log-Euclidean (Geodesic) Interpolation\", fontsize=12, fontweight=\"bold\")\nax.set_xlabel(\"x\", fontsize=10)\nax.set_ylabel(\"y\", fontsize=10)\n\n# Add colorbar\nsm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=plt.Normalize(0, 1))\nsm.set_array([])\ncbar = plt.colorbar(sm, ax=axes, orientation=\"horizontal\", pad=0.1, aspect=40)\ncbar.set_label(\"Interpolation parameter t (0=A, 1=B)\", fontsize=10)\n\nplt.suptitle(\n    \"Interpolation Between SPD Matrices\",\n    fontsize=14,\n    fontweight=\"bold\",\n    y=1.02,\n)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determinant Along Interpolation Paths\n\nWe can further see the swelling effect by plotting the determinant\nalong the interpolation path. The Euclidean path shows determinant\nincrease in the middle, while the geodesic path is monotonic.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "det_euclidean = []\ndet_geodesic = []\n\nfor t in t_values:\n    interp_euc = interpolate_euclidean(A, B, t)\n    interp_geo = interpolate_log_euclidean(A, B, t)\n    det_euclidean.append(torch.linalg.det(interp_euc).item())\n    det_geodesic.append(torch.linalg.det(interp_geo).item())\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.plot(\n    t_values, det_euclidean, \"r-o\", linewidth=2, markersize=8, label=\"Euclidean Path\"\n)\nax.plot(\n    t_values, det_geodesic, \"b-s\", linewidth=2, markersize=8, label=\"Log-Euclidean Path\"\n)\n\n# Reference line: geometric interpolation of determinants\ndet_geo_interp = [\n    det_euclidean[0] ** (1 - t) * det_euclidean[-1] ** t for t in t_values\n]\nax.plot(\n    t_values,\n    det_geo_interp,\n    \"g--\",\n    linewidth=2,\n    label=\"Geometric Interpolation of det\",\n)\n\nax.axhline(y=det_euclidean[0], color=\"gray\", linestyle=\":\", alpha=0.7)\nax.axhline(y=det_euclidean[-1], color=\"gray\", linestyle=\":\", alpha=0.7)\n\nax.set_xlabel(\"Interpolation parameter t\", fontsize=12)\nax.set_ylabel(\"Determinant\", fontsize=12)\nax.set_title(\n    \"Determinant Along Interpolation Paths\\n(Swelling visible in Euclidean path)\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Distances in a Batch\n\nSPD Learn supports batched operations for efficient computation.\nLet us compute pairwise distances between multiple SPD matrices.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_random_spd_batch(batch_size, n=2, dtype=torch.float64):\n    \"\"\"Generate batch of random SPD matrices using PyTorch.\"\"\"\n    matrices = []\n    for _ in range(batch_size):\n        Q, _ = torch.linalg.qr(torch.randn(n, n, dtype=dtype))\n        eigvals = torch.abs(torch.randn(n, dtype=dtype)) + 0.5\n        matrices.append(Q @ torch.diag(eigvals) @ Q.T)\n    return torch.stack(matrices)\n\n\n# Generate batch of SPD matrices\nbatch = generate_random_spd_batch(5, n=2)\nprint(f\"Batch shape: {batch.shape}\")\n\n# Compute pairwise Log-Euclidean distances\nn_matrices = batch.shape[0]\ndistances = torch.zeros(n_matrices, n_matrices, dtype=torch.float64)\n\nfor i in range(n_matrices):\n    for j in range(n_matrices):\n        distances[i, j] = log_euclidean_distance(batch[i], batch[j])\n\nprint(\"\\nPairwise Log-Euclidean Distance Matrix:\")\nprint(distances.numpy().round(3))\n\n# Visualize\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(distances.numpy(), cmap=\"viridis\")\nax.set_xticks(range(n_matrices))\nax.set_yticks(range(n_matrices))\nax.set_xlabel(\"Matrix Index\", fontsize=12)\nax.set_ylabel(\"Matrix Index\", fontsize=12)\nax.set_title(\"Pairwise Log-Euclidean Distances\", fontsize=14, fontweight=\"bold\")\nplt.colorbar(im, ax=ax, label=\"Distance\")\n\n# Add text annotations\nfor i in range(n_matrices):\n    for j in range(n_matrices):\n        text = ax.text(\n            j, i, f\"{distances[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"white\"\n        )\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this tutorial, we learned:\n\n1. **SPD matrices** are symmetric matrices with positive eigenvalues.\n   They appear in covariance matrices, diffusion tensors, and kernels.\n\n2. **SPD matrices do not form a vector space** - the Euclidean mean\n   exhibits the \"swelling effect\" where the determinant is inflated.\n\n3. **The SPD manifold is a cone** in the space of symmetric matrices.\n\n4. **Riemannian geometry** provides proper tools for working with SPD\n   matrices:\n\n   - ``matrix_log`` maps SPD to symmetric (tangent space)\n   - ``matrix_exp`` maps symmetric back to SPD\n   - ``log_euclidean_distance`` computes manifold-respecting distance\n   - ``log_euclidean_mean`` computes the geometric mean\n\n5. **Log-Euclidean operations** avoid the swelling effect and\n   respect the manifold structure.\n\nThese concepts form the foundation for deep learning on SPD manifolds\n:cite:p:`huang2017riemannian`, which is the focus of the SPD Learn library.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cleanup\nimport matplotlib.pyplot as plt\nplt.close('all')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}