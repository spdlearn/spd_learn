{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# SPD Learn Example\n# ==================\n#\n# First, install the required packages:\n\n!uv pip install -q spd_learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Testing Robustness of SPD Models with Riemannian PGD Attack\n\nThis example demonstrates how to implement and evaluate the Riemannian\nProjected Gradient Descent (R-PGD) attack :cite:p:`timoz2026riemannian` on\nmodels that operate on Symmetric Positive Definite (SPD) matrices using\n**SPD Learn**.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Citation\nIf you use this tutorial or the R-PGD attack implementation in your research,\nplease cite:\n\n```text\nTimoz, D., de Surrel, T., & Yger, F. (2026). Riemannian adversarial attacks\non Symmetric Positive Definite matrices. In International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP).\n```\nBibTeX entry:\n\n```bibtex\n@inproceedings{timoz2026riemannian,\n  title={Riemannian adversarial attacks on Symmetric Positive Definite matrices},\n  author={Timoz, D. and de Surrel, T. and Yger, F.},\n  booktitle={International Conference on Acoustics, Speech, and Signal\n             Processing - ICASSP},\n  year={2026},\n  pages={1--8},\n  organization={Springer},\n}\n```\nAnd please cite **SPD Learn**:\n\n```bibtex\n@article{aristimunha2025spdlearn,\n  title={SPDlearn: A Geometric Deep Learning Python Library for\n         Neural Decoding Through Trivialization},\n  author={Aristimunha, Bruno and Ju, Ce and Collas, Antoine and\n          Bouchard, Florent and Mian, Ammar and Thirion, Bertrand and\n          Chevallier, Sylvain and Kobler, Reinmar},\n  journal={To be submitted},\n  year={2026},\n  url={https://github.com/spdlearn/spd_learn}\n}\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nDeep neural networks are vulnerable to adversarial attacks due to their\nhigh capacity and sensitivity to small, carefully crafted perturbations.\nThese vulnerabilities can be especially concerning when the models operate\non geometrically structured inputs like SPD matrices, where preserving\nthe intrinsic properties of the data is crucial.\n\nThe objective is to construct a perturbation of a SPD matrix such that the\nRiemannian distance to the original sample is minimal, while the prediction\nis completely altered (misclassified).\n\nThere are two main types of attacks:\n\n- **Black-box attacks**: model parameters unknown, but predictions obtainable\n- **White-box attacks**: model parameters known, gradients computable\n\nSince a model robust to white-box attacks is generally robust to black-box\nattacks, we focus on white-box attacks.\n\nFor background on SPD geometry, see :doc:`/background/2_geometry_essentials`.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The R-PGD Attack Algorithm\n\nOne method behind adversarial attacks is to increase the loss function\nwhile respecting a budget constraint $\\epsilon$. The standard\nProjected Gradient Descent (PGD) attack performs gradient ascent with\nprojections back onto a constraint ball.\n\nHowever, Euclidean PGD fails on SPD matrices because:\n\n1. Perturbations may leave the SPD manifold\n2. The Euclidean distance doesn't capture the intrinsic geometry\n\n**SPD Learn** implements the Riemannian PGD attack that performs updates\nalong the affine-invariant geometry:\n\n\\begin{align}\\Sigma_{k+1} = \\Pi_{B_\\epsilon(\\Sigma_0)}\n   \\left( \\exp_{\\Sigma_k}(\\alpha \\cdot \\text{grad } J(\\Sigma_k, Y)) \\right)\\end{align}\n\nwhere:\n\n- $\\exp_{\\Sigma}$ is the exponential map ensuring we stay on the manifold\n- $\\Pi_{B_\\epsilon}$ projects onto the geodesic ball of radius $\\epsilon$\n- $\\text{grad } J$ is the Riemannian gradient of the loss\n\nThe projection onto the ball $B_\\epsilon(\\Sigma_0) = \\{\\Sigma \\in S^d_{++}\n| \\delta_r(\\Sigma_0, \\Sigma) \\leq \\epsilon\\}$ uses:\n\n\\begin{align}\\Pi_{B_\\epsilon(\\Sigma_0)}(\\Sigma) =\n   \\begin{cases}\n   \\exp_{\\Sigma_0}\\left(\\frac{\\epsilon}{\\delta_r(\\Sigma_0,\\Sigma)}\n   \\log_{\\Sigma_0}(\\Sigma)\\right) & \\text{if } \\delta_r(\\Sigma_0, \\Sigma) > \\epsilon \\\\\n   \\Sigma & \\text{otherwise}\n   \\end{cases}\\end{align}\n\nThis ensures the attack respects the budget using Riemannian distance, which\nlimits the loss of geometric information and helps preserve the semantics of\nthe original sample.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom braindecode import EEGClassifier\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.paradigms import MotorImagery\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom skorch.callbacks import EpochScoring, GradientNormClipping\nfrom skorch.dataset import ValidSplit\n\nfrom spd_learn.functional import spd_rpgd_attack\nfrom spd_learn.models import SPDNet\nfrom spd_learn.modules import CovLayer\n\n\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n\nWe use the motor imagery dataset BNCI2014-001 from MOABB. Following the\npaper, we apply a band-pass filter with range [7; 35] Hz. We use a single\nsubject for demonstration purposes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subject_id = 1\n\ndataset = BNCI2014_001()\nparadigm = MotorImagery(n_classes=4, fmin=7, fmax=35)\n\n# Prepare data\ncov_layer = CovLayer()\nX, labels, meta = paradigm.get_data(dataset=dataset, subjects=[subject_id])\nX = torch.tensor(X, dtype=torch.float32)\nX = cov_layer(X)\nle = LabelEncoder()\ny = le.fit_transform(labels)\n\n# Split by session (inter-session setup as from the paper)\ntrain_idx = meta.query(\"session == '0train'\").index.to_numpy()\ntest_idx = meta.query(\"session == '1test'\").index.to_numpy()\n\nprint(f\"Training samples: {len(train_idx)}, Test samples: {len(test_idx)}\")\nprint(f\"SPD matrix shape: {X.shape[1]}x{X.shape[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the SPDNet Model\n\nSPDNet :cite:p:`huang2017riemannian` is a neural network architecture\ndesigned to operate on SPD matrices. It introduces three layers that\npreserve the intrinsic geometry:\n\n- :class:`~spd_learn.modules.BiMap`: bilinear projection $f(\\\\Sigma) = W\\\\Sigma W^{\\\\top}$\n- :class:`~spd_learn.modules.ReEig`: enforces positive eigenvalues via spectral rectification\n- :class:`~spd_learn.modules.LogEig`: maps to tangent space for Euclidean classification\n\nThe model contains 2 BiMap and ReEig layers followed by LogEig and FC layers.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_chans = 22\nmodel = SPDNet(input_type=\"cov\", n_chans=n_chans, n_outputs=4)\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.CrossEntropyLoss,\n    optimizer=torch.optim.Adam,\n    optimizer__lr=1e-3,\n    train_split=ValidSplit(0.1, stratified=True, random_state=42),\n    batch_size=32,\n    max_epochs=20,\n    callbacks=[\n        (\n            \"train_acc\",\n            EpochScoring(\n                \"accuracy\", lower_is_better=False, on_train=True, name=\"train_acc\"\n            ),\n        ),\n        (\"gradient_clip\", GradientNormClipping(gradient_clip_value=1.0)),\n    ],\n    verbose=1,\n)\n\n# Train\nclf.fit(X[train_idx], y[train_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing the Riemannian PGD Attack\n\nWe now use **SPD Learn's** :func:`~spd_learn.functional.spd_rpgd_attack`\nto generate adversarial examples. The attack parameters are:\n\n- ``eps``: Maximum perturbation radius (geodesic budget)\n- ``n_iterations``: Number of PGD iterations (paper uses 50)\n- ``step_size``: Step size $\\alpha$ for each iteration (paper uses 0.1)\n\nFollowing the paper, convergence is reached when the loss variation falls\nbelow $10^{-4}$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n\n# Select samples to attack\nX_test = X[test_idx]\ny_test = y[test_idx]\n\ninitial_accuracy = accuracy_score(y_test, clf.predict(X_test))\nprint(f\"Initial accuracy on clean test data: {initial_accuracy * 100:.2f}%\")\n\n# Parameters for the attack (from the paper)\nepsilon = 0.5  # Maximum perturbation radius (geodesic budget)\nn_iterations = 50  # Number of PGD iterations\nstep_size = 0.1  # Step size for each iteration (alpha in the paper)\n\n# Generate adversarial examples\nX_adv = spd_rpgd_attack(\n    clf,\n    X_test,\n    torch.tensor(y_test),\n    eps=epsilon,\n    criterion=criterion,\n    n_iterations=n_iterations,\n    step_size=step_size,\n)\n\n# Evaluate the model on adversarial examples\nadv_accuracy = accuracy_score(y_test, clf.predict(X_adv))\nprint(f\"Accuracy on adversarial test data: {adv_accuracy * 100:.2f}%\")\nprint(f\"Attack success rate: {(1 - adv_accuracy / initial_accuracy) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating Model Robustness Across Budgets\n\nWe evaluate the model's robustness by attacking with different perturbation\nbudgets. As shown in the paper, R-PGD keeps progressing steadily until it\nreaches 100% success rate, unlike Euclidean PGD which saturates.\n\nThis is because Euclidean attacks must follow curved trajectories on the\nmanifold using straight-line updates. The frequent projections back onto\nthe manifold decrease the effectiveness of each update.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epsilons = torch.linspace(0.0, 2.5, steps=15)\naccuracies = []\n\nfor eps in epsilons:\n    X_adv = spd_rpgd_attack(\n        clf,\n        X_test,\n        torch.tensor(y_test),\n        eps=eps.item(),\n        criterion=criterion,\n        n_iterations=n_iterations,\n        step_size=step_size,\n    )\n    adv_accuracy = accuracy_score(y_test, clf.predict(X_adv))\n    accuracies.append(adv_accuracy)\n    print(f\"Epsilon: {eps.item():.2f}, Adversarial Accuracy: {adv_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Results\n\nWe plot the adversarial accuracy as a function of the perturbation budget.\nThe Riemannian distance can differ significantly from the Euclidean one and\nbetter captures the intrinsic geometry of the manifold, making it more\nappropriate in the context of adversarial attacks.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5))\nax.plot(epsilons.numpy(), [acc * 100 for acc in accuracies], marker=\"o\", linewidth=2)\nax.axhline(y=25, color=\"r\", linestyle=\"--\", label=\"Random chance (4 classes)\")\nax.axhline(y=initial_accuracy * 100, color=\"g\", linestyle=\"--\", label=\"Clean accuracy\")\nax.fill_between(\n    epsilons.numpy(),\n    [acc * 100 for acc in accuracies],\n    initial_accuracy * 100,\n    alpha=0.3,\n)\nax.set_title(\"SPDNet Robustness under Riemannian PGD Attack\", fontsize=14)\nax.set_xlabel(r\"$\\epsilon$ (Geodesic Perturbation Radius)\", fontsize=12)\nax.set_ylabel(\"Adversarial Accuracy (%)\", fontsize=12)\nax.legend(loc=\"upper right\")\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 100)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Clean vs Adversarial Predictions\n\nWe can visualize how predictions change under adversarial perturbations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clean_preds = clf.predict(X_test)\nadv_preds = clf.predict(X_adv)\n\n# Count prediction changes\nchanged_mask = clean_preds != adv_preds\nn_changed = changed_mask.sum()\nn_total = len(y_test)\n\nprint(f\"\\nPrediction changes under attack (epsilon={epsilon}):\")\nprint(\n    f\"  Changed predictions: {n_changed}/{n_total} ({100 * n_changed / n_total:.1f}%)\"\n)\n\n# Confusion between clean and adversarial\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Clean predictions\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\nConfusionMatrixDisplay.from_predictions(\n    y_test, clean_preds, ax=axes[0], cmap=\"Blues\", colorbar=False\n)\naxes[0].set_title(f\"Clean Predictions\\n(Accuracy: {initial_accuracy * 100:.1f}%)\")\n\n# Adversarial predictions\nConfusionMatrixDisplay.from_predictions(\n    y_test, adv_preds, ax=axes[1], cmap=\"Oranges\", colorbar=False\n)\naxes[1].set_title(f\"Adversarial Predictions\\n(Accuracy: {adv_accuracy * 100:.1f}%)\")\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Riemannian Attacks Outperform Euclidean Attacks\n\nThe paper demonstrates that Euclidean PGD encounters increasing difficulty\nin completely degrading the model's performance beyond a certain budget.\nThis happens because:\n\n1. At higher budgets, Euclidean attacks tend to move outside the SPD manifold\n2. The mandatory projection step significantly reduces effective perturbation\n3. Euclidean attacks approximate curved geodesics with straight-line updates\n\nIn contrast, R-PGD:\n\n- Stays on the manifold by construction (using exponential map)\n- Respects the intrinsic geometry with Riemannian gradients\n- Uses geodesic distance for meaningful budget constraints\n\nThe Riemannian PGD attack has the advantage of defining the attack budget\nusing a Riemannian distance, which limits the loss of geometric information\ncaused by the attack and helps preserve the semantics of the original sample.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this example, we demonstrated how to use **SPD Learn** to implement\nthe Riemannian PGD attack on SPD models. Key takeaways:\n\n1. R-PGD effectively reduces model accuracy while respecting manifold geometry\n2. The attack uses geodesic distance for meaningful budget constraints\n3. Unlike Euclidean attacks, R-PGD doesn't saturate at higher budgets\n\nThis highlights the importance of robustness analysis in non-Euclidean\nlearning and opens the door to future work on defense strategies for\nmanifold deep models, such as Riemannian adversarial training or manifold\nregularization.\n\n## References\n\n.. bibliography::\n   :filter: docname in docnames\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cleanup\nimport matplotlib.pyplot as plt\nplt.close('all')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}