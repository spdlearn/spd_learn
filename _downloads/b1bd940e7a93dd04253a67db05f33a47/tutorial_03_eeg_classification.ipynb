{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# SPD Learn Example\n# ==================\n#\n# First, install the required packages:\n\n!uv pip install -q spd_learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# End-to-End EEG Classification Tutorial\n\nThis comprehensive tutorial demonstrates how to build an end-to-end EEG\nclassification pipeline for motor imagery using SPD Learn. We cover everything\nfrom data loading to model selection, training, and evaluation.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to EEG and Motor Imagery Classification\n\nElectroencephalography (EEG) measures electrical activity in the brain\nthrough electrodes placed on the scalp. Motor imagery (MI) is a mental\nprocess where a person imagines performing a motor action without\nactually executing it.\n\n**Why Motor Imagery Classification?**\n\n- **Brain-Computer Interfaces (BCIs)**: Allows paralyzed patients to\n  control devices through thought alone\n- **Rehabilitation**: Helps stroke patients recover motor function\n- **Gaming and Entertainment**: Enables hands-free control\n\n**The SPD Approach**\n\nTraditional approaches use spatial filters like Common Spatial Patterns\n(CSP) to extract discriminative features. SPD Learn takes this further\nby operating directly on the manifold of Symmetric Positive Definite\n(SPD) matrices (covariance matrices), preserving their geometric structure.\n\nThis tutorial uses the BNCI2014_001 dataset :cite:p:`tangermann2012review`\n(BCI Competition IV 2a), which contains 4-class motor imagery data from\n9 subjects.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\nFirst, we import the necessary libraries:\n\n- **MOABB**: For loading standardized EEG datasets with proper preprocessing\n- **Braindecode**: For the EEGClassifier wrapper (scikit-learn compatible)\n- **SPD Learn**: For geometric deep learning models\n- **scikit-learn**: For evaluation metrics and cross-validation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom braindecode import EEGClassifier\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.paradigms import MotorImagery\nfrom sklearn.metrics import (\n    ConfusionMatrixDisplay,\n    accuracy_score,\n    balanced_accuracy_score,\n    confusion_matrix,\n)\nfrom sklearn.preprocessing import LabelEncoder\nfrom skorch.callbacks import EarlyStopping, EpochScoring, GradientNormClipping\nfrom skorch.dataset import ValidSplit\n\nfrom spd_learn.models import EEGSPDNet, SPDNet, TSMNet\n\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings(\"ignore\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data from MOABB\n\nMOABB (Mother of All BCI Benchmarks) provides standardized access to\nmany EEG datasets. We use the BNCI2014_001 dataset:\n\n- **9 subjects**: Each recorded on 2 days (sessions)\n- **4 classes**: Left hand, right hand, feet, tongue\n- **22 EEG channels**: Standard 10-20 montage\n- **250 Hz sampling rate**: After resampling\n\nThe dataset is split into training (session 1) and testing (session 2),\nsimulating real-world cross-session transfer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the dataset\ndataset = BNCI2014_001()\n\n# Create paradigm with 4 motor imagery classes\n# MOABB handles filtering (8-35 Hz bandpass is applied by default for MI)\nparadigm = MotorImagery(n_classes=4)\n\nprint(\"=\" * 60)\nprint(\"Dataset Information\")\nprint(\"=\" * 60)\nprint(f\"Dataset: {dataset.code}\")\nprint(f\"Number of subjects: {len(dataset.subject_list)}\")\nprint(\"Sessions per subject: 2 (train + test)\")\nprint(\"Classes: left_hand, right_hand, feet, tongue\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing Recommendations\n\nProper preprocessing is crucial for good BCI performance. MOABB handles\nmost preprocessing automatically, but here are key considerations:\n\n**Filtering**\n\n- Motor imagery is characterized by Event-Related Desynchronization (ERD)\n  and Synchronization (ERS) in the mu (8-12 Hz) and beta (13-30 Hz) bands\n- Default: 8-35 Hz bandpass filter (captures both mu and beta)\n- For multi-frequency analysis, use FilterBankMotorImagery\n\n**Epoching**\n\n- Motor imagery effects typically occur 0.5-4 seconds after cue onset\n- Default: 0 to 4 seconds post-cue\n- Baseline correction is applied automatically\n\n**Artifact Handling**\n\n- Eye blinks and muscle artifacts can contaminate signals\n- MOABB applies basic artifact rejection\n- For production: Consider ICA or other artifact removal methods\n\n.. tip::\n   For SPD methods, signal quality is crucial because noise affects\n   the covariance matrix estimation. Ensure clean data before training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cache configuration for faster repeated runs\ncache_config = dict(\n    save_raw=True,\n    save_epochs=True,\n    save_array=True,\n    use=True,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n)\n\n# Load data for a single subject (we'll do proper evaluation later)\nsubject_id = 1\nX, labels, meta = paradigm.get_data(\n    dataset=dataset, subjects=[subject_id], cache_config=cache_config\n)\n\n# Encode labels to integers\nle = LabelEncoder()\ny = le.fit_transform(labels)\n\nprint(f\"\\nData loaded for Subject {subject_id}:\")\nprint(f\"  Shape: {X.shape} (trials, channels, timepoints)\")\nprint(\"  Sampling rate: 250 Hz\")\nprint(f\"  Epoch length: {X.shape[2] / 250:.1f} seconds\")\nprint(f\"  Classes: {le.classes_}\")\n\n# Split by session (simulates real-world scenario)\ntrain_idx = meta.query(\"session == '0train'\").index.to_numpy()\ntest_idx = meta.query(\"session == '1test'\").index.to_numpy()\n\nprint(\"\\nData split:\")\nprint(f\"  Training (Session 1): {len(train_idx)} trials\")\nprint(f\"  Testing (Session 2): {len(test_idx)} trials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection Guide\n\nSPD Learn provides several models optimized for different scenarios.\nHere's a guide to help you choose:\n\n**SPDNet** :cite:p:`huang2017riemannian` - The Classic Choice\n\n- Best for: Simple pipelines, pre-computed covariance matrices\n- Pros: Simple architecture, fast training, interpretable\n- Cons: No temporal feature learning\n- Use when: You want a baseline or have limited data\n\n**TSMNet** :cite:p:`kobler2022spd` - Best for Session Transfer\n\n- Best for: Cross-session/cross-subject scenarios\n- Pros: SPDBatchNormMeanVar enables domain adaptation without labels\n- Cons: More parameters, requires more data\n- Use when: You need to transfer to new sessions/subjects\n\n**EEGSPDNet** :cite:p:`wilson2025deep` - Channel-Specific Processing\n\n- Best for: When spatial information is important\n- Pros: Learns channel-specific temporal filters\n- Cons: More memory intensive\n- Use when: Channels have different characteristics\n\n**TensorCSPNet** :cite:p:`ju2022tensor` - Multi-Frequency Analysis\n\n- Best for: Filter bank approaches with multiple frequency bands\n- Pros: Captures frequency-specific spatial patterns\n- Cons: Requires FilterBankMotorImagery paradigm\n- Use when: Different frequency bands carry complementary information\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For this tutorial, we'll compare SPDNet, TSMNet, and EEGSPDNet on\n   standard (single-band) motor imagery data.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_chans = X.shape[1]  # 22 channels\nn_outputs = len(le.classes_)  # 4 classes\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Model Architectures\")\nprint(\"=\" * 60)\n\n# SPDNet: Simple but effective\nspdnet = SPDNet(\n    n_chans=n_chans,\n    n_outputs=n_outputs,\n    subspacedim=n_chans,  # Keep full dimensionality\n    threshold=1e-4,  # ReEig threshold\n)\nprint(\"\\nSPDNet:\")\nprint(f\"  Parameters: {sum(p.numel() for p in spdnet.parameters()):,}\")\nprint(\"  Architecture: CovLayer -> BiMap -> ReEig -> LogEig -> Linear\")\n\n# TSMNet: With built-in feature extraction and batch normalization\ntsmnet = TSMNet(\n    n_chans=n_chans,\n    n_outputs=n_outputs,\n    n_temp_filters=8,  # Temporal filters\n    temp_kernel_length=50,  # ~200ms at 250Hz\n    n_spatiotemp_filters=32,  # Spatiotemporal features\n    n_bimap_filters=16,  # BiMap output dimension\n    reeig_threshold=1e-4,\n)\nprint(\"\\nTSMNet:\")\nprint(f\"  Parameters: {sum(p.numel() for p in tsmnet.parameters()):,}\")\nprint(\n    \"  Architecture: TempConv -> SpatialConv -> CovLayer -> BiMap -> SPDBatchNormMeanVar -> LogEig\"\n)\n\n# EEGSPDNet: Channel-specific convolution\neegspdnet = EEGSPDNet(\n    n_chans=n_chans,\n    n_outputs=n_outputs,\n    n_filters=4,  # 4 filters per channel\n    bimap_sizes=(2, 2),  # Scaling factor and depth\n    filter_time_length=25,  # ~100ms at 250Hz\n    spd_drop_prob=0.0,  # Disable SPD dropout for stability\n)\nprint(\"\\nEEGSPDNet:\")\nprint(f\"  Parameters: {sum(p.numel() for p in eegspdnet.parameters()):,}\")\nprint(\"  Architecture: GroupedConv1D -> CovPool -> BiMap -> ReEig -> LogEig -> Linear\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n\nSPD networks require careful hyperparameter selection for stable training.\nHere are the key settings:\n\n**Learning Rate**\n\n- Use low learning rates (1e-4 to 5e-4)\n- Riemannian optimization is sensitive to step size\n- Too high: Training diverges, NaN losses\n- Too low: Slow convergence\n\n**Gradient Clipping**\n\n- Essential for SPD networks\n- Prevents exploding gradients during eigenvalue operations\n- Recommended: gradient_clip_value=1.0\n\n**Optimizer**\n\n- Adam works well (adaptive learning rate)\n- SGD with momentum can also work but needs more tuning\n- AdamW with weight decay for regularization\n\n**Batch Size**\n\n- 16-64 trials typically works well\n- Larger batches give more stable covariance estimates\n- Limited by GPU memory\n\n**Early Stopping**\n\n- Monitor validation loss\n- Patience of 10-20 epochs is reasonable\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\nbatch_size = 32\nmax_epochs = 10  # Reduced for documentation build speed (was 200)\nlearning_rate = 1e-4  # CRITICAL: Use low learning rate\n\n# Device selection\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"\\n{'=' * 60}\")\nprint(\"Training Configuration\")\nprint(\"=\" * 60)\nprint(f\"Device: {device}\")\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Max epochs: {max_epochs}\")\nprint(f\"Learning rate: {learning_rate}\")\nprint(\"Gradient clipping: 1.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Models\n\nWe'll train each model and compare their performance. The key components\nof our training setup:\n\n- **EEGClassifier**: Braindecode wrapper for scikit-learn compatibility\n- **GradientNormClipping**: Prevents gradient explosion\n- **EarlyStopping**: Stops training when validation loss plateaus\n- **EpochScoring**: Tracks accuracy during training\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_classifier(model, learning_rate=1e-4, max_epochs=100, batch_size=32):\n    \"\"\"Create an EEGClassifier with proper settings for SPD networks.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model to train.\n    learning_rate : float\n        Learning rate (use low values like 1e-4).\n    max_epochs : int\n        Maximum number of training epochs.\n    batch_size : int\n        Batch size for training.\n\n    Returns\n    -------\n    EEGClassifier\n        Configured classifier ready for training.\n    \"\"\"\n    clf = EEGClassifier(\n        model,\n        criterion=torch.nn.CrossEntropyLoss,\n        optimizer=torch.optim.Adam,\n        optimizer__lr=learning_rate,\n        # Validation split for early stopping and monitoring\n        train_split=ValidSplit(0.2, stratified=True, random_state=42),\n        batch_size=batch_size,\n        max_epochs=max_epochs,\n        callbacks=[\n            # Track training accuracy\n            (\n                \"train_acc\",\n                EpochScoring(\n                    \"accuracy\", lower_is_better=False, on_train=True, name=\"train_acc\"\n                ),\n            ),\n            # CRITICAL: Gradient clipping for SPD network stability\n            (\"gradient_clip\", GradientNormClipping(gradient_clip_value=1.0)),\n            # Early stopping to prevent overfitting\n            (\n                \"early_stop\",\n                EarlyStopping(\n                    monitor=\"valid_loss\",\n                    patience=15,\n                    threshold=1e-4,\n                    lower_is_better=True,\n                ),\n            ),\n        ],\n        device=device,\n        verbose=0,  # Set to 1 for training progress\n    )\n    return clf\n\n\n# Store results for comparison\nresults = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training SPDNet\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Training SPDNet\")\nprint(\"=\" * 60)\n\n# Create fresh model instance\nspdnet = SPDNet(\n    n_chans=n_chans,\n    n_outputs=n_outputs,\n    subspacedim=n_chans,\n    threshold=1e-4,\n)\n\nclf_spdnet = create_classifier(spdnet, learning_rate=1e-4)\nclf_spdnet.fit(X[train_idx], y[train_idx])\n\n# Evaluate\ny_pred_train_spdnet = clf_spdnet.predict(X[train_idx])\ny_pred_test_spdnet = clf_spdnet.predict(X[test_idx])\n\nresults[\"SPDNet\"] = {\n    \"train_acc\": accuracy_score(y[train_idx], y_pred_train_spdnet),\n    \"test_acc\": accuracy_score(y[test_idx], y_pred_test_spdnet),\n    \"test_bal_acc\": balanced_accuracy_score(y[test_idx], y_pred_test_spdnet),\n    \"y_pred\": y_pred_test_spdnet,\n    \"history\": clf_spdnet.history,\n}\n\nprint(f\"Train Accuracy: {results['SPDNet']['train_acc'] * 100:.2f}%\")\nprint(f\"Test Accuracy:  {results['SPDNet']['test_acc'] * 100:.2f}%\")\nprint(f\"Test Balanced:  {results['SPDNet']['test_bal_acc'] * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training TSMNet\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Training TSMNet\")\nprint(\"=\" * 60)\n\n# Create fresh model instance\ntsmnet = TSMNet(\n    n_chans=n_chans,\n    n_outputs=n_outputs,\n    n_temp_filters=8,\n    temp_kernel_length=50,\n    n_spatiotemp_filters=32,\n    n_bimap_filters=16,\n    reeig_threshold=1e-4,\n)\n\nclf_tsmnet = create_classifier(tsmnet, learning_rate=1e-4)\nclf_tsmnet.fit(X[train_idx], y[train_idx])\n\n# Evaluate\ny_pred_train_tsmnet = clf_tsmnet.predict(X[train_idx])\ny_pred_test_tsmnet = clf_tsmnet.predict(X[test_idx])\n\nresults[\"TSMNet\"] = {\n    \"train_acc\": accuracy_score(y[train_idx], y_pred_train_tsmnet),\n    \"test_acc\": accuracy_score(y[test_idx], y_pred_test_tsmnet),\n    \"test_bal_acc\": balanced_accuracy_score(y[test_idx], y_pred_test_tsmnet),\n    \"y_pred\": y_pred_test_tsmnet,\n    \"history\": clf_tsmnet.history,\n}\n\nprint(f\"Train Accuracy: {results['TSMNet']['train_acc'] * 100:.2f}%\")\nprint(f\"Test Accuracy:  {results['TSMNet']['test_acc'] * 100:.2f}%\")\nprint(f\"Test Balanced:  {results['TSMNet']['test_bal_acc'] * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training EEGSPDNet\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Training EEGSPDNet\")\nprint(\"=\" * 60)\n\n# Create fresh model instance\neegspdnet = EEGSPDNet(\n    n_chans=n_chans,\n    n_outputs=n_outputs,\n    n_filters=4,\n    bimap_sizes=(2, 2),\n    filter_time_length=25,\n    spd_drop_prob=0.0,\n)\n\nclf_eegspdnet = create_classifier(eegspdnet, learning_rate=1e-4)\nclf_eegspdnet.fit(X[train_idx], y[train_idx])\n\n# Evaluate\ny_pred_train_eegspdnet = clf_eegspdnet.predict(X[train_idx])\ny_pred_test_eegspdnet = clf_eegspdnet.predict(X[test_idx])\n\nresults[\"EEGSPDNet\"] = {\n    \"train_acc\": accuracy_score(y[train_idx], y_pred_train_eegspdnet),\n    \"test_acc\": accuracy_score(y[test_idx], y_pred_test_eegspdnet),\n    \"test_bal_acc\": balanced_accuracy_score(y[test_idx], y_pred_test_eegspdnet),\n    \"y_pred\": y_pred_test_eegspdnet,\n    \"history\": clf_eegspdnet.history,\n}\n\nprint(f\"Train Accuracy: {results['EEGSPDNet']['train_acc'] * 100:.2f}%\")\nprint(f\"Test Accuracy:  {results['EEGSPDNet']['test_acc'] * 100:.2f}%\")\nprint(f\"Test Balanced:  {results['EEGSPDNet']['test_bal_acc'] * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of Results\n\nLet's visualize the training progress and compare model performance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16, 10))\n\n# 1. Model Comparison Bar Chart\nax1 = fig.add_subplot(2, 3, 1)\nmodels = list(results.keys())\ntest_accs = [results[m][\"test_acc\"] * 100 for m in models]\ntrain_accs = [results[m][\"train_acc\"] * 100 for m in models]\n\nx = np.arange(len(models))\nwidth = 0.35\n\nbars1 = ax1.bar(\n    x - width / 2, train_accs, width, label=\"Train\", color=\"#3498db\", alpha=0.8\n)\nbars2 = ax1.bar(\n    x + width / 2, test_accs, width, label=\"Test\", color=\"#e74c3c\", alpha=0.8\n)\n\nax1.set_ylabel(\"Accuracy (%)\", fontsize=12)\nax1.set_title(\"Model Comparison\", fontsize=14, fontweight=\"bold\")\nax1.set_xticks(x)\nax1.set_xticklabels(models, fontsize=11)\nax1.legend(fontsize=10)\nax1.set_ylim([0, 100])\nax1.axhline(y=25, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Chance level\")\nax1.grid(True, alpha=0.3, axis=\"y\")\n\n# Add value labels\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.annotate(\n        f\"{height:.1f}\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 3),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=9,\n    )\nfor bar in bars2:\n    height = bar.get_height()\n    ax1.annotate(\n        f\"{height:.1f}\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 3),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=9,\n    )\n\n# 2-4. Training Loss Curves\nfor idx, (model_name, color) in enumerate(\n    [(\"SPDNet\", \"#3498db\"), (\"TSMNet\", \"#2ecc71\"), (\"EEGSPDNet\", \"#9b59b6\")]\n):\n    ax = fig.add_subplot(2, 3, idx + 2)\n    history = results[model_name][\"history\"]\n    epochs = range(1, len(history) + 1)\n\n    ax.plot(\n        epochs, history[:, \"train_loss\"], \"-\", color=color, label=\"Train\", linewidth=2\n    )\n    ax.plot(\n        epochs,\n        history[:, \"valid_loss\"],\n        \"--\",\n        color=color,\n        alpha=0.7,\n        label=\"Valid\",\n        linewidth=2,\n    )\n    ax.set_xlabel(\"Epoch\", fontsize=11)\n    ax.set_ylabel(\"Loss\", fontsize=11)\n    ax.set_title(f\"{model_name} Training Curves\", fontsize=12, fontweight=\"bold\")\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n\n# 5-6. Confusion Matrices (best and worst performing models)\nsorted_models = sorted(\n    results.keys(), key=lambda m: results[m][\"test_acc\"], reverse=True\n)\nbest_model = sorted_models[0]\n\nax5 = fig.add_subplot(2, 3, 5)\ncm = confusion_matrix(y[test_idx], results[best_model][\"y_pred\"])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\ndisp.plot(ax=ax5, cmap=\"Blues\", values_format=\"d\")\nax5.set_title(\n    f\"Best: {best_model}\\nTest Acc: {results[best_model]['test_acc'] * 100:.1f}%\",\n    fontsize=12,\n    fontweight=\"bold\",\n)\n\n# Summary statistics\nax6 = fig.add_subplot(2, 3, 6)\nax6.axis(\"off\")\nsummary_text = \"Results Summary\\n\" + \"=\" * 30 + \"\\n\\n\"\nfor model in sorted_models:\n    summary_text += f\"{model}:\\n\"\n    summary_text += f\"  Train: {results[model]['train_acc'] * 100:.1f}%\\n\"\n    summary_text += f\"  Test:  {results[model]['test_acc'] * 100:.1f}%\\n\"\n    summary_text += f\"  Balanced: {results[model]['test_bal_acc'] * 100:.1f}%\\n\\n\"\nsummary_text += \"=\" * 30 + \"\\n\"\nsummary_text += \"Chance level: 25.0%\"\nax6.text(\n    0.1, 0.5, summary_text, fontsize=12, family=\"monospace\", verticalalignment=\"center\"\n)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Validation Evaluation\n\nFor a more robust evaluation, we can use cross-validation. Since we have\nsession information, we use session-based splits which better simulate\nreal-world scenarios.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Due to computation time, we demonstrate with the simplest model.\n   For production, evaluate all models with proper cross-validation.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Cross-Validation Evaluation\")\nprint(\"=\" * 60)\n\n\ndef evaluate_cross_session(model_class, model_kwargs, subjects=[1, 2, 3]):\n    \"\"\"Evaluate model using cross-session validation.\n\n    Parameters\n    ----------\n    model_class : class\n        Model class to instantiate.\n    model_kwargs : dict\n        Keyword arguments for model initialization.\n    subjects : list\n        List of subject IDs to evaluate.\n\n    Returns\n    -------\n    dict\n        Dictionary with mean and std accuracy across subjects.\n    \"\"\"\n    accuracies = []\n\n    for subj in subjects:\n        # Load data\n        X_subj, labels_subj, meta_subj = paradigm.get_data(\n            dataset=dataset, subjects=[subj], cache_config=cache_config\n        )\n        y_subj = le.fit_transform(labels_subj)\n\n        # Split by session\n        train_idx_subj = meta_subj.query(\"session == '0train'\").index.to_numpy()\n        test_idx_subj = meta_subj.query(\"session == '1test'\").index.to_numpy()\n\n        # Create fresh model\n        model = model_class(**model_kwargs)\n        clf = create_classifier(model, max_epochs=50)  # Fewer epochs for speed\n\n        # Train and evaluate\n        clf.fit(X_subj[train_idx_subj], y_subj[train_idx_subj])\n        y_pred = clf.predict(X_subj[test_idx_subj])\n        acc = accuracy_score(y_subj[test_idx_subj], y_pred)\n        accuracies.append(acc)\n        print(f\"  Subject {subj}: {acc * 100:.2f}%\")\n\n    return {\"mean\": np.mean(accuracies), \"std\": np.std(accuracies), \"all\": accuracies}\n\n\n# Evaluate SPDNet on multiple subjects\nprint(\"\\nSPDNet Cross-Session Evaluation:\")\nspdnet_cv = evaluate_cross_session(\n    SPDNet,\n    {\n        \"n_chans\": n_chans,\n        \"n_outputs\": n_outputs,\n        \"subspacedim\": n_chans,\n        \"threshold\": 1e-4,\n    },\n    subjects=[1],  # Reduced for documentation build speed (was [1, 2, 3])\n)\nprint(f\"\\nSPDNet: {spdnet_cv['mean'] * 100:.1f}% +/- {spdnet_cv['std'] * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting Tips\n\nHere are common issues and solutions when training SPD networks:\n\n**Problem: NaN losses or diverging training**\n\n- Solution 1: Reduce learning rate (try 1e-5)\n- Solution 2: Increase gradient clipping threshold\n- Solution 3: Check for NaN/Inf in input data\n- Solution 4: Add small epsilon to covariance matrices for numerical stability\n\n**Problem: Model performs at chance level**\n\n- Solution 1: Verify data loading and label encoding\n- Solution 2: Check that train/test split is correct\n- Solution 3: Increase model capacity (more filters)\n- Solution 4: Try longer training with early stopping\n\n**Problem: Large gap between train and test accuracy (overfitting)**\n\n- Solution 1: Add regularization (weight decay in optimizer)\n- Solution 2: Use dropout (final_layer_drop_prob for EEGSPDNet)\n- Solution 3: Reduce model complexity\n- Solution 4: Use data augmentation\n\n**Problem: Training is too slow**\n\n- Solution 1: Use GPU (set device=\"cuda\")\n- Solution 2: Reduce batch size\n- Solution 3: Use mixed precision training\n- Solution 4: Reduce model complexity\n\n**Problem: Out of memory errors**\n\n- Solution 1: Reduce batch size\n- Solution 2: Use gradient accumulation\n- Solution 3: Use a simpler model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Troubleshooting Checklist\")\nprint(\"=\" * 60)\nprint(\"\"\"\nIf your model isn't working:\n\n1. Data Quality\n   [ ] Check for NaN/Inf values: np.any(np.isnan(X)) should be False\n   [ ] Verify shapes: X should be (n_trials, n_channels, n_timepoints)\n   [ ] Ensure proper filtering (8-35 Hz for motor imagery)\n\n2. Training Settings\n   [ ] Learning rate is low (1e-4 or lower)\n   [ ] Gradient clipping is enabled (1.0)\n   [ ] Batch size is reasonable (16-64)\n\n3. Model Selection\n   [ ] SPDNet: For simple baselines\n   [ ] TSMNet: For session transfer (has SPDBatchNormMeanVar)\n   [ ] EEGSPDNet: For channel-specific processing\n   [ ] TensorCSPNet: For multi-frequency (filter bank)\n\n4. Numerical Stability\n   [ ] ReEig threshold > 0 (default 1e-4)\n   [ ] SPD dropout disabled if unstable (spd_drop_prob=0.0)\n\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n\nIn this tutorial, we covered:\n\n1. **Data Loading**: Using MOABB for standardized EEG data access\n2. **Preprocessing**: Default filtering for motor imagery (8-35 Hz)\n3. **Model Selection**: SPDNet (simple), TSMNet (transfer), EEGSPDNet (spatial)\n4. **Training**: Low learning rate (1e-4), gradient clipping, Adam optimizer\n5. **Evaluation**: Cross-session validation for realistic estimates\n\n**Best Practices Summary**:\n\n- Always use gradient clipping with SPD networks\n- Start with low learning rates (1e-4) and increase if needed\n- Monitor both training and validation loss for overfitting\n- Use cross-session/cross-subject evaluation for realistic estimates\n- Consider TSMNet for session transfer scenarios\n\n**Next Steps**:\n\n- Try TensorCSPNet with FilterBankMotorImagery for multi-frequency analysis\n- Explore domain adaptation with TSMNet's SPDBatchNormMeanVar\n- Implement your own preprocessing pipeline for specific needs\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cleanup\nimport matplotlib.pyplot as plt\nplt.close('all')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}