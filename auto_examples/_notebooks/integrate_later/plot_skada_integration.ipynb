{
  "cells": [
    {
      "id": "7ab8c648",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install -q spd_learn moabb braindecode scikit-learn matplotlib\n\n# For GPU support (recommended for faster training)\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Domain Adaptation with skada and SPD Learn\n\nThis tutorial demonstrates how to integrate SPD Learn with\n[skada](https://scikit-adaptation.github.io/) (scikit-learn domain adaptation)\nfor cross-subject EEG classification. We show how to combine SPD-based feature\nextraction with various domain adaptation strategies.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\n**Domain adaptation** is a machine learning technique that addresses the\nchallenge of applying a model trained on data from one distribution (source\ndomain) to data from a different but related distribution (target domain).\n\nIn EEG-based Brain-Computer Interfaces (BCIs), domain adaptation is crucial\nbecause:\n\n- **Subject variability**: EEG signals differ significantly between subjects\n  due to anatomical differences, electrode placement, and neural patterns\n- **Session variability**: Even the same subject shows drift over time due to\n  electrode impedance changes and mental state variations\n- **Limited labeled data**: Collecting labeled EEG is expensive and\n  time-consuming, requiring subjects to perform specific tasks\n\nThis tutorial combines two powerful approaches:\n\n1. **SPD Learn**: Extracts geometric features from covariance matrices that\n   lie on the SPD (Symmetric Positive Definite) manifold\n2. **skada**: Provides domain adaptation algorithms to align feature\n   distributions between source and target domains\n\n**Methods demonstrated:**\n\n- **SPDBatchNormMeanVar**: Native Riemannian domain adaptation\n  :cite:p:`kobler2022spd`\n- **CORAL**: Correlation Alignment :cite:p:`sun2016coral`\n- **Subspace Alignment**: Linear subspace mapping\n  :cite:p:`fernando2013subspace`\n- **Optimal Transport**: Sample transport between domains\n  :cite:p:`courty2017optimal`\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport warnings\n\nfrom typing import Dict, List, Optional, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.paradigms import MotorImagery\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score,\n    balanced_accuracy_score,\n    confusion_matrix,\n)\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom spd_learn.modules import BiMap, CovLayer, LogEig, ReEig, SPDBatchNormMeanVar\n\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom skada import (\n    CORALAdapter,\n    EntropicOTMapping,\n    SubspaceAlignment,\n    make_da_pipeline,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading EEG Data\n\nWe use the BNCI2014_001 dataset (BCI Competition IV 2a) with:\n\n- **Source domain**: Subject 1 (training data with labels)\n- **Target domain**: Subject 2 (testing/adaptation - labels only for\n  evaluation)\n\nThis simulates a common BCI scenario: train a model on one subject and\ndeploy it on another subject without collecting new labeled data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = BNCI2014_001()\nparadigm = MotorImagery(n_classes=2)  # Left vs Right hand\n\n# Cache configuration for faster loading\ncache_config = dict(\n    save_raw=True,\n    save_epochs=True,\n    save_array=True,\n    use=True,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n)\n\n# Define source and target subjects\nsource_subject = 1\ntarget_subject = 2\n\nprint(f\"Loading data for subjects {source_subject} and {target_subject}...\")\nX, labels, meta = paradigm.get_data(\n    dataset=dataset,\n    subjects=[source_subject, target_subject],\n    cache_config=cache_config,\n)\n\n# Encode labels\nle = LabelEncoder()\ny = le.fit_transform(labels)\n\n# Split by subject\nsource_mask = meta[\"subject\"] == source_subject\ntarget_mask = meta[\"subject\"] == target_subject\n\nX_source: np.ndarray = X[source_mask]\ny_source: np.ndarray = y[source_mask]\nX_target: np.ndarray = X[target_mask]\ny_target: np.ndarray = y[target_mask]\n\nprint(f\"\\nSource domain (Subject {source_subject}): {len(X_source)} samples\")\nprint(f\"Target domain (Subject {target_subject}): {len(X_target)} samples\")\nprint(f\"Number of channels: {X.shape[1]}\")\nprint(f\"Number of time points: {X.shape[2]}\")\nprint(f\"Classes: {le.classes_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPD Feature Extraction\n\nWe create an SPD-based feature extractor that transforms raw EEG signals\ninto geometric features suitable for domain adaptation:\n\n1. **CovLayer**: Computes covariance matrices from raw EEG\n2. **BiMap**: Learns a bilinear mapping for dimensionality reduction\n3. **ReEig**: Applies a non-linearity to eigenvalues for numerical stability\n4. **LogEig**: Projects SPD matrices to tangent space at identity (Euclidean)\n\nThe tangent space features are then suitable for standard ML classifiers\nand domain adaptation methods.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SPDFeatureExtractor(torch.nn.Module):\n    \"\"\"Extract tangent space features from raw EEG using SPD operations.\n\n    This module computes covariance matrices from EEG signals and projects\n    them to the tangent space at the identity matrix, producing Euclidean\n    features suitable for standard classifiers.\n\n    Parameters\n    ----------\n    n_channels : int\n        Number of EEG channels in the input data.\n    subspace_dim : int, optional\n        Dimension of the BiMap output (reduced SPD matrix size).\n        If None, uses the full channel dimension. Smaller values reduce\n        feature dimensionality but may lose discriminative information.\n    threshold : float, default=1e-4\n        ReEig threshold for numerical stability. Eigenvalues below this\n        threshold are clipped to prevent numerical issues in matrix\n        operations.\n\n    Attributes\n    ----------\n    output_dim : int\n        Number of output features (upper triangular elements of the\n        subspace_dim x subspace_dim matrix).\n\n    Examples\n    --------\n    >>> extractor = SPDFeatureExtractor(n_channels=22, subspace_dim=10)\n    >>> X = torch.randn(32, 22, 500)  # batch of 32 EEG trials\n    >>> features = extractor(X)\n    >>> features.shape\n    torch.Size([32, 55])  # 10*(10+1)/2 = 55 features\n    \"\"\"\n\n    def __init__(\n        self,\n        n_channels: int,\n        subspace_dim: Optional[int] = None,\n        threshold: float = 1e-4,\n    ) -> None:\n        super().__init__()\n        if subspace_dim is None:\n            subspace_dim = n_channels\n\n        self.n_channels = n_channels\n        self.subspace_dim = subspace_dim\n        self.threshold = threshold\n\n        self.cov = CovLayer()\n        self.bimap = BiMap(n_channels, subspace_dim)\n        self.reeig = ReEig(threshold=threshold)\n        self.logeig = LogEig(upper=True, flatten=True)\n\n        # Output dimension: upper triangular elements\n        self.output_dim = subspace_dim * (subspace_dim + 1) // 2\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Extract features from raw EEG.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Raw EEG of shape (batch, channels, time).\n\n        Returns\n        -------\n        torch.Tensor\n            Tangent space features of shape (batch, output_dim).\n        \"\"\"\n        cov = self.cov(x)\n        spd = self.bimap(cov)\n        spd = self.reeig(spd)\n        features = self.logeig(spd)\n        return features\n\n\nclass SPDFeatureExtractorWithBN(torch.nn.Module):\n    \"\"\"SPD feature extractor with batch normalization for domain adaptation.\n\n    This extractor includes SPDBatchNormMeanVar, which normalizes SPD matrices\n    using the Fr\u00e9chet mean on the Riemannian manifold. The running\n    statistics can be adapted to a target domain without retraining.\n\n    Parameters\n    ----------\n    n_channels : int\n        Number of EEG channels.\n    subspace_dim : int, optional\n        Dimension of BiMap output. If None, uses full channel dimension.\n    threshold : float, default=1e-4\n        ReEig threshold for numerical stability.\n    bn_momentum : float, default=0.1\n        Momentum for SPDBatchNormMeanVar running statistics update.\n        Higher values adapt faster but may be less stable.\n\n    Notes\n    -----\n    For domain adaptation:\n\n    1. Train the extractor on source domain (train mode)\n    2. Reset running stats and adapt on target domain (train mode for BN only)\n    3. Extract target features (eval mode)\n    \"\"\"\n\n    def __init__(\n        self,\n        n_channels: int,\n        subspace_dim: Optional[int] = None,\n        threshold: float = 1e-4,\n        bn_momentum: float = 0.1,\n    ) -> None:\n        super().__init__()\n        if subspace_dim is None:\n            subspace_dim = n_channels\n\n        self.n_channels = n_channels\n        self.subspace_dim = subspace_dim\n        self.threshold = threshold\n\n        self.cov = CovLayer()\n        self.bimap = BiMap(n_channels, subspace_dim)\n        self.reeig = ReEig(threshold=threshold)\n        self.spdbn = SPDBatchNormMeanVar(\n            subspace_dim, momentum=bn_momentum, affine=True\n        )\n        self.logeig = LogEig(upper=True, flatten=True)\n\n        self.output_dim = subspace_dim * (subspace_dim + 1) // 2\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Extract features with batch normalization.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Raw EEG of shape (batch, channels, time).\n\n        Returns\n        -------\n        torch.Tensor\n            Normalized tangent space features of shape (batch, output_dim).\n        \"\"\"\n        cov = self.cov(x)\n        spd = self.bimap(cov)\n        spd = self.reeig(spd)\n        spd = self.spdbn(spd)\n        features = self.logeig(spd)\n        return features\n\n\ndef extract_features(\n    extractor: torch.nn.Module,\n    X: np.ndarray,\n    batch_size: int = 64,\n) -> np.ndarray:\n    \"\"\"Extract features from data using the given extractor.\n\n    Parameters\n    ----------\n    extractor : torch.nn.Module\n        Feature extractor module (SPDFeatureExtractor or\n        SPDFeatureExtractorWithBN).\n    X : np.ndarray\n        EEG data of shape (n_samples, n_channels, n_times).\n    batch_size : int, default=64\n        Batch size for processing.\n\n    Returns\n    -------\n    np.ndarray\n        Feature array of shape (n_samples, feature_dim).\n    \"\"\"\n    extractor.eval()\n    X_tensor = torch.from_numpy(X).float()\n\n    features_list = []\n    with torch.no_grad():\n        for i in range(0, len(X_tensor), batch_size):\n            batch = X_tensor[i : i + batch_size]\n            features = extractor(batch)\n            features_list.append(features)\n\n    return torch.cat(features_list, dim=0).numpy()\n\n\ndef train_extractor_on_source(\n    extractor: torch.nn.Module,\n    X_source: np.ndarray,\n    n_epochs: int = 5,\n    batch_size: int = 32,\n) -> torch.nn.Module:\n    \"\"\"Train extractor on source domain to learn BiMap weights and BN stats.\n\n    This function passes source data through the extractor multiple times\n    to update the BiMap weights (if in train mode) and SPDBatchNormMeanVar running\n    statistics.\n\n    Parameters\n    ----------\n    extractor : torch.nn.Module\n        Feature extractor with SPDBatchNormMeanVar.\n    X_source : np.ndarray\n        Source domain EEG data.\n    n_epochs : int, default=5\n        Number of passes through the source data.\n    batch_size : int, default=32\n        Batch size for training.\n\n    Returns\n    -------\n    torch.nn.Module\n        Extractor with updated running statistics.\n    \"\"\"\n    extractor.train()\n    X_tensor = torch.from_numpy(X_source).float()\n\n    for epoch in range(n_epochs):\n        # Shuffle data\n        perm = torch.randperm(len(X_tensor))\n        X_shuffled = X_tensor[perm]\n\n        for i in range(0, len(X_shuffled), batch_size):\n            batch = X_shuffled[i : i + batch_size]\n            with torch.no_grad():\n                _ = extractor(batch)\n\n    return extractor\n\n\ndef adapt_spdbn_to_target(\n    extractor: torch.nn.Module,\n    X_target: np.ndarray,\n    n_passes: int = 10,\n    batch_size: int = 32,\n    reset_stats: bool = True,\n) -> torch.nn.Module:\n    \"\"\"Adapt SPDBatchNormMeanVar statistics to target domain.\n\n    This implements Source-Free Unsupervised Domain Adaptation (SFUDA)\n    by updating only the SPDBatchNormMeanVar running statistics on unlabeled\n    target data. No labels are required from the target domain.\n\n    Parameters\n    ----------\n    extractor : torch.nn.Module\n        Feature extractor with SPDBatchNormMeanVar layer(s).\n    X_target : np.ndarray\n        Target domain data (unlabeled).\n    n_passes : int, default=10\n        Number of passes through target data for statistics update.\n        More passes provide better statistics estimation.\n    batch_size : int, default=32\n        Batch size for adaptation.\n    reset_stats : bool, default=True\n        If True, reset running statistics before adaptation.\n        This allows the model to fully adapt to the target domain.\n\n    Returns\n    -------\n    torch.nn.Module\n        Extractor with SPDBatchNormMeanVar adapted to target domain.\n\n    Notes\n    -----\n    The adaptation works by:\n\n    1. Setting the model to eval mode (freezing BiMap weights)\n    2. Setting SPDBatchNormMeanVar to train mode (updating running stats)\n    3. Passing target data through to update the Fr\u00e9chet mean\n    4. The running mean adapts to center target data at identity\n    \"\"\"\n    # Freeze all layers except SPDBatchNormMeanVar\n    extractor.eval()\n\n    # Find and configure SPDBatchNormMeanVar layers\n    spdbn_modules = []\n    for module in extractor.modules():\n        if isinstance(module, SPDBatchNormMeanVar):\n            spdbn_modules.append(module)\n            if reset_stats:\n                module.reset_running_stats()\n            module.train()  # Enable running stats update\n\n    if len(spdbn_modules) == 0:\n        print(\"  Warning: No SPDBatchNormMeanVar layers found!\")\n        return extractor\n\n    print(f\"  Adapting {len(spdbn_modules)} SPDBatchNormMeanVar layer(s)...\")\n\n    X_tensor = torch.from_numpy(X_target).float()\n\n    # Multiple passes through target data\n    with torch.no_grad():\n        for pass_idx in range(n_passes):\n            perm = torch.randperm(len(X_tensor))\n            X_shuffled = X_tensor[perm]\n            for i in range(0, len(X_shuffled), batch_size):\n                batch = X_shuffled[i : i + batch_size]\n                _ = extractor(batch)\n\n    # Set everything back to eval mode\n    extractor.eval()\n    return extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction Setup\n\nWe experiment with different subspace dimensions to find the optimal\ntrade-off between feature richness and generalization.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_channels = X.shape[1]\n\n# Try different subspace dimensions\n# Larger subspace = more features but may overfit\n# Smaller subspace = fewer features but better generalization\nsubspace_dim = 12  # Reduced from 22 to 12 for better generalization\n\nprint(f\"\\nUsing subspace dimension: {subspace_dim}\")\nprint(f\"Output feature dimension: {subspace_dim * (subspace_dim + 1) // 2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline: No Domain Adaptation\n\nFirst, we establish a baseline by training on source features and\ntesting on target features without any adaptation. This shows the\nperformance drop due to domain shift.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Extracting SPD Features (Baseline - No BatchNorm)\")\nprint(\"=\" * 60)\n\n# Create extractor WITHOUT batch normalization for baseline\nextractor_baseline = SPDFeatureExtractor(n_channels, subspace_dim)\nextractor_baseline.eval()\n\n# Extract features (same extractor for both domains to ensure fair comparison)\nfeatures_source = extract_features(extractor_baseline, X_source)\nfeatures_target = extract_features(extractor_baseline, X_target)\n\nprint(f\"Source features shape: {features_source.shape}\")\nprint(f\"Target features shape: {features_target.shape}\")\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_source_scaled = scaler.fit_transform(features_source)\nfeatures_target_scaled = scaler.transform(features_target)\n\n# Train classifier on source\nclf_baseline = LogisticRegression(max_iter=1000, random_state=SEED)\nclf_baseline.fit(features_source_scaled, y_source)\n\n# Evaluate\ny_pred_source = clf_baseline.predict(features_source_scaled)\ny_pred_target_baseline = clf_baseline.predict(features_target_scaled)\n\nsource_acc = accuracy_score(y_source, y_pred_source)\ntarget_acc_baseline = accuracy_score(y_target, y_pred_target_baseline)\ntarget_bal_acc_baseline = balanced_accuracy_score(y_target, y_pred_target_baseline)\n\nprint(\"\\n\" + \"-\" * 50)\nprint(\"Baseline Results (No Domain Adaptation)\")\nprint(\"-\" * 50)\nprint(f\"Source Accuracy:            {source_acc * 100:.2f}%\")\nprint(f\"Target Accuracy:            {target_acc_baseline * 100:.2f}%\")\nprint(f\"Target Balanced Accuracy:   {target_bal_acc_baseline * 100:.2f}%\")\nprint(f\"Performance Drop:           {(source_acc - target_acc_baseline) * 100:.2f}%\")\n\n# Store results for summary\nresults: Dict[str, Dict[str, float]] = {\n    \"No Adaptation\": {\n        \"accuracy\": target_acc_baseline,\n        \"balanced_accuracy\": target_bal_acc_baseline,\n        \"improvement\": 0.0,\n    }\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Domain Shift\n\nBefore applying domain adaptation, let's visualize the distribution\nshift between source and target domains using PCA projection.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_domain_shift_comprehensive(\n    features_source: np.ndarray,\n    features_target: np.ndarray,\n    y_source: np.ndarray,\n    y_target: np.ndarray,\n    class_names: List[str],\n    title: str = \"Domain Shift Visualization\",\n    figsize: Tuple[int, int] = (16, 12),\n) -> plt.Figure:\n    \"\"\"Comprehensive visualization of domain shift.\n\n    Creates a 2x3 grid showing:\n    - Domain distributions (PCA)\n    - Source and target by class\n    - Feature histograms per domain\n    - Class-conditional distributions\n\n    Parameters\n    ----------\n    features_source : np.ndarray\n        Source domain features.\n    features_target : np.ndarray\n        Target domain features.\n    y_source : np.ndarray\n        Source domain labels.\n    y_target : np.ndarray\n        Target domain labels.\n    class_names : List[str]\n        Names of the classes.\n    title : str\n        Overall figure title.\n    figsize : Tuple[int, int]\n        Figure size.\n\n    Returns\n    -------\n    plt.Figure\n        The matplotlib figure.\n    \"\"\"\n    # Combine features for PCA\n    features_all = np.vstack([features_source, features_target])\n    pca = PCA(n_components=2)\n    features_2d = pca.fit_transform(features_all)\n\n    n_source = len(features_source)\n    source_2d = features_2d[:n_source]\n    target_2d = features_2d[n_source:]\n\n    fig, axes = plt.subplots(2, 3, figsize=figsize)\n\n    # --- Row 1 ---\n    # Plot 1: All data by domain\n    ax1 = axes[0, 0]\n    ax1.scatter(\n        source_2d[:, 0],\n        source_2d[:, 1],\n        c=\"blue\",\n        alpha=0.5,\n        label=\"Source\",\n        marker=\"o\",\n        s=40,\n    )\n    ax1.scatter(\n        target_2d[:, 0],\n        target_2d[:, 1],\n        c=\"red\",\n        alpha=0.5,\n        label=\"Target\",\n        marker=\"s\",\n        s=40,\n    )\n    ax1.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0] * 100:.1f}%)\")\n    ax1.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1] * 100:.1f}%)\")\n    ax1.set_title(\"Domain Distribution\", fontweight=\"bold\")\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Plot 2: Source domain by class\n    ax2 = axes[0, 1]\n    n_classes = len(np.unique(y_source))\n    colors = plt.cm.tab10(np.linspace(0, 1, max(n_classes, 4)))[:n_classes]\n    for label_idx, label in enumerate(np.unique(y_source)):\n        mask = y_source == label\n        ax2.scatter(\n            source_2d[mask, 0],\n            source_2d[mask, 1],\n            c=colors[label_idx],\n            alpha=0.6,\n            label=f\"{class_names[label_idx]}\",\n            marker=\"o\",\n            s=40,\n        )\n    ax2.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0] * 100:.1f}%)\")\n    ax2.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1] * 100:.1f}%)\")\n    ax2.set_title(\"Source Domain (by class)\", fontweight=\"bold\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    # Plot 3: Target domain by class\n    ax3 = axes[0, 2]\n    for label_idx, label in enumerate(np.unique(y_target)):\n        mask = y_target == label\n        ax3.scatter(\n            target_2d[mask, 0],\n            target_2d[mask, 1],\n            c=colors[label_idx],\n            alpha=0.6,\n            label=f\"{class_names[label_idx]}\",\n            marker=\"s\",\n            s=40,\n        )\n    ax3.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0] * 100:.1f}%)\")\n    ax3.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1] * 100:.1f}%)\")\n    ax3.set_title(\"Target Domain (by class)\", fontweight=\"bold\")\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n\n    # --- Row 2 ---\n    # Plot 4: Feature histogram (first 3 features)\n    ax4 = axes[1, 0]\n    for feat_idx in range(min(3, features_source.shape[1])):\n        ax4.hist(\n            features_source[:, feat_idx],\n            bins=20,\n            alpha=0.5,\n            label=f\"Source feat {feat_idx}\",\n            density=True,\n        )\n        ax4.hist(\n            features_target[:, feat_idx],\n            bins=20,\n            alpha=0.5,\n            label=f\"Target feat {feat_idx}\",\n            density=True,\n            linestyle=\"--\",\n        )\n    ax4.set_xlabel(\"Feature Value\")\n    ax4.set_ylabel(\"Density\")\n    ax4.set_title(\"Feature Distribution (first 3)\", fontweight=\"bold\")\n    ax4.legend(fontsize=8)\n    ax4.grid(True, alpha=0.3)\n\n    # Plot 5: Class-conditional distributions (Source)\n    ax5 = axes[1, 1]\n    for label_idx, label in enumerate(np.unique(y_source)):\n        mask_s = y_source == label\n        ax5.hist(\n            source_2d[mask_s, 0],\n            bins=15,\n            alpha=0.5,\n            label=f\"Source-{class_names[label_idx]}\",\n            color=colors[label_idx],\n            density=True,\n        )\n    ax5.set_xlabel(\"PC1\")\n    ax5.set_ylabel(\"Density\")\n    ax5.set_title(\"Class Distribution (Source)\", fontweight=\"bold\")\n    ax5.legend()\n    ax5.grid(True, alpha=0.3)\n\n    # Plot 6: Class-conditional distributions (Target)\n    ax6 = axes[1, 2]\n    for label_idx, label in enumerate(np.unique(y_target)):\n        mask_t = y_target == label\n        ax6.hist(\n            target_2d[mask_t, 0],\n            bins=15,\n            alpha=0.5,\n            label=f\"Target-{class_names[label_idx]}\",\n            color=colors[label_idx],\n            density=True,\n        )\n    ax6.set_xlabel(\"PC1\")\n    ax6.set_ylabel(\"Density\")\n    ax6.set_title(\"Class Distribution (Target)\", fontweight=\"bold\")\n    ax6.legend()\n    ax6.grid(True, alpha=0.3)\n\n    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n    plt.tight_layout()\n    return fig\n\n\n# Visualize original domain shift\nclass_names = [str(c) for c in le.classes_]\nfig = plot_domain_shift_comprehensive(\n    features_source,\n    features_target,\n    y_source,\n    y_target,\n    class_names=class_names,\n    title=\"SPD Feature Space - Before Domain Adaptation\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPDBatchNormMeanVar Domain Adaptation\n\nSPDBatchNormMeanVar provides a native Riemannian approach to domain adaptation\nthat operates directly on SPD matrices. The key insight is that\nsubject-to-subject variability manifests as a shift in the Fr\u00e9chet mean\nof the SPD distribution.\n\n**How SPDBatchNormMeanVar adaptation works:**\n\n1. During training on source, SPDBatchNormMeanVar learns running statistics\n   (Fr\u00e9chet mean and dispersion) of the source distribution\n2. For adaptation, we reset the running statistics and pass unlabeled\n   target data through the layer\n3. The new running mean captures the target distribution's center\n4. At inference, target data is normalized to have the same geometric\n   center as source data was during training\n\nThis is **Source-Free Unsupervised Domain Adaptation (SFUDA)** because:\n\n- No source data is needed during adaptation (source-free)\n- No target labels are needed (unsupervised)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"SPDBatchNormMeanVar Domain Adaptation\")\nprint(\"=\" * 60)\n\n# Create extractor WITH batch normalization\nextractor_bn = SPDFeatureExtractorWithBN(n_channels, subspace_dim, bn_momentum=0.1)\n\n# Step 1: Train on source domain to establish baseline statistics\nprint(\"\\nStep 1: Training on source domain...\")\nextractor_bn = train_extractor_on_source(\n    extractor_bn, X_source, n_epochs=5, batch_size=32\n)\n\n# Extract source features (for training classifier)\nfeatures_source_bn = extract_features(extractor_bn, X_source)\n\n# Step 2: Extract target features WITHOUT adaptation (for comparison)\nprint(\"Step 2: Extracting target features without adaptation...\")\nfeatures_target_no_adapt = extract_features(extractor_bn, X_target)\n\n# Step 3: Adapt to target domain\nprint(\"Step 3: Adapting SPDBatchNormMeanVar to target domain...\")\nextractor_bn = adapt_spdbn_to_target(\n    extractor_bn, X_target, n_passes=10, reset_stats=True\n)\n\n# Step 4: Extract target features AFTER adaptation\nprint(\"Step 4: Extracting target features after adaptation...\")\nfeatures_target_bn = extract_features(extractor_bn, X_target)\n\n# Train classifier on source features (with BN)\nscaler_bn = StandardScaler()\nfeatures_source_bn_scaled = scaler_bn.fit_transform(features_source_bn)\nfeatures_target_no_adapt_scaled = scaler_bn.transform(features_target_no_adapt)\nfeatures_target_bn_scaled = scaler_bn.transform(features_target_bn)\n\nclf_spdbn = LogisticRegression(max_iter=1000, random_state=SEED)\nclf_spdbn.fit(features_source_bn_scaled, y_source)\n\n# Evaluate without adaptation\ny_pred_target_no_adapt = clf_spdbn.predict(features_target_no_adapt_scaled)\nacc_no_adapt = accuracy_score(y_target, y_pred_target_no_adapt)\n\n# Evaluate with adaptation\ny_pred_spdbn = clf_spdbn.predict(features_target_bn_scaled)\nspdbn_acc = accuracy_score(y_target, y_pred_spdbn)\nspdbn_bal_acc = balanced_accuracy_score(y_target, y_pred_spdbn)\n\nprint(\"\\n\" + \"-\" * 50)\nprint(\"SPDBatchNormMeanVar Results\")\nprint(\"-\" * 50)\nprint(f\"Before adaptation:        {acc_no_adapt * 100:.2f}%\")\nprint(f\"After adaptation:         {spdbn_acc * 100:.2f}%\")\nprint(f\"Balanced Accuracy:        {spdbn_bal_acc * 100:.2f}%\")\nprint(f\"Improvement:              {(spdbn_acc - acc_no_adapt) * 100:+.2f}%\")\nprint(f\"vs Baseline (no BN):      {(spdbn_acc - target_acc_baseline) * 100:+.2f}%\")\n\nresults[\"SPDBatchNormMeanVar\"] = {\n    \"accuracy\": spdbn_acc,\n    \"balanced_accuracy\": spdbn_bal_acc,\n    \"improvement\": spdbn_acc - target_acc_baseline,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Adapted Features\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_domain_shift_comprehensive(\n    features_source_bn,\n    features_target_bn,\n    y_source,\n    y_target,\n    class_names=class_names,\n    title=\"SPD Feature Space - After SPDBatchNormMeanVar Adaptation\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Domain Adaptation with skada\n\nIf skada is installed, we can compare SPDBatchNormMeanVar with other domain\nadaptation methods. These methods operate on the Euclidean features\n(after LogEig projection) rather than directly on SPD matrices.\n\n**Key concept:** skada uses `sample_domain` to distinguish domains:\n\n- **Positive values (1)**: Source domain samples\n- **Negative values (-1)**: Target domain samples\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"skada Domain Adaptation Methods\")\nprint(\"=\" * 60)\n\n# Prepare data in skada format\nX_combined = np.vstack([features_source, features_target])\ny_combined = np.concatenate([y_source, -np.ones(len(y_target))])\nsample_domain = np.concatenate(\n    [np.ones(len(features_source)), -np.ones(len(features_target))]\n)\n\nprint(\"\\nData prepared for skada:\")\nprint(f\"  Combined features shape: {X_combined.shape}\")\nprint(f\"  Source samples (domain=1): {int(np.sum(sample_domain > 0))}\")\nprint(f\"  Target samples (domain=-1): {int(np.sum(sample_domain < 0))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CORAL (Correlation Alignment)\n\nCORAL :cite:p:`sun2016coral` aligns the second-order statistics (covariance)\nof the\nsource and target feature distributions. This is particularly\nsuitable for SPD-derived features since they already capture\ncovariance structure.\n\n\\begin{align}\\min_A \\|A^T C_S A - C_T\\|_F^2\\end{align}\n\nwhere $C_S$ and $C_T$ are the covariance matrices of\nsource and target features, respectively.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"-\" * 50)\nprint(\"CORAL (Correlation Alignment)\")\nprint(\"-\" * 50)\n\ncoral_pipeline = make_da_pipeline(\n    StandardScaler(),\n    CORALAdapter(reg=1e-3),\n    LogisticRegression(max_iter=1000, random_state=SEED),\n)\n\ncoral_pipeline.fit(X_combined, y_combined, sample_domain=sample_domain)\ny_pred_coral = coral_pipeline.predict(features_target)\n\ncoral_acc = accuracy_score(y_target, y_pred_coral)\ncoral_bal_acc = balanced_accuracy_score(y_target, y_pred_coral)\n\nprint(f\"Target Accuracy:          {coral_acc * 100:.2f}%\")\nprint(f\"Target Balanced Accuracy: {coral_bal_acc * 100:.2f}%\")\nprint(f\"Improvement over baseline: {(coral_acc - target_acc_baseline) * 100:+.2f}%\")\n\nresults[\"CORAL\"] = {\n    \"accuracy\": coral_acc,\n    \"balanced_accuracy\": coral_bal_acc,\n    \"improvement\": coral_acc - target_acc_baseline,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subspace Alignment\n\nSubspace Alignment :cite:p:`fernando2013subspace` learns a linear\ntransformation that aligns\nthe principal subspaces of source and target domains.\n\nThe method:\n\n1. Computes PCA bases for source and target domains\n2. Finds optimal rotation to align the subspaces\n3. Projects source data into the aligned space\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"-\" * 50)\nprint(\"Subspace Alignment\")\nprint(\"-\" * 50)\n\nsa_clf = SubspaceAlignment(\n    base_estimator=LogisticRegression(max_iter=1000, random_state=SEED),\n    n_components=min(10, features_source.shape[1]),\n)\n\nsa_clf.fit(X_combined, y_combined, sample_domain=sample_domain)\ny_pred_sa = sa_clf.predict(features_target)\n\nsa_acc = accuracy_score(y_target, y_pred_sa)\nsa_bal_acc = balanced_accuracy_score(y_target, y_pred_sa)\n\nprint(f\"Target Accuracy:          {sa_acc * 100:.2f}%\")\nprint(f\"Target Balanced Accuracy: {sa_bal_acc * 100:.2f}%\")\nprint(f\"Improvement over baseline: {(sa_acc - target_acc_baseline) * 100:+.2f}%\")\n\nresults[\"Subspace Alignment\"] = {\n    \"accuracy\": sa_acc,\n    \"balanced_accuracy\": sa_bal_acc,\n    \"improvement\": sa_acc - target_acc_baseline,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entropic Optimal Transport\n\nOptimal Transport :cite:p:`courty2017optimal` finds the minimum cost mapping\nbetween source\nand target distributions. The entropic regularization makes the\noptimization problem tractable and provides smoother mappings.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"-\" * 50)\nprint(\"Entropic Optimal Transport\")\nprint(\"-\" * 50)\n\ntry:\n    ot_clf = EntropicOTMapping(\n        base_estimator=LogisticRegression(max_iter=1000, random_state=SEED),\n        reg_e=1.0,\n    )\n\n    ot_clf.fit(X_combined, y_combined, sample_domain=sample_domain)\n    y_pred_ot = ot_clf.predict(features_target)\n\n    ot_acc = accuracy_score(y_target, y_pred_ot)\n    ot_bal_acc = balanced_accuracy_score(y_target, y_pred_ot)\n\n    print(f\"Target Accuracy:          {ot_acc * 100:.2f}%\")\n    print(f\"Target Balanced Accuracy: {ot_bal_acc * 100:.2f}%\")\n    print(f\"Improvement over baseline: {(ot_acc - target_acc_baseline) * 100:+.2f}%\")\n\n    results[\"Entropic OT\"] = {\n        \"accuracy\": ot_acc,\n        \"balanced_accuracy\": ot_bal_acc,\n        \"improvement\": ot_acc - target_acc_baseline,\n    }\nexcept Exception as e:\n    print(f\"OT method failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrices\n\nLet's visualize the confusion matrices to understand where each\nmethod makes mistakes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrices(\n    y_true: np.ndarray,\n    predictions: Dict[str, np.ndarray],\n    class_names: List[str],\n    figsize: Tuple[int, int] = (15, 5),\n) -> plt.Figure:\n    \"\"\"Plot confusion matrices for multiple methods.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        True labels.\n    predictions : Dict[str, np.ndarray]\n        Dictionary mapping method names to predicted labels.\n    class_names : List[str]\n        Names of classes.\n    figsize : Tuple[int, int]\n        Figure size.\n\n    Returns\n    -------\n    plt.Figure\n        The matplotlib figure.\n    \"\"\"\n    n_methods = len(predictions)\n    fig, axes = plt.subplots(1, n_methods, figsize=figsize)\n\n    if n_methods == 1:\n        axes = [axes]\n\n    for ax, (method_name, y_pred) in zip(axes, predictions.items()):\n        cm = confusion_matrix(y_true, y_pred)\n        cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n\n        im = ax.imshow(cm_normalized, interpolation=\"nearest\", cmap=plt.cm.Blues)\n        ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n        ax.set(\n            xticks=np.arange(len(class_names)),\n            yticks=np.arange(len(class_names)),\n            xticklabels=class_names,\n            yticklabels=class_names,\n            xlabel=\"Predicted\",\n            ylabel=\"True\",\n        )\n\n        # Add text annotations\n        thresh = cm_normalized.max() / 2.0\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                ax.text(\n                    j,\n                    i,\n                    f\"{cm[i, j]}\\n({cm_normalized[i, j]:.1%})\",\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"white\" if cm_normalized[i, j] > thresh else \"black\",\n                    fontsize=9,\n                )\n\n        acc = accuracy_score(y_true, y_pred)\n        ax.set_title(f\"{method_name}\\nAcc: {acc * 100:.1f}%\", fontweight=\"bold\")\n\n    plt.suptitle(\"Confusion Matrices - Target Domain\", fontsize=14, fontweight=\"bold\")\n    plt.tight_layout()\n    return fig\n\n\n# Prepare predictions for confusion matrix plot\npredictions_dict = {\n    \"No Adaptation\": y_pred_target_baseline,\n    \"SPDBatchNormMeanVar\": y_pred_spdbn,\n}\n\npredictions_dict[\"CORAL\"] = y_pred_coral\npredictions_dict[\"Subspace Align\"] = y_pred_sa\nif \"Entropic OT\" in results:\n    predictions_dict[\"Entropic OT\"] = y_pred_ot\n\nfig = plot_confusion_matrices(y_target, predictions_dict, class_names)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n\nCompare all domain adaptation methods in a comprehensive table.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\nprint(\"Domain Adaptation Results Summary\")\nprint(\"=\" * 70)\nprint(f\"{'Method':<25} {'Accuracy':>12} {'Balanced Acc':>14} {'Improvement':>12}\")\nprint(\"-\" * 70)\n\nfor method_name, method_results in results.items():\n    acc = method_results[\"accuracy\"] * 100\n    bal_acc = method_results[\"balanced_accuracy\"] * 100\n    imp = method_results[\"improvement\"] * 100\n    imp_str = f\"{imp:+.2f}%\" if method_name != \"No Adaptation\" else \"-\"\n    print(f\"{method_name:<25} {acc:>10.2f}% {bal_acc:>12.2f}% {imp_str:>12}\")\n\nprint(\"-\" * 70)\nprint(f\"{'Chance Level':<25} {'50.00%':>12}\")\nprint(f\"{'Source Accuracy':<25} {source_acc * 100:>10.2f}%\")\n\n# Find best method\nbest_method = max(results.keys(), key=lambda k: results[k][\"accuracy\"])\nprint(f\"\\nBest method: {best_method} ({results[best_method]['accuracy'] * 100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy Comparison Bar Chart\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_accuracy_comparison(\n    results: Dict[str, Dict[str, float]],\n    source_acc: float,\n    figsize: Tuple[int, int] = (12, 6),\n) -> plt.Figure:\n    \"\"\"Plot accuracy comparison bar chart.\n\n    Parameters\n    ----------\n    results : Dict[str, Dict[str, float]]\n        Results dictionary with accuracy for each method.\n    source_acc : float\n        Source domain accuracy.\n    figsize : Tuple[int, int]\n        Figure size.\n\n    Returns\n    -------\n    plt.Figure\n        The matplotlib figure.\n    \"\"\"\n    methods = list(results.keys())\n    accuracies = [results[m][\"accuracy\"] * 100 for m in methods]\n    improvements = [results[m][\"improvement\"] * 100 for m in methods]\n\n    # Color scheme\n    colors = []\n    for imp in improvements:\n        if imp == 0:\n            colors.append(\"#e74c3c\")  # Red for baseline\n        elif imp > 0:\n            colors.append(\"#2ecc71\")  # Green for positive\n        else:\n            colors.append(\"#f39c12\")  # Orange for negative\n\n    fig, ax = plt.subplots(figsize=figsize)\n\n    bars = ax.bar(methods, accuracies, color=colors, edgecolor=\"black\", linewidth=1.5)\n\n    # Add value labels\n    for bar, acc, imp in zip(bars, accuracies, improvements):\n        label = f\"{acc:.1f}%\"\n        if imp != 0:\n            label += f\"\\n({imp:+.1f}%)\"\n        ax.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 1,\n            label,\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=10,\n            fontweight=\"bold\",\n        )\n\n    # Reference lines\n    ax.axhline(y=50, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"Chance (50%)\")\n    ax.axhline(\n        y=source_acc * 100,\n        color=\"blue\",\n        linestyle=\":\",\n        alpha=0.7,\n        label=f\"Source Acc ({source_acc * 100:.1f}%)\",\n    )\n\n    ax.set_ylabel(\"Target Accuracy (%)\", fontsize=12)\n    ax.set_xlabel(\"Domain Adaptation Method\", fontsize=12)\n    ax.set_title(\n        \"Domain Adaptation Method Comparison\\n(Cross-Subject EEG Classification)\",\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n    ax.set_ylim([0, 110])\n    ax.legend(loc=\"lower right\")\n    ax.grid(True, alpha=0.3, axis=\"y\")\n\n    # Rotate x-axis labels if many methods\n    if len(methods) > 4:\n        plt.xticks(rotation=15, ha=\"right\")\n\n    plt.tight_layout()\n    return fig\n\n\nfig = plot_accuracy_comparison(results, source_acc)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before vs After Adaptation Comparison\n\nDirect visual comparison of feature distributions before and after\nSPDBatchNormMeanVar adaptation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_before_after_comparison(\n    features_source: np.ndarray,\n    features_target_before: np.ndarray,\n    features_target_after: np.ndarray,\n    y_source: np.ndarray,\n    y_target: np.ndarray,\n    figsize: Tuple[int, int] = (14, 5),\n) -> plt.Figure:\n    \"\"\"Plot before/after comparison of domain adaptation.\n\n    Parameters\n    ----------\n    features_source : np.ndarray\n        Source domain features.\n    features_target_before : np.ndarray\n        Target features before adaptation.\n    features_target_after : np.ndarray\n        Target features after adaptation.\n    y_source : np.ndarray\n        Source labels.\n    y_target : np.ndarray\n        Target labels.\n    figsize : Tuple[int, int]\n        Figure size.\n\n    Returns\n    -------\n    plt.Figure\n        The matplotlib figure.\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=figsize)\n\n    # Fit PCA on all data\n    all_features = np.vstack(\n        [features_source, features_target_before, features_target_after]\n    )\n    pca = PCA(n_components=2)\n    pca.fit(all_features)\n\n    source_2d = pca.transform(features_source)\n    target_before_2d = pca.transform(features_target_before)\n    target_after_2d = pca.transform(features_target_after)\n\n    # Plot 1: Source distribution\n    ax1 = axes[0]\n    ax1.scatter(\n        source_2d[:, 0],\n        source_2d[:, 1],\n        c=\"blue\",\n        alpha=0.6,\n        label=\"Source\",\n        s=30,\n    )\n    ax1.scatter(\n        source_2d[:, 0].mean(),\n        source_2d[:, 1].mean(),\n        c=\"blue\",\n        s=200,\n        marker=\"*\",\n        edgecolors=\"black\",\n        label=\"Source mean\",\n    )\n    ax1.set_xlabel(\"PC1\")\n    ax1.set_ylabel(\"PC2\")\n    ax1.set_title(\"Source Domain\", fontweight=\"bold\")\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Plot 2: Before adaptation\n    ax2 = axes[1]\n    ax2.scatter(\n        source_2d[:, 0],\n        source_2d[:, 1],\n        c=\"blue\",\n        alpha=0.3,\n        label=\"Source\",\n        s=20,\n    )\n    ax2.scatter(\n        target_before_2d[:, 0],\n        target_before_2d[:, 1],\n        c=\"red\",\n        alpha=0.6,\n        label=\"Target\",\n        s=30,\n    )\n    # Mark means\n    ax2.scatter(\n        source_2d[:, 0].mean(),\n        source_2d[:, 1].mean(),\n        c=\"blue\",\n        s=200,\n        marker=\"*\",\n        edgecolors=\"black\",\n    )\n    ax2.scatter(\n        target_before_2d[:, 0].mean(),\n        target_before_2d[:, 1].mean(),\n        c=\"red\",\n        s=200,\n        marker=\"*\",\n        edgecolors=\"black\",\n    )\n    ax2.set_xlabel(\"PC1\")\n    ax2.set_ylabel(\"PC2\")\n    ax2.set_title(\"Before Adaptation\", fontweight=\"bold\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    # Compute domain shift (distance between means)\n    shift_before = np.linalg.norm(\n        source_2d.mean(axis=0) - target_before_2d.mean(axis=0)\n    )\n    ax2.text(\n        0.05,\n        0.95,\n        f\"Shift: {shift_before:.2f}\",\n        transform=ax2.transAxes,\n        fontsize=10,\n        verticalalignment=\"top\",\n        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n    )\n\n    # Plot 3: After adaptation\n    ax3 = axes[2]\n    ax3.scatter(\n        source_2d[:, 0],\n        source_2d[:, 1],\n        c=\"blue\",\n        alpha=0.3,\n        label=\"Source\",\n        s=20,\n    )\n    ax3.scatter(\n        target_after_2d[:, 0],\n        target_after_2d[:, 1],\n        c=\"green\",\n        alpha=0.6,\n        label=\"Target (adapted)\",\n        s=30,\n    )\n    # Mark means\n    ax3.scatter(\n        source_2d[:, 0].mean(),\n        source_2d[:, 1].mean(),\n        c=\"blue\",\n        s=200,\n        marker=\"*\",\n        edgecolors=\"black\",\n    )\n    ax3.scatter(\n        target_after_2d[:, 0].mean(),\n        target_after_2d[:, 1].mean(),\n        c=\"green\",\n        s=200,\n        marker=\"*\",\n        edgecolors=\"black\",\n    )\n    ax3.set_xlabel(\"PC1\")\n    ax3.set_ylabel(\"PC2\")\n    ax3.set_title(\"After Adaptation\", fontweight=\"bold\")\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n\n    shift_after = np.linalg.norm(source_2d.mean(axis=0) - target_after_2d.mean(axis=0))\n    ax3.text(\n        0.05,\n        0.95,\n        f\"Shift: {shift_after:.2f}\",\n        transform=ax3.transAxes,\n        fontsize=10,\n        verticalalignment=\"top\",\n        bbox=dict(boxstyle=\"round\", facecolor=\"lightgreen\", alpha=0.5),\n    )\n\n    plt.suptitle(\n        \"SPDBatchNormMeanVar Domain Adaptation Effect\",\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n    plt.tight_layout()\n    return fig\n\n\n# We need to re-extract features for proper comparison\n# Reset and re-train for clean comparison\nextractor_compare = SPDFeatureExtractorWithBN(n_channels, subspace_dim, bn_momentum=0.1)\nextractor_compare = train_extractor_on_source(extractor_compare, X_source, n_epochs=5)\n\n# Get features before adaptation\nfeatures_source_compare = extract_features(extractor_compare, X_source)\nfeatures_target_before_compare = extract_features(extractor_compare, X_target)\n\n# Adapt and get features after\nextractor_compare = adapt_spdbn_to_target(extractor_compare, X_target, n_passes=10)\nfeatures_target_after_compare = extract_features(extractor_compare, X_target)\n\nfig = plot_before_after_comparison(\n    features_source_compare,\n    features_target_before_compare,\n    features_target_after_compare,\n    y_source,\n    y_target,\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Table\n\nComprehensive summary of all methods with their characteristics.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\nprint(\"Method Summary Table\")\nprint(\"=\" * 80)\nprint(f\"{'Method':<20} {'Type':<15} {'Requires':<15} {'Accuracy':>10} {'Notes':<20}\")\nprint(\"-\" * 80)\n\nmethod_info = {\n    \"No Adaptation\": (\"Baseline\", \"Nothing\", \"Direct transfer\"),\n    \"SPDBatchNormMeanVar\": (\"Riemannian\", \"Unlabeled target\", \"Native SPD adaptation\"),\n}\nmethod_info.update(\n    {\n        \"CORAL\": (\"Statistical\", \"Unlabeled target\", \"Covariance alignment\"),\n        \"Subspace Alignment\": (\"Geometric\", \"Unlabeled target\", \"PCA-based\"),\n        \"Entropic OT\": (\"Optimal Transport\", \"Unlabeled target\", \"Sample mapping\"),\n    }\n)\n\nfor method_name, (method_type, requires, notes) in method_info.items():\n    if method_name in results:\n        acc = results[method_name][\"accuracy\"] * 100\n        print(\n            f\"{method_name:<20} {method_type:<15} {requires:<15} {acc:>8.1f}% {notes}\"\n        )\n\nprint(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n\n**Key insights from this tutorial:**\n\n**SPD Features for Domain Adaptation:**\n\n- SPD-based features capture the geometric structure of EEG covariance\n  matrices, which encode both power and connectivity information\n- The tangent space projection (LogEig) provides Euclidean features\n  suitable for standard domain adaptation methods\n- SPDBatchNormMeanVar provides a native Riemannian approach that operates\n  directly on the SPD manifold\n\n**Method Comparison:**\n\n- **SPDBatchNormMeanVar** adapts the Fr\u00e9chet mean of the SPD distribution,\n  which is the natural center on the Riemannian manifold\n- **CORAL** aligns covariance matrices in the Euclidean feature space,\n  complementing the geometric features\n- **Subspace Alignment** finds a shared linear subspace, useful when\n  domains have similar structure but different orientations\n- **Optimal Transport** provides principled sample-to-sample mapping\n\n**Practical Recommendations:**\n\n1. **Start with SPDBatchNormMeanVar** for SPD networks (native, no extra\n   dependencies, efficient)\n2. **Use CORAL** when second-order alignment is sufficient\n3. **Try Subspace Alignment** for high-dimensional scenarios\n4. **Combine multiple methods** for robust performance\n\n**Limitations:**\n\n- Domain adaptation assumes source and target share the same classes\n- Large distribution shifts may require more sophisticated methods\n- Performance depends on the quality of the source model\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}