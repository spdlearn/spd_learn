{
  "cells": [
    {
      "id": "6e7a2919",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install -q spd_learn moabb braindecode scikit-learn matplotlib\n\n# For GPU support (recommended for faster training)\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Cross-Dataset EEG Classification with SPDNet\n\nThis tutorial demonstrates **cross-dataset generalization** using SPDNet\nfor motor imagery EEG classification. We use Leave-One-Dataset-Out (LODO)\ncross-validation to evaluate how well the model transfers between\ndifferent EEG recording setups.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nCross-dataset generalization is one of the most challenging problems\nin EEG-based brain-computer interfaces. Different datasets vary in:\n\n- **Recording equipment**: Different amplifiers, electrode types\n- **Electrode configurations**: Varying channel counts and placements\n- **Subject populations**: Age, experience, health status\n- **Experimental protocols**: Task instructions, timing, feedback\n\nSPDNet's geometric approach :cite:p:`huang2017riemannian` operating on the\nSPD manifold can help learn representations that are more robust to these\nvariations, since covariance matrices capture second-order statistics that\nare somewhat invariant to amplitude scaling differences.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This is a **challenging benchmark**. Even small positive transfer\n   indicates that the model has learned generalizable features rather\n   than dataset-specific artifacts.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom braindecode import EEGClassifier\nfrom braindecode.datasets import MOABBDataset\nfrom braindecode.preprocessing import (\n    Pick,\n    Preprocessor,\n    create_windows_from_events,\n    exponential_moving_standardize,\n    preprocess,\n)\nfrom braindecode.util import set_random_seeds\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\nfrom sklearn.utils import check_random_state\nfrom skorch.callbacks import EpochScoring, GradientNormClipping\nfrom skorch.dataset import ValidSplit\nfrom skorch.helper import SliceDataset\nfrom torch.utils.data import ConcatDataset\n\nfrom spd_learn.models import SPDNet\n\n\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\nWe configure the experiment with fixed random seeds for reproducibility.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 42\nset_random_seeds(seed, cuda=False)\nrandom_state = check_random_state(seed)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Datasets\n\nWe load two motor imagery datasets from MOABB:\n\n- **BNCI2014_001** (BCI Competition IV 2a) :cite:p:`tangermann2012review`:\n  9 subjects, 22 channels, 4-class motor imagery (left hand, right hand,\n  feet, tongue)\n- **Zhou2016** :cite:p:`zhou2016fully`: 4 subjects, 14 channels, 3-class\n  motor imagery\n\nFor cross-dataset transfer, we need to:\n\n1. Select **common channels** available in both datasets\n2. Use **common classes** (left hand vs right hand)\n3. Apply **consistent preprocessing** (same frequency band, sampling rate)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache_config = dict(\n    use=True,\n    save_raw=True,\n    save_epochs=False,\n    save_array=False,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n    verbose=False,\n)\n\n# Load BNCI2014_001 dataset\n# Using subset of subjects for faster demonstration\nprint(\"Loading BNCI2014_001 dataset...\")\nbnci2014_01 = MOABBDataset(\n    \"BNCI2014_001\",\n    subject_ids=[1, 2],  # 2 subjects for demonstration\n    dataset_load_kwargs={\"cache_config\": cache_config},\n)\n\n# Load Zhou2016 dataset\nprint(\"Loading Zhou2016 dataset...\")\n# Note: Zhou2016 has different subject ID numbering, loading all available subjects\nzhou2016 = MOABBDataset(\n    \"Zhou2016\",\n    subject_ids=None,  # Load all available subjects\n    dataset_load_kwargs={\"cache_config\": cache_config},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing Pipeline\n\nWe apply a standardized preprocessing pipeline to both datasets:\n\n1. **Channel selection**: Pick channels common to both datasets (C3, Cz, C4)\n   - These are central motor cortex electrodes, most relevant for\n   motor imagery classification\n\n2. **Resampling**: Downsample to 125 Hz\n   - Reduces computational cost while preserving relevant frequencies\n\n3. **Amplitude scaling**: Convert to microvolts\n   - Standardizes units across datasets\n\n4. **Bandpass filtering**: 4-38 Hz\n   - Includes mu (8-12 Hz) and beta (13-30 Hz) bands relevant for\n   motor imagery\n\n5. **Exponential moving standardization**\n   - Adaptive normalization that handles non-stationarity\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(dataset, target_sfreq=125):\n    \"\"\"Apply standardized preprocessing pipeline.\n\n    Parameters\n    ----------\n    dataset : BaseConcatDataset\n        Raw braindecode dataset.\n    target_sfreq : float\n        Target sampling frequency in Hz.\n\n    Returns\n    -------\n    BaseConcatDataset\n        Preprocessed dataset.\n\n    Notes\n    -----\n    The preprocessing steps are chosen to maximize compatibility\n    across different recording setups while preserving the neural\n    information relevant for motor imagery classification.\n    \"\"\"\n    preprocessors = [\n        # Select motor cortex channels common to both datasets\n        Pick(picks=[\"C3\", \"Cz\", \"C4\"]),\n        # Resample to common frequency\n        Preprocessor(\"resample\", sfreq=target_sfreq),\n        # Convert to microvolts (standardize amplitude units)\n        Preprocessor(lambda data: np.multiply(data, 1e6)),\n        # Bandpass filter for motor imagery bands (mu + beta)\n        Preprocessor(\"filter\", l_freq=4.0, h_freq=38.0, verbose=False),\n        # Exponential moving standardization for non-stationarity\n        Preprocessor(\n            exponential_moving_standardize,\n            factor_new=1e-3,\n            init_block_size=1000,\n        ),\n    ]\n    return preprocess(dataset, preprocessors, n_jobs=1)\n\n\nprint(\"\\nPreprocessing datasets...\")\nprint(\"  - Selecting channels: C3, Cz, C4\")\nprint(\"  - Resampling to 125 Hz\")\nprint(\"  - Bandpass filtering: 4-38 Hz\")\n\nbnci2014_01 = preprocess_dataset(bnci2014_01)\nzhou2016 = preprocess_dataset(zhou2016)\n\nprint(\"Preprocessing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Windows\n\nWe extract 1-second windows from the continuous EEG data.\nUsing fixed-size windows ensures consistent input dimensions\nfor the neural network.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Label mapping: only use left/right hand (common across datasets)\nmapping = {\"left_hand\": 0, \"right_hand\": 1}\n\nwindow_size_samples = 125  # 1 second at 125 Hz\nwindow_stride_samples = 125  # Non-overlapping windows\n\nprint(\n    f\"\\nCreating windows (size={window_size_samples} samples, stride={window_stride_samples})...\"\n)\n\nwindows_bnci2014_01 = create_windows_from_events(\n    bnci2014_01,\n    window_size_samples=window_size_samples,\n    window_stride_samples=window_stride_samples,\n    preload=True,\n    mapping=mapping,\n    n_jobs=1,\n)\n\nwindows_zhou2016 = create_windows_from_events(\n    zhou2016,\n    preload=True,\n    window_size_samples=window_size_samples,\n    window_stride_samples=window_stride_samples,\n    mapping=mapping,\n    n_jobs=1,\n)\n\n# Prepare dataset list for cross-validation\ndataset_list = [windows_bnci2014_01, windows_zhou2016]\ndataset_names = [\"BNCI2014_001\", \"Zhou2016\"]\n\nprint(f\"  BNCI2014_001: {len(windows_bnci2014_01)} windows\")\nprint(f\"  Zhou2016: {len(windows_zhou2016)} windows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPDNet Model Architecture\n\nSPDNet processes data through the following stages:\n\n1. **Input**: Raw EEG signal $X \\in \\mathbb{R}^{C \\times T}$\n\n2. **Covariance computation**: Inside the network, covariance is computed\n\n   .. math::\n\n      \\Sigma = \\frac{1}{T-1} X X^T \\in \\text{SPD}(C)\n\n3. **BiMap layers**: Dimensionality reduction while preserving SPD structure\n\n   .. math::\n\n      Y = W^T \\Sigma W, \\quad W \\in \\mathbb{R}^{C_{in} \\times C_{out}}\n\n4. **ReEig layers**: Non-linear activation via eigenvalue rectification\n\n   .. math::\n\n      Y = U \\max(\\Lambda, \\epsilon) U^T\n\n5. **LogEig layer**: Project to tangent space for linear classification\n\n   .. math::\n\n      y = \\text{vec}(\\log(Y))\n\nFor cross-dataset transfer, the geometric operations on the SPD manifold\nhelp learn features that are more invariant to dataset-specific variations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_chans = 3  # C3, Cz, C4\nn_classes = 2  # Left hand vs right hand\n\nprint(\"\\nModel configuration:\")\nprint(f\"  Input channels: {n_chans}\")\nprint(f\"  Output classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leave-One-Dataset-Out Cross-Validation\n\nWe perform Leave-One-Dataset-Out (LODO) cross-validation:\n\n- **Fold 1**: Train on Zhou2016, test on BNCI2014_001\n- **Fold 2**: Train on BNCI2014_001, test on Zhou2016\n\nThis evaluates whether the model can learn features that generalize\nacross recording setups, rather than overfitting to dataset-specific\ncharacteristics.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\nbatch_size = 64\nmax_epochs = 50  # More epochs for better convergence\nlearning_rate = 1e-3\n\nresults = []\nfold_histories = []\n\nfor fold_idx in range(len(dataset_list)):\n    # Split datasets\n    test_set = dataset_list[fold_idx]\n    train_sets = [ds for j, ds in enumerate(dataset_list) if j != fold_idx]\n\n    # Combine training sets\n    train_set = ConcatDataset(train_sets)\n    y_train = np.array(list(SliceDataset(train_set, 1)))\n    y_test = np.array(list(SliceDataset(test_set, 1)))\n\n    print(f\"\\n{'=' * 60}\")\n    print(f\"Fold {fold_idx + 1}: Test on {dataset_names[fold_idx]}\")\n    print(f\"{'=' * 60}\")\n    print(\n        f\"Train: {len(train_set)} samples from {[n for j, n in enumerate(dataset_names) if j != fold_idx]}\"\n    )\n    print(f\"Test:  {len(test_set)} samples from {dataset_names[fold_idx]}\")\n\n    # Create fresh SPDNet model for each fold\n    model = SPDNet(\n        n_chans=n_chans,\n        n_outputs=n_classes,\n    )\n\n    # Create classifier with braindecode\n    clf = EEGClassifier(\n        module=model,\n        criterion=torch.nn.CrossEntropyLoss,\n        optimizer=torch.optim.AdamW,\n        optimizer__lr=learning_rate,\n        optimizer__weight_decay=1e-4,\n        train_split=ValidSplit(0.2, stratified=True, random_state=random_state),\n        batch_size=batch_size,\n        max_epochs=max_epochs,\n        callbacks=[\n            (\n                \"train_acc\",\n                EpochScoring(\n                    \"accuracy\", lower_is_better=False, on_train=True, name=\"train_acc\"\n                ),\n            ),\n            (\"gradient_clip\", GradientNormClipping(gradient_clip_value=1.0)),\n        ],\n        device=device,\n        classes=list(range(n_classes)),\n        verbose=1,\n    )\n\n    # Train the model\n    clf.fit(train_set, y=y_train)\n    fold_histories.append(clf.history)\n\n    # Evaluate on test set\n    y_pred = clf.predict(test_set)\n    test_accuracy = accuracy_score(y_test, y_pred)\n    test_balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n    test_confusion = confusion_matrix(y_test, y_pred)\n\n    results.append(\n        {\n            \"fold\": fold_idx + 1,\n            \"test_dataset\": dataset_names[fold_idx],\n            \"train_datasets\": [n for j, n in enumerate(dataset_names) if j != fold_idx],\n            \"accuracy\": test_accuracy,\n            \"balanced_accuracy\": test_balanced_accuracy,\n            \"confusion_matrix\": test_confusion,\n            \"n_train\": len(train_set),\n            \"n_test\": len(test_set),\n        }\n    )\n\n    print(f\"\\nFold {fold_idx + 1} Results:\")\n    print(f\"  Accuracy:          {test_accuracy:.4f}\")\n    print(f\"  Balanced Accuracy: {test_balanced_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n\nWe summarize the cross-dataset classification results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Cross-Dataset Classification Results\")\nprint(\"=\" * 60)\n\nfor r in results:\n    print(\n        f\"\\nFold {r['fold']}: Train on {r['train_datasets']} \u2192 Test on {r['test_dataset']}\"\n    )\n    print(f\"  Accuracy:          {r['accuracy']:.4f}\")\n    print(f\"  Balanced Accuracy: {r['balanced_accuracy']:.4f}\")\n\nmean_acc = np.mean([r[\"accuracy\"] for r in results])\nmean_bal_acc = np.mean([r[\"balanced_accuracy\"] for r in results])\nprint(\"\\nOverall Performance:\")\nprint(f\"  Mean Accuracy:          {mean_acc:.4f}\")\nprint(f\"  Mean Balanced Accuracy: {mean_bal_acc:.4f}\")\nprint(\"  Chance Level:           0.5000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Results\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Training curves for each fold\nax1 = axes[0, 0]\nfor fold_idx, history in enumerate(fold_histories):\n    epochs = range(1, len(history) + 1)\n    ax1.plot(\n        epochs,\n        history[:, \"train_loss\"],\n        linestyle=\"-\",\n        label=f\"Fold {fold_idx + 1} Train\",\n        alpha=0.8,\n    )\n    ax1.plot(\n        epochs,\n        history[:, \"valid_loss\"],\n        linestyle=\"--\",\n        label=f\"Fold {fold_idx + 1} Valid\",\n        alpha=0.8,\n    )\nax1.set_xlabel(\"Epoch\", fontsize=12)\nax1.set_ylabel(\"Loss\", fontsize=12)\nax1.set_title(\"Training and Validation Loss\", fontsize=14)\nax1.legend(fontsize=9)\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Accuracy comparison\nax2 = axes[0, 1]\nx_pos = np.arange(len(results))\nbar_width = 0.35\n\nbars1 = ax2.bar(\n    x_pos - bar_width / 2,\n    [r[\"accuracy\"] for r in results],\n    bar_width,\n    label=\"Accuracy\",\n    color=\"#3498db\",\n    edgecolor=\"black\",\n)\nbars2 = ax2.bar(\n    x_pos + bar_width / 2,\n    [r[\"balanced_accuracy\"] for r in results],\n    bar_width,\n    label=\"Balanced Accuracy\",\n    color=\"#2ecc71\",\n    edgecolor=\"black\",\n)\n\nax2.axhline(y=0.5, color=\"red\", linestyle=\"--\", label=\"Chance\", alpha=0.7)\nax2.set_xlabel(\"Test Dataset\", fontsize=12)\nax2.set_ylabel(\"Score\", fontsize=12)\nax2.set_title(\"Cross-Dataset Classification Performance\", fontsize=14)\nax2.set_xticks(x_pos)\nax2.set_xticklabels([r[\"test_dataset\"] for r in results])\nax2.set_ylim([0, 1])\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3, axis=\"y\")\n\n# Add value labels on bars\nfor bar in bars1:\n    height = bar.get_height()\n    ax2.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.02,\n        f\"{height:.2f}\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=9,\n    )\nfor bar in bars2:\n    height = bar.get_height()\n    ax2.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.02,\n        f\"{height:.2f}\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=9,\n    )\n\n# Plot 3 & 4: Confusion matrices for each fold\nfor fold_idx, r in enumerate(results):\n    ax = axes[1, fold_idx]\n    cm = r[\"confusion_matrix\"]\n    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n    ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n    # Add text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(\n                j,\n                i,\n                format(cm[i, j], \"d\"),\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n                fontsize=14,\n            )\n\n    ax.set_xlabel(\"Predicted Label\", fontsize=11)\n    ax.set_ylabel(\"True Label\", fontsize=11)\n    ax.set_title(\n        f\"Fold {fold_idx + 1}: Test on {r['test_dataset']}\\nAcc: {r['accuracy']:.2%}\",\n        fontsize=12,\n    )\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xticklabels([\"Left Hand\", \"Right Hand\"])\n    ax.set_yticklabels([\"Left Hand\", \"Right Hand\"])\n\nplt.tight_layout()\nplt.suptitle(\"Cross-Dataset Transfer Learning Results\", fontsize=16, y=1.02)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n\nCross-dataset generalization is inherently challenging due to the\nsignificant differences between recording setups. Key observations:\n\n**Why SPDNet helps with cross-dataset transfer:**\n\n- **Geometric invariance**: Operating on the SPD manifold provides\n  some invariance to amplitude scaling differences between setups\n- **Second-order statistics**: Covariance matrices capture signal\n  relationships that are more stable across conditions than raw amplitudes\n- **Riemannian operations**: The geometric operations respect the\n  intrinsic structure of covariance matrices\n\n**Factors affecting cross-dataset performance:**\n\n1. **Channel overlap**: We used only 3 common channels (C3, Cz, C4),\n   which limits the spatial information available\n2. **Subject variability**: Even within datasets, subjects vary significantly\n3. **Task differences**: Subtle protocol variations affect the recorded signals\n\n**Recommendations for improving cross-dataset transfer:**\n\n- Use **more subjects** for training to learn more generalizable features\n- Apply **domain adaptation techniques** (see TSMNet example)\n- Use **channel interpolation** to leverage more electrodes\n- Consider **data augmentation** on the SPD manifold\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this tutorial, we demonstrated:\n\n1. Loading and preprocessing multiple EEG datasets for cross-dataset transfer\n2. Selecting common channels and classes across datasets\n3. Training SPDNet with Leave-One-Dataset-Out cross-validation\n4. Evaluating cross-dataset generalization performance\n\nKey takeaways:\n\n- Cross-dataset transfer is challenging but important for practical BCIs\n- SPDNet's geometric approach provides some robustness to dataset variations\n- Standardized preprocessing is crucial for fair cross-dataset comparison\n- Performance above chance indicates learned generalizable features\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}