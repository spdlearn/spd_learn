{
  "cells": [
    {
      "id": "5c922c80",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install -q spd_learn moabb braindecode scikit-learn matplotlib\n\n# For GPU support (recommended for faster training)\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-Supervised Contrastive Learning on SPD Manifolds for EEG\n\nThis example demonstrates self-supervised contrastive learning on the SPD\nmanifold for EEG data, inspired by the DeepGeoCCA framework\n:cite:p:`ju2024self`.\n\nThe key idea is to learn representations by maximizing the correlation between\ndifferent \"views\" of the same EEG trial in the tangent space of the SPD manifold.\nThis enables learning meaningful features without labels.\n\n.. topic:: Method Overview\n\n    **Geodesic Correlation**: Traditional correlation measures don't respect\n    the geometry of SPD matrices. The geodesic correlation projects SPD matrices\n    to the tangent space (via matrix logarithm) and computes correlation there.\n\n    **Contrastive Learning**: We create two views of each EEG trial by:\n    1. Using different subsets of channels (spatial views)\n    2. Using different time windows (temporal views)\n\n    The model learns to maximize correlation between views of the same trial\n    while learning discriminative SPD representations.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example requires: ``braindecode``, ``moabb``.\n   Install with: ``pip install braindecode moabb``</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom spd_learn.modules import BiMap, CovLayer, LogEig, ReEig, SPDBatchNormMeanVar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Synthetic Multi-View EEG Data\n\nWe create synthetic EEG data with two spatial views:\n- View 1: Central channels (motor cortex)\n- View 2: Lateral channels\n\nEach view captures different aspects of the same neural activity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_eeg(\n    n_samples=200, n_channels=22, n_times=256, n_classes=2, seed=42\n):\n    \"\"\"Generate synthetic EEG data with class-specific covariance structure.\"\"\"\n    np.random.seed(seed)\n\n    X_list = []\n    y_list = []\n\n    for class_idx in range(n_classes):\n        n_per_class = n_samples // n_classes\n\n        # Create class-specific mixing matrix\n        # Different classes have different spatial patterns\n        A = np.random.randn(n_channels, n_channels) * 0.5\n        A = A + class_idx * 0.3 * np.eye(n_channels)  # Class-specific bias\n\n        for _ in range(n_per_class):\n            # Generate source signals with class-specific characteristics\n            sources = np.random.randn(n_channels, n_times)\n\n            # Add class-specific frequency content\n            t = np.linspace(0, 1, n_times)\n            if class_idx == 0:\n                # Class 0: More alpha activity (8-12 Hz)\n                sources[0] += 2 * np.sin(2 * np.pi * 10 * t)\n                sources[1] += 1.5 * np.sin(2 * np.pi * 11 * t)\n            else:\n                # Class 1: More beta activity (15-25 Hz)\n                sources[0] += 2 * np.sin(2 * np.pi * 20 * t)\n                sources[1] += 1.5 * np.sin(2 * np.pi * 18 * t)\n\n            # Mix sources\n            X = A @ sources\n            X_list.append(X)\n            y_list.append(class_idx)\n\n    X = np.stack(X_list).astype(np.float32)\n    y = np.array(y_list)\n\n    # Shuffle\n    idx = np.random.permutation(len(y))\n    return X[idx], y[idx]\n\n\n# Generate data\nprint(\"Generating synthetic EEG data...\")\nX_data, y_data = generate_synthetic_eeg(n_samples=300, n_channels=22, n_times=256)\nprint(f\"Data shape: {X_data.shape}, Labels shape: {y_data.shape}\")\n\n# Define channel views (simulating different electrode groups)\n# View 1: Central channels (like C3, Cz, C4 in 10-20 system)\nview1_channels = [7, 8, 9, 10, 11]  # 5 channels\n# View 2: Frontal/Parietal channels\nview2_channels = [0, 1, 2, 3, 4, 15, 16, 17, 18, 19]  # 10 channels\n\nprint(f\"View 1 channels: {len(view1_channels)}\")\nprint(f\"View 2 channels: {len(view2_channels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-View Dataset\n\nCreate a dataset that returns two views of each trial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MultiViewEEGDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset returning two spatial views of EEG data.\"\"\"\n\n    def __init__(self, X, y, view1_channels, view2_channels):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y).long()\n        self.view1_ch = view1_channels\n        self.view2_ch = view2_channels\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        view1 = x[self.view1_ch]  # (n_ch1, n_times)\n        view2 = x[self.view2_ch]  # (n_ch2, n_times)\n        return view1, view2, self.y[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPD Encoder Network\n\nEach view is encoded using an SPD neural network that:\n1. Computes covariance matrix (CovLayer)\n2. Applies BiMap for dimensionality reduction\n3. Applies ReEig for non-linearity\n4. Projects to tangent space (LogEig)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SPDEncoder(nn.Module):\n    \"\"\"SPD-based encoder for EEG covariance features.\n\n    Maps raw EEG signals to SPD representations and then to\n    tangent space features.\n    \"\"\"\n\n    def __init__(self, n_channels, embed_dim, output_spd=False):\n        super().__init__()\n        self.output_spd = output_spd\n\n        # Covariance estimation\n        self.cov = CovLayer()\n\n        # SPD transformation layers\n        self.bimap = BiMap(n_channels, embed_dim)\n        self.reeig = ReEig(threshold=1e-4)\n        self.batchnorm = SPDBatchNormMeanVar(embed_dim)\n\n        # Project to tangent space\n        self.logeig = LogEig(upper=True, flatten=True)\n\n        # Output dimension\n        self.output_dim = embed_dim * (embed_dim + 1) // 2\n\n    def forward(self, x):\n        \"\"\"\n        Parameters\n        ----------\n        x : torch.Tensor\n            EEG signals of shape (batch, channels, time)\n\n        Returns\n        -------\n        torch.Tensor\n            Tangent space features of shape (batch, output_dim)\n            or SPD matrices if output_spd=True\n        \"\"\"\n        # Compute covariance\n        cov = self.cov(x)  # (batch, channels, channels)\n\n        # SPD transformations\n        spd = self.bimap(cov)\n        spd = self.reeig(spd)\n        spd = self.batchnorm(spd)\n\n        if self.output_spd:\n            return spd\n\n        # Project to tangent space\n        features = self.logeig(spd)\n        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Geodesic Correlation Loss\n\nThe geodesic correlation measures similarity between SPD matrices\nby computing correlation in the tangent space. This respects the\nRiemannian geometry of SPD matrices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class GeodesicCorrelationLoss(nn.Module):\n    \"\"\"Loss based on geodesic correlation for SPD matrices.\n\n    Maximizes correlation between paired views in tangent space.\n    \"\"\"\n\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n\n        # Learnable projections for CCA-like alignment\n        feat_dim = embed_dim * (embed_dim + 1) // 2\n        self.proj1 = nn.Linear(feat_dim, feat_dim, bias=False)\n        self.proj2 = nn.Linear(feat_dim, feat_dim, bias=False)\n\n    def forward(self, z1, z2):\n        \"\"\"\n        Compute geodesic correlation loss.\n\n        Parameters\n        ----------\n        z1, z2 : torch.Tensor\n            Tangent space features from two views, shape (batch, feat_dim)\n\n        Returns\n        -------\n        torch.Tensor\n            Negative correlation (to minimize)\n        \"\"\"\n        batch_size = z1.shape[0]\n\n        # Center the features\n        z1 = z1 - z1.mean(dim=0, keepdim=True)\n        z2 = z2 - z2.mean(dim=0, keepdim=True)\n\n        # Apply learnable projections\n        z1_proj = self.proj1(z1)\n        z2_proj = self.proj2(z2)\n\n        # Normalize\n        z1_norm = F.normalize(z1_proj, dim=1)\n        z2_norm = F.normalize(z2_proj, dim=1)\n\n        # Compute correlation matrix\n        corr_matrix = z1_norm @ z2_norm.T  # (batch, batch)\n\n        # Positive pairs are on diagonal\n        pos_corr = torch.diag(corr_matrix).mean()\n\n        # Negative pairs are off-diagonal\n        mask = ~torch.eye(batch_size, dtype=bool, device=z1.device)\n        neg_corr = corr_matrix[mask].mean()\n\n        # Contrastive loss: maximize positive, minimize negative correlation\n        loss = -pos_corr + 0.5 * neg_corr.abs()\n\n        return loss, pos_corr.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contrastive Model\n\nCombines two encoders (one per view) with the geodesic correlation loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ContrastiveSPDNet(nn.Module):\n    \"\"\"Contrastive learning model for multi-view SPD data.\"\"\"\n\n    def __init__(self, n_channels_view1, n_channels_view2, embed_dim):\n        super().__init__()\n        self.encoder1 = SPDEncoder(n_channels_view1, embed_dim)\n        self.encoder2 = SPDEncoder(n_channels_view2, embed_dim)\n        self.loss_fn = GeodesicCorrelationLoss(embed_dim)\n\n    def forward(self, view1, view2):\n        \"\"\"Encode both views.\"\"\"\n        z1 = self.encoder1(view1)\n        z2 = self.encoder2(view2)\n        return z1, z2\n\n    def compute_loss(self, view1, view2):\n        \"\"\"Compute contrastive loss.\"\"\"\n        z1, z2 = self.forward(view1, view2)\n        loss, pos_corr = self.loss_fn(z1, z2)\n        return loss, pos_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n\nSelf-supervised pretraining: learn to correlate different views.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create datasets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_data, y_data, test_size=0.3, random_state=SEED, stratify=y_data\n)\n\ntrain_dataset = MultiViewEEGDataset(X_train, y_train, view1_channels, view2_channels)\ntest_dataset = MultiViewEEGDataset(X_test, y_test, view1_channels, view2_channels)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=32, shuffle=True, drop_last=True\n)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Initialize model\nembed_dim = 4  # Small embedding dimension\nmodel = ContrastiveSPDNet(\n    n_channels_view1=len(view1_channels),\n    n_channels_view2=len(view2_channels),\n    embed_dim=embed_dim,\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Training\nn_epochs = 50\ntrain_losses = []\ntrain_correlations = []\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Self-Supervised Pretraining\")\nprint(\"=\" * 50)\n\nfor epoch in range(n_epochs):\n    model.train()\n    epoch_loss = 0\n    epoch_corr = 0\n\n    for view1, view2, _ in train_loader:\n        view1 = view1.to(device)\n        view2 = view2.to(device)\n\n        optimizer.zero_grad()\n        loss, pos_corr = model.compute_loss(view1, view2)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_corr += pos_corr\n\n    avg_loss = epoch_loss / len(train_loader)\n    avg_corr = epoch_corr / len(train_loader)\n    train_losses.append(avg_loss)\n    train_correlations.append(avg_corr)\n\n    if (epoch + 1) % 10 == 0:\n        print(\n            f\"Epoch {epoch + 1:3d} | Loss: {avg_loss:.4f} | Correlation: {avg_corr:.4f}\"\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Progress\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(train_losses, \"b-\", linewidth=2)\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].set_title(\"Contrastive Loss\")\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(train_correlations, \"g-\", linewidth=2)\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"Correlation\")\naxes[1].set_title(\"View Correlation (Positive Pairs)\")\naxes[1].grid(True, alpha=0.3)\naxes[1].axhline(y=0, color=\"r\", linestyle=\"--\", alpha=0.5)\n\nplt.suptitle(\"Self-Supervised Pretraining Progress\", fontweight=\"bold\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Learned Representations\n\nUse the pretrained encoder to extract features for downstream classification.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def extract_features(model, data_loader, device):\n    \"\"\"Extract features using the pretrained encoders.\"\"\"\n    model.eval()\n    features_v1, features_v2, labels = [], [], []\n\n    with torch.no_grad():\n        for view1, view2, y in data_loader:\n            view1 = view1.to(device)\n            view2 = view2.to(device)\n\n            z1, z2 = model(view1, view2)\n\n            features_v1.append(z1.cpu().numpy())\n            features_v2.append(z2.cpu().numpy())\n            labels.append(y.numpy())\n\n    return (\n        np.concatenate(features_v1),\n        np.concatenate(features_v2),\n        np.concatenate(labels),\n    )\n\n\n# Extract features\ntrain_z1, train_z2, train_labels = extract_features(model, train_loader, device)\ntest_z1, test_z2, test_labels = extract_features(model, test_loader, device)\n\n# Concatenate features from both views\ntrain_features = np.concatenate([train_z1, train_z2], axis=1)\ntest_features = np.concatenate([test_z1, test_z2], axis=1)\n\nprint(f\"\\nExtracted feature shape: {train_features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downstream Classification\n\nEvaluate the learned representations on a classification task.\nWe compare: pretrained features vs. random features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Train classifier on pretrained features\nclf_pretrained = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\nclf_pretrained.fit(train_features, train_labels)\npretrained_acc = accuracy_score(test_labels, clf_pretrained.predict(test_features))\n\n# Compare with random initialization\nmodel_random = ContrastiveSPDNet(\n    n_channels_view1=len(view1_channels),\n    n_channels_view2=len(view2_channels),\n    embed_dim=embed_dim,\n).to(device)\n\n# Extract features from random model\nrandom_train_z1, random_train_z2, _ = extract_features(\n    model_random, train_loader, device\n)\nrandom_test_z1, random_test_z2, _ = extract_features(model_random, test_loader, device)\n\nrandom_train_features = np.concatenate([random_train_z1, random_train_z2], axis=1)\nrandom_test_features = np.concatenate([random_test_z1, random_test_z2], axis=1)\n\nclf_random = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\nclf_random.fit(random_train_features, train_labels)\nrandom_acc = accuracy_score(test_labels, clf_random.predict(random_test_features))\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Downstream Classification Results\")\nprint(\"=\" * 50)\nprint(f\"Random initialization accuracy:  {random_acc:.4f}\")\nprint(f\"Pretrained (SSL) accuracy:       {pretrained_acc:.4f}\")\nprint(f\"Improvement:                     {(pretrained_acc - random_acc) * 100:+.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Feature Space\n\nProject features to 2D for visualization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n\n\npca = PCA(n_components=2)\nfeatures_2d = pca.fit_transform(test_features)\n\nfig, ax = plt.subplots(figsize=(8, 6))\nscatter = ax.scatter(\n    features_2d[:, 0],\n    features_2d[:, 1],\n    c=test_labels,\n    cmap=\"coolwarm\",\n    alpha=0.7,\n    s=50,\n)\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Learned SPD Feature Space (PCA Projection)\")\nplt.colorbar(scatter, label=\"Class\")\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nThis example demonstrated self-supervised contrastive learning on the SPD\nmanifold for EEG data. Key takeaways:\n\n1. **Geodesic Correlation**: By projecting SPD matrices to the tangent space,\n   we can compute meaningful correlations that respect the geometry.\n\n2. **Multi-View Learning**: Different channel subsets provide complementary\n   views of the same neural activity, enabling contrastive learning.\n\n3. **Pretraining Benefits**: The self-supervised pretraining improved\n   downstream classification compared to random initialization.\n\n### Extensions\n\n- Use real EEG data from MOABB datasets\n- Try different view creation strategies (temporal, frequency bands)\n- Implement the full DeepGeoCCA loss with distribution constraints\n- Apply to multi-modal data (EEG + fMRI)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}