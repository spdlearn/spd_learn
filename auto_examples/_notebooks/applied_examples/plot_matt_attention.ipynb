{
  "cells": [
    {
      "id": "3d551812",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install -q spd_learn moabb braindecode scikit-learn matplotlib\n\n# For GPU support (recommended for faster training)\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Manifold Attention with MAtt\n\nThis tutorial demonstrates how to use MAtt (Manifold Attention Network)\nfor EEG classification. MAtt applies attention mechanisms on the SPD\nmanifold to weight temporal segments by their discriminative importance.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nMAtt :cite:p:`pan2022matt` introduces **attention mechanisms on the SPD\nmanifold**:\n\n1. **Patch-based Processing**: Segments the signal into temporal patches\n2. **Covariance per Patch**: Computes SPD matrices for each segment\n3. **Manifold Attention**: Weights patches using Log-Euclidean distances\n4. **Aggregation**: Combines weighted SPD matrices for classification\n\nThis allows the model to focus on the most discriminative time periods\nwithin each trial, improving classification and interpretability.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom braindecode import EEGClassifier\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.paradigms import MotorImagery\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom skorch.callbacks import EpochScoring, GradientNormClipping\nfrom skorch.dataset import ValidSplit\n\nfrom spd_learn.models import MAtt\n\n\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Dataset\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = BNCI2014_001()\nparadigm = MotorImagery(n_classes=4)\n\nprint(f\"Dataset: {dataset.code}\")\nprint(\"Paradigm: 4-class motor imagery\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the MAtt Model\n\nMAtt architecture:\n\n1. **Spatial Conv**: Learns spatial filters\n2. **Temporal Conv**: Extracts temporal features\n3. **Patch Embedding**: Segments into n_patches temporal windows\n4. **Covariance + TraceNorm**: SPD matrix per patch\n5. **AttentionManifold**: Queries, keys, values on SPD manifold\n6. **ReEig + LogEig**: Project to tangent space\n7. **Linear**: Classification\n\nThe attention mechanism computes:\n\n\\begin{align}\\text{attention}(Q, K) = \\text{softmax}\\left(\\frac{1}{1 + \\log(1 + d_{LE}(Q, K))}\\right)\\end{align}\n\nwhere $d_{LE}$ is the Log-Euclidean distance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_chans = 22\nn_outputs = 4\n\nmodel = MAtt(\n    n_chans=n_chans,\n    n_outputs=n_outputs,\n    n_patches=6,  # Number of temporal segments\n    temporal_out_channels=32,  # Temporal feature dimension\n    temporal_kernel_size=25,  # ~100ms at 250Hz\n    temporal_padding=12,  # Keep time dimension\n    attention_in_features=32,  # Input to attention (must match temporal_out_channels)\n    attention_out_features=24,  # Output from attention\n)\n\nprint(\"MAtt Architecture:\")\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subject_id = 1\nbatch_size = 32\nmax_epochs = 100\nlearning_rate = 1e-4  # Low learning rate for stable SPD learning\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"\\nUsing device: {device}\")\n\n# Cache configuration\ncache_config = dict(\n    save_raw=True,\n    save_epochs=True,\n    save_array=True,\n    use=True,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n)\n\n# Load data\nX, labels, meta = paradigm.get_data(\n    dataset=dataset, subjects=[subject_id], cache_config=cache_config\n)\n\n# Encode labels\nle = LabelEncoder()\ny = le.fit_transform(labels)\n\n# Split by session\ntrain_idx = meta.query(\"session == '0train'\").index.to_numpy()\ntest_idx = meta.query(\"session == '1test'\").index.to_numpy()\n\nprint(f\"\\nData shape: {X.shape}\")\nprint(f\"Training samples: {len(train_idx)}\")\nprint(f\"Test samples: {len(test_idx)}\")\n\n# Create classifier\n# Note: SPD networks benefit from gradient clipping to prevent\n# divergence during training on the Riemannian manifold.\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.CrossEntropyLoss,\n    optimizer=torch.optim.Adam,\n    optimizer__lr=learning_rate,\n    train_split=ValidSplit(0.1, stratified=True, random_state=42),\n    batch_size=batch_size,\n    max_epochs=max_epochs,\n    callbacks=[\n        (\n            \"train_acc\",\n            EpochScoring(\n                \"accuracy\", lower_is_better=False, on_train=True, name=\"train_acc\"\n            ),\n        ),\n        (\"gradient_clip\", GradientNormClipping(gradient_clip_value=1.0)),\n    ],\n    device=device,\n    verbose=1,\n)\n\n# Train\nclf.fit(X[train_idx], y[train_idx])\n\n# Evaluate\ny_pred_train = clf.predict(X[train_idx])\ny_pred_test = clf.predict(X[test_idx])\n\ntrain_acc = accuracy_score(y[train_idx], y_pred_train)\ntest_acc = accuracy_score(y[test_idx], y_pred_test)\n\nprint(f\"\\n{'=' * 50}\")\nprint(f\"Results for Subject {subject_id}\")\nprint(f\"{'=' * 50}\")\nprint(f\"Train Accuracy: {train_acc * 100:.2f}%\")\nprint(f\"Test Accuracy:  {test_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Results\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Training history\nhistory = clf.history\nepochs = range(1, len(history) + 1)\n\nax1 = axes[0]\nax1.plot(epochs, history[:, \"train_loss\"], \"b-\", label=\"Train Loss\", linewidth=2)\nax1.plot(epochs, history[:, \"valid_loss\"], \"r--\", label=\"Valid Loss\", linewidth=2)\nax1.set_xlabel(\"Epoch\", fontsize=12)\nax1.set_ylabel(\"Loss\", fontsize=12)\nax1.set_title(\"Training and Validation Loss\", fontsize=14)\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3)\n\nax2 = axes[1]\nax2.plot(epochs, history[:, \"train_acc\"], \"b-\", label=\"Train Acc\", linewidth=2)\nax2.plot(epochs, history[:, \"valid_acc\"], \"r--\", label=\"Valid Acc\", linewidth=2)\nax2.set_xlabel(\"Epoch\", fontsize=12)\nax2.set_ylabel(\"Accuracy\", fontsize=12)\nax2.set_title(\"Training and Validation Accuracy\", fontsize=14)\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\nax2.set_ylim([0, 1])\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Manifold Attention\n\nThe attention mechanism in MAtt operates differently from standard attention:\n\n**Standard Attention** (Euclidean):\n\n\\begin{align}\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) V\\end{align}\n\n**Manifold Attention** (Log-Euclidean):\n\n\\begin{align}\\text{energy}_{ij} = d_{LE}(Q_i, K_j) = \\|\\log(Q_i) - \\log(K_j)\\|_F\\end{align}\n\n\\begin{align}\\text{weights}_{ij} = \\frac{1}{1 + \\log(1 + \\text{energy}_{ij})}\\end{align}\n\n\\begin{align}\\text{output}_i = \\sum_j \\text{softmax}(\\text{weights})_{ij} \\odot V_j\\end{align}\n\nThis respects the Riemannian geometry of SPD matrices, computing\nmeaningful distances on the manifold rather than in Euclidean space.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this tutorial, we demonstrated:\n\n1. Creating a MAtt model with patch-based temporal segmentation\n2. Training for motor imagery classification\n3. Understanding the manifold attention mechanism\n\nMAtt is particularly useful when:\n\n- Different time segments have varying discriminative power\n- You want interpretable attention weights\n- The data has complex temporal dynamics\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}