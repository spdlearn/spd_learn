{
  "cells": [
    {
      "id": "ef27170b",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install -q spd_learn moabb braindecode scikit-learn matplotlib\n\n# For GPU support (recommended for faster training)\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Radar Image Classification with SPD Learn\n\nThis tutorial demonstrates how to use **spd_learn** for synthetic aperture\nradar (SAR) polarimetric image classification using **real UAVSAR data**.\nPolarimetric SAR data naturally produces symmetric positive definite (SPD)\ncovariance matrices, making this an ideal application for SPD-based\nmachine learning.\n\nWe use the UAVSAR dataset from NASA's Uninhabited Aerial Vehicle Synthetic\nAperture Radar system, with pseudo-labels generated via Riemannian clustering.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nPolarimetric SAR (PolSAR) :cite:p:`lee2009polarimetric` systems transmit and\nreceive electromagnetic waves in multiple polarization states (typically\nHH, HV, VH, VV), capturing rich information about the scattering properties\nof terrain and objects.\n\nThe data is commonly represented as coherency or covariance matrices,\nwhich are **inherently SPD matrices**. This makes SPD-based deep learning\napproaches particularly well-suited for PolSAR classification tasks.\n\n**The UAVSAR Dataset**\n\nThe UAVSAR (Uninhabited Aerial Vehicle Synthetic Aperture Radar) is a\nNASA/JPL airborne SAR system. The data used here covers the Los Angeles\narea with:\n\n- **Source**: NASA/JPL UAVSAR, L-band\n- **Format**: Polarimetric scattering vectors (3 channels: HH, HV, VV)\n- **Hosted on**: Zenodo (open access)\n\nIn this tutorial, we:\n\n1. Load real UAVSAR PolSAR data\n2. Visualize the radar image using Pauli RGB decomposition\n   :cite:p:`cloude1996review`\n3. Generate pseudo-labels using Riemannian K-Means clustering\n4. Train SPDNet :cite:p:`huang2017riemannian` and compare with pyRiemann\n   baselines\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport logging\nimport warnings\n\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom braindecode import EEGClassifier\nfrom numpy.typing import NDArray\nfrom pyriemann.classification import MDM\nfrom pyriemann.clustering import Kmeans\nfrom pyriemann.tangentspace import TangentSpace\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    ConfusionMatrixDisplay,\n    accuracy_score,\n    balanced_accuracy_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom skorch.callbacks import EarlyStopping, EpochScoring\nfrom skorch.dataset import ValidSplit\n\nfrom spd_learn.models import SPDNet\n\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Set random seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## UAVSAR Data Loading\n\nThe UAVSAR dataset is hosted on Zenodo and contains processed PolSAR\nscattering vectors from the Los Angeles area.\n\nData source: https://zenodo.org/records/10625505\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_data_dir() -> Path:\n    \"\"\"Get the data directory for storing downloaded datasets.\"\"\"\n    data_dir = Path.home() / \".spd_learn_data\" / \"uavsar\"\n    data_dir.mkdir(parents=True, exist_ok=True)\n    return data_dir\n\n\ndef verify_npy_file(filepath: Path, expected_shape: tuple = (2360, 600, 3, 4)) -> bool:\n    \"\"\"Verify that a .npy file is complete and valid.\n\n    Parameters\n    ----------\n    filepath : Path\n        Path to the .npy file.\n    expected_shape : tuple\n        Expected shape of the array.\n\n    Returns\n    -------\n    bool\n        True if file is valid, False otherwise.\n    \"\"\"\n    try:\n        data = np.load(filepath)\n        if data.shape != expected_shape:\n            logger.warning(f\"Unexpected shape: {data.shape} vs {expected_shape}\")\n            return False\n        return True\n    except Exception as e:\n        logger.warning(f\"File verification failed: {e}\")\n        return False\n\n\ndef download_uavsar(data_path: Path, scene: int = 1) -> Path:\n    \"\"\"Download the UAVSAR dataset from Zenodo.\n\n    Parameters\n    ----------\n    data_path : Path\n        Path to the destination folder for data download.\n    scene : {1, 2}\n        Scene index to download.\n\n    Returns\n    -------\n    Path\n        Path to the downloaded file.\n    \"\"\"\n    assert scene in [1, 2], f\"Unknown scene {scene} for UAVSAR dataset\"\n    filename = f\"scene{scene}.npy\"\n    src = f\"https://zenodo.org/records/10625505/files/{filename}?download=1\"\n\n    if not data_path.exists():\n        data_path.mkdir(parents=True, exist_ok=True)\n\n    dst = data_path / filename\n\n    # Check if file exists and is valid\n    if dst.exists():\n        logger.info(f\"Verifying existing file: {dst}\")\n        if verify_npy_file(dst):\n            logger.info(\"File verified successfully!\")\n            return dst\n        else:\n            logger.warning(\"File corrupted or incomplete. Re-downloading...\")\n            dst.unlink()  # Remove corrupted file\n\n    logger.info(f\"Downloading UAVSAR scene {scene} from Zenodo...\")\n    logger.info(f\"Source: {src}\")\n    logger.info(f\"Destination: {dst}\")\n    logger.info(\"This may take a few minutes for the ~136MB file...\")\n    urlretrieve(src, dst)\n\n    # Verify the downloaded file\n    if not verify_npy_file(dst):\n        raise RuntimeError(\n            \"Downloaded file is corrupted. Please try again or \"\n            \"manually download from: https://zenodo.org/records/10625505\"\n        )\n\n    logger.info(\"Download complete and verified!\")\n    return dst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Dataset\n\nWe load the UAVSAR data which contains polarimetric scattering vectors.\nThe data has shape (height, width, 3, n_dates) where the 3 channels\ncorrespond to [HH, HV, VV] polarizations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\nprint(\"Loading Real UAVSAR PolSAR Data\")\nprint(\"=\" * 60)\n\ndata_path = get_data_dir()\nfile_path = download_uavsar(data_path, scene=1)\n\n# Load the data\ndata = np.load(file_path)\nprint(f\"Original data shape: {data.shape}\")\n# Shape: (height, width, 3, n_dates)\n\n# Select first date\ndate_idx = 0\ndata = data[:, :, :, date_idx]\nprint(f\"After date selection: {data.shape}\")\n\n# Store original dimensions for visualization\nh_orig, w_orig, n_channels = data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the SAR Image\n\nWe visualize the radar image using standard SAR representations:\n\n1. **Total Power (Span)**: Sum of all polarimetric powers\n2. **Pauli RGB**: Color composite showing scattering mechanisms\n\n   - Red: ``abs(HH - VV)`` (double-bounce)\n   - Green: ``abs(HV)`` (volume scattering)\n   - Blue: ``abs(HH + VV)`` (surface scattering)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute intensity image (in dB)\nintensity = 20 * np.log10(np.sum(np.abs(data) ** 2, axis=2) + 1e-10)\n\n# Compute Pauli RGB components\nHH = data[:, :, 0]\nHV = data[:, :, 1]\nVV = data[:, :, 2]\n\n# Pauli decomposition\npauli_red = np.abs(HH - VV)  # Double-bounce\npauli_green = np.abs(HV) * np.sqrt(2)  # Volume\npauli_blue = np.abs(HH + VV)  # Surface\n\n\ndef normalize_for_display(img: NDArray, percentile: tuple = (2, 98)) -> NDArray:\n    \"\"\"Normalize image for display with percentile clipping.\"\"\"\n    p_low, p_high = np.percentile(img, percentile)\n    img_norm = (img - p_low) / (p_high - p_low + 1e-10)\n    return np.clip(img_norm, 0, 1)\n\n\n# Create Pauli RGB image\npauli_rgb = np.stack(\n    [\n        normalize_for_display(pauli_red),\n        normalize_for_display(pauli_green),\n        normalize_for_display(pauli_blue),\n    ],\n    axis=2,\n)\n\n# Create figure with SAR visualizations\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Total power (Span)\nax = axes[0]\nim = ax.imshow(intensity, cmap=\"gray\", aspect=\"auto\")\nax.set_title(\"SAR Intensity (dB)\", fontsize=12, fontweight=\"bold\")\nax.set_xlabel(\"Range\")\nax.set_ylabel(\"Azimuth\")\nplt.colorbar(im, ax=ax, label=\"dB\")\n\n# Pauli RGB\nax = axes[1]\nax.imshow(pauli_rgb, aspect=\"auto\")\nax.set_title(\"Pauli RGB Decomposition\", fontsize=12, fontweight=\"bold\")\nax.set_xlabel(\"Range\")\nax.set_ylabel(\"Azimuth\")\n\n# Add legend for Pauli colors\nfrom matplotlib.patches import Patch\n\n\nlegend_elements = [\n    Patch(facecolor=\"red\", label=\"Double-bounce |HH-VV|\"),\n    Patch(facecolor=\"green\", label=\"Volume |HV|\"),\n    Patch(facecolor=\"blue\", label=\"Surface |HH+VV|\"),\n]\nax.legend(handles=legend_elements, loc=\"upper right\", fontsize=8)\n\n# Individual channels\nax = axes[2]\nax.imshow(normalize_for_display(np.abs(HV)), cmap=\"viridis\", aspect=\"auto\")\nax.set_title(\"Cross-pol |HV| (Volume Scattering)\", fontsize=12, fontweight=\"bold\")\nax.set_xlabel(\"Range\")\nax.set_ylabel(\"Azimuth\")\n\nplt.suptitle(\n    \"UAVSAR Los Angeles - Polarimetric SAR Visualization\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Covariance Matrices\n\nFor SPD-based classification, we compute local covariance matrices\nusing a sliding window approach. Each pixel gets a 3x3 covariance\nmatrix estimated from its neighborhood.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Computing Covariance Matrices\")\nprint(\"=\" * 60)\n\n# Downsample for computational efficiency\ndownsample_h = 7\ndownsample_w = 4\ndata_ds = data[::downsample_h, ::downsample_w, :]\nh_ds, w_ds, _ = data_ds.shape\nprint(f\"Downsampled shape: {data_ds.shape}\")\n\n# Reshape for covariance estimation\n# We'll compute covariance from the scattering vector at each pixel\n# Shape: (n_pixels, n_channels)\ndata_flat = data_ds.reshape(-1, n_channels)\nn_pixels = data_flat.shape[0]\nprint(f\"Number of pixels: {n_pixels}\")\n\n# Compute covariance matrices from scattering vectors\n# C = k * k^H where k = [HH, HV, VV]\nprint(\"Computing covariance matrices...\")\ncovs = np.zeros((n_pixels, 3, 3), dtype=np.float64)\n\nfor i in range(n_pixels):\n    k = data_flat[i]  # Scattering vector\n    C = np.outer(k, np.conj(k))  # Outer product\n    covs[i] = np.real(C)  # Take real part\n\n# Ensure strong positive definiteness for Riemannian operations\n# Riemannian mean computation requires well-conditioned matrices\nprint(\"Ensuring positive definiteness...\")\nmin_eigenvalue = 1e-4  # Minimum eigenvalue for numerical stability\nfor i in range(n_pixels):\n    # Symmetrize\n    covs[i] = (covs[i] + covs[i].T) / 2\n    # Compute eigenvalues\n    eigvals = np.linalg.eigvalsh(covs[i])\n    # Add regularization to ensure minimum eigenvalue\n    if np.min(eigvals) < min_eigenvalue:\n        covs[i] += (min_eigenvalue - np.min(eigvals) + 1e-6) * np.eye(3)\n\nX = covs\nprint(f\"Covariance matrices shape: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering for Pseudo-Labels\n\nSince UAVSAR doesn't have ground truth labels, we use Riemannian\nK-Means clustering to segment the image into terrain types.\nThe clusters serve as pseudo-labels for classification.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Riemannian K-Means Clustering\")\nprint(\"=\" * 60)\n\nn_clusters = 3  # Reduced from 5 for faster documentation build\n\nprint(f\"Clustering into {n_clusters} classes using Riemannian K-Means...\")\nkmeans = Kmeans(n_clusters=n_clusters, metric=\"riemann\", random_state=SEED)\ny = kmeans.fit_predict(X)\n\n# Reshape labels back to image\nlabels_image = y.reshape(h_ds, w_ds)\n\n# Count samples per cluster\nunique, counts = np.unique(y, return_counts=True)\nprint(\"\\nCluster distribution:\")\nfor cluster, count in zip(unique, counts):\n    print(f\"  Cluster {cluster}: {count} pixels ({count / len(y) * 100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Clustering Results\n\nWe display the clustering results as a segmentation map overlaid\non the original SAR image, similar to pyRiemann's visualization.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Downsample Pauli RGB for comparison\npauli_rgb_ds = pauli_rgb[::downsample_h, ::downsample_w, :]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Original Pauli RGB (downsampled)\nax = axes[0]\nax.imshow(pauli_rgb_ds, aspect=\"auto\")\nax.set_title(\"Pauli RGB (Downsampled)\", fontsize=12, fontweight=\"bold\")\nax.set_xlabel(\"Range\")\nax.set_ylabel(\"Azimuth\")\n\n# Clustering result\nax = axes[1]\nim = ax.imshow(labels_image, cmap=\"tab10\", aspect=\"auto\")\nax.set_title(\n    f\"Riemannian K-Means ({n_clusters} clusters)\", fontsize=12, fontweight=\"bold\"\n)\nax.set_xlabel(\"Range\")\nax.set_ylabel(\"Azimuth\")\nplt.colorbar(im, ax=ax, label=\"Cluster\")\n\n# Overlay: Pauli RGB with cluster boundaries\nax = axes[2]\nax.imshow(pauli_rgb_ds, aspect=\"auto\")\nax.contour(\n    labels_image, levels=n_clusters - 1, colors=\"white\", linewidths=0.5, alpha=0.7\n)\nax.set_title(\"Pauli RGB with Cluster Boundaries\", fontsize=12, fontweight=\"bold\")\nax.set_xlabel(\"Range\")\nax.set_ylabel(\"Azimuth\")\n\nplt.suptitle(\"UAVSAR Segmentation Results\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Cluster Characteristics\n\nLet's examine the covariance structure of each cluster to understand\nwhat terrain types they might represent.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, n_clusters, figsize=(3 * n_clusters, 6))\n\nfor cluster_idx in range(n_clusters):\n    # Get samples from this cluster\n    mask = y == cluster_idx\n    cluster_covs = X[mask]\n\n    # Mean covariance matrix\n    mean_cov = np.mean(cluster_covs, axis=0)\n\n    # Plot mean covariance\n    ax = axes[0, cluster_idx]\n    im = ax.imshow(mean_cov, cmap=\"viridis\", aspect=\"equal\")\n    ax.set_title(f\"Cluster {cluster_idx}\\nMean Cov\", fontsize=10, fontweight=\"bold\")\n    ax.set_xticks([0, 1, 2])\n    ax.set_yticks([0, 1, 2])\n    ax.set_xticklabels([\"HH\", \"HV\", \"VV\"], fontsize=8)\n    ax.set_yticklabels([\"HH\", \"HV\", \"VV\"], fontsize=8)\n    plt.colorbar(im, ax=ax, fraction=0.046)\n\n    # Eigenvalue distribution\n    ax = axes[1, cluster_idx]\n    eigenvalues = np.array([np.linalg.eigvalsh(c) for c in cluster_covs])\n    colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\"]\n    for eig_idx in range(3):\n        ax.hist(\n            eigenvalues[:, eig_idx],\n            bins=30,\n            alpha=0.6,\n            color=colors[eig_idx],\n            label=f\"\u03bb{eig_idx + 1}\",\n        )\n    ax.set_xlabel(\"Eigenvalue\", fontsize=9)\n    ax.set_ylabel(\"Count\", fontsize=9)\n    ax.set_title(\"Eigenvalues\", fontsize=10)\n    if cluster_idx == n_clusters - 1:\n        ax.legend(fontsize=7)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle(\"Cluster Analysis: Covariance Structure\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Splitting for Classification\n\nWe split the data into training and test sets for classification.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_classes = n_clusters\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=SEED\n)\n\nprint(f\"\\nTraining set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification with pyRiemann Baselines\n\nWe compare SPDNet against pyRiemann classifiers:\n\n1. **MDM**: Minimum Distance to Mean\n2. **Tangent Space + Logistic Regression**\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Training Classifiers\")\nprint(\"=\" * 60)\n\n# MDM Classifier\nprint(\"\\n[1] Training MDM classifier...\")\nclf_mdm = MDM(metric=\"riemann\")\nclf_mdm.fit(X_train, y_train)\ny_pred_mdm = clf_mdm.predict(X_test)\nacc_mdm = accuracy_score(y_test, y_pred_mdm)\nbal_acc_mdm = balanced_accuracy_score(y_test, y_pred_mdm)\nprint(f\"    MDM Accuracy: {acc_mdm * 100:.2f}%\")\nprint(f\"    MDM Balanced Accuracy: {bal_acc_mdm * 100:.2f}%\")\n\n# Tangent Space + Logistic Regression\nprint(\"\\n[2] Training Tangent Space + LR classifier...\")\nclf_ts = make_pipeline(\n    TangentSpace(metric=\"riemann\"),\n    LogisticRegression(random_state=SEED, max_iter=1000),\n)\nclf_ts.fit(X_train, y_train)\ny_pred_ts = clf_ts.predict(X_test)\nacc_ts = accuracy_score(y_test, y_pred_ts)\nbal_acc_ts = balanced_accuracy_score(y_test, y_pred_ts)\nprint(f\"    TS+LR Accuracy: {acc_ts * 100:.2f}%\")\nprint(f\"    TS+LR Balanced Accuracy: {bal_acc_ts * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPDNet for Radar Classification\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n[3] Training SPDNet...\")\nmodel = SPDNet(\n    input_type=\"cov\",\n    n_chans=3,\n    n_outputs=n_classes,\n    subspacedim=3,\n    threshold=1e-4,\n)\n\nprint(\"\\nSPDNet Architecture:\")\nprint(model)\n\nclf_spd = EEGClassifier(\n    module=model,\n    criterion=torch.nn.CrossEntropyLoss,\n    optimizer=torch.optim.AdamW,\n    optimizer__lr=1e-2,\n    optimizer__weight_decay=1e-4,\n    train_split=ValidSplit(0.1, stratified=True, random_state=SEED),\n    batch_size=64,\n    max_epochs=20,  # Reduced from 100 for faster documentation build\n    callbacks=[\n        (\n            \"train_acc\",\n            EpochScoring(\n                \"accuracy\", lower_is_better=False, on_train=True, name=\"train_acc\"\n            ),\n        ),\n        EarlyStopping(monitor=\"valid_loss\", patience=15),\n    ],\n    device=device,\n    verbose=1,\n)\n\nclf_spd.fit(X_train, y_train)\n\ny_pred_spd = clf_spd.predict(X_test)\nacc_spd = accuracy_score(y_test, y_pred_spd)\nbal_acc_spd = balanced_accuracy_score(y_test, y_pred_spd)\nprint(f\"\\nSPDNet Accuracy: {acc_spd * 100:.2f}%\")\nprint(f\"SPDNet Balanced Accuracy: {bal_acc_spd * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification Maps\n\nWe create classification maps by predicting the class for each pixel\nand displaying as an image.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Generating Classification Maps\")\nprint(\"=\" * 60)\n\n# Predict for all pixels\ny_pred_all_mdm = clf_mdm.predict(X)\ny_pred_all_ts = clf_ts.predict(X)\ny_pred_all_spd = clf_spd.predict(X)\n\n# Reshape to images\nmap_mdm = y_pred_all_mdm.reshape(h_ds, w_ds)\nmap_ts = y_pred_all_ts.reshape(h_ds, w_ds)\nmap_spd = y_pred_all_spd.reshape(h_ds, w_ds)\n\n# Visualization\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Top row: Classification maps\nfor ax, (name, pred_map, acc) in zip(\n    axes[0],\n    [\n        (\"MDM\", map_mdm, acc_mdm),\n        (\"TS + LogReg\", map_ts, acc_ts),\n        (\"SPDNet\", map_spd, acc_spd),\n    ],\n):\n    im = ax.imshow(pred_map, cmap=\"tab10\", aspect=\"auto\")\n    ax.set_title(f\"{name}\\nAccuracy: {acc * 100:.1f}%\", fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Range\")\n    ax.set_ylabel(\"Azimuth\")\n    plt.colorbar(im, ax=ax, label=\"Class\")\n\n# Bottom row: Overlays on Pauli RGB\nfor ax, (name, pred_map) in zip(\n    axes[1],\n    [(\"MDM\", map_mdm), (\"TS + LogReg\", map_ts), (\"SPDNet\", map_spd)],\n):\n    ax.imshow(pauli_rgb_ds, aspect=\"auto\")\n    ax.contour(\n        pred_map, levels=n_clusters - 1, colors=\"yellow\", linewidths=0.5, alpha=0.8\n    )\n    ax.set_title(f\"{name} Boundaries on Pauli RGB\", fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Range\")\n    ax.set_ylabel(\"Azimuth\")\n\nplt.suptitle(\"Classification Results Comparison\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = {\n    \"MDM\": {\"accuracy\": acc_mdm, \"balanced_accuracy\": bal_acc_mdm},\n    \"TS + LogReg\": {\"accuracy\": acc_ts, \"balanced_accuracy\": bal_acc_ts},\n    \"SPDNet\": {\"accuracy\": acc_spd, \"balanced_accuracy\": bal_acc_spd},\n}\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Results Summary\")\nprint(\"=\" * 60)\nprint(f\"\\n{'Method':<20} {'Accuracy':<15} {'Balanced Acc':<15}\")\nprint(\"-\" * 50)\nfor method, metrics in results.items():\n    print(\n        f\"{method:<20} {metrics['accuracy'] * 100:>6.2f}%        {metrics['balanced_accuracy'] * 100:>6.2f}%\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Visualization\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16, 10))\n\n# Accuracy comparison\nax1 = fig.add_subplot(2, 2, 1)\nmethods = list(results.keys())\naccuracies = [results[m][\"accuracy\"] for m in methods]\nbal_accuracies = [results[m][\"balanced_accuracy\"] for m in methods]\n\nx = np.arange(len(methods))\nwidth = 0.35\n\nbars1 = ax1.bar(\n    x - width / 2,\n    accuracies,\n    width,\n    label=\"Accuracy\",\n    color=\"#3498db\",\n    edgecolor=\"black\",\n)\nbars2 = ax1.bar(\n    x + width / 2,\n    bal_accuracies,\n    width,\n    label=\"Balanced Acc\",\n    color=\"#2ecc71\",\n    edgecolor=\"black\",\n)\n\nax1.set_xlabel(\"Method\", fontsize=12)\nax1.set_ylabel(\"Score\", fontsize=12)\nax1.set_title(\"Classification Performance\", fontsize=14, fontweight=\"bold\")\nax1.set_xticks(x)\nax1.set_xticklabels(methods)\nax1.set_ylim([0, 1.1])\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3, axis=\"y\")\n\nfor bar in bars1 + bars2:\n    ax1.text(\n        bar.get_x() + bar.get_width() / 2,\n        bar.get_height() + 0.02,\n        f\"{bar.get_height():.2%}\",\n        ha=\"center\",\n        fontsize=9,\n    )\n\n# Training history\nax2 = fig.add_subplot(2, 2, 2)\nhistory = clf_spd.history\nepochs = range(1, len(history) + 1)\nax2.plot(epochs, history[:, \"train_loss\"], \"b-\", label=\"Train Loss\", linewidth=2)\nax2.plot(epochs, history[:, \"valid_loss\"], \"r--\", label=\"Valid Loss\", linewidth=2)\nax2.set_xlabel(\"Epoch\", fontsize=12)\nax2.set_ylabel(\"Loss\", fontsize=12)\nax2.set_title(\"SPDNet Training History\", fontsize=14, fontweight=\"bold\")\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\n# Confusion matrices\ncluster_labels = [f\"C{i}\" for i in range(n_classes)]\n\nax3 = fig.add_subplot(2, 3, 4)\nConfusionMatrixDisplay.from_predictions(\n    y_test,\n    y_pred_mdm,\n    ax=ax3,\n    display_labels=cluster_labels,\n    cmap=\"Blues\",\n    colorbar=False,\n)\nax3.set_title(\"MDM\", fontsize=12, fontweight=\"bold\")\n\nax4 = fig.add_subplot(2, 3, 5)\nConfusionMatrixDisplay.from_predictions(\n    y_test,\n    y_pred_ts,\n    ax=ax4,\n    display_labels=cluster_labels,\n    cmap=\"Blues\",\n    colorbar=False,\n)\nax4.set_title(\"TS + LogReg\", fontsize=12, fontweight=\"bold\")\n\nax5 = fig.add_subplot(2, 3, 6)\nConfusionMatrixDisplay.from_predictions(\n    y_test,\n    y_pred_spd,\n    ax=ax5,\n    display_labels=cluster_labels,\n    cmap=\"Blues\",\n    colorbar=False,\n)\nax5.set_title(\"SPDNet\", fontsize=12, fontweight=\"bold\")\n\nplt.suptitle(\n    \"UAVSAR Classification with SPD Methods\", fontsize=16, fontweight=\"bold\", y=1.02\n)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this tutorial, we demonstrated:\n\n1. **Real PolSAR data**: Loaded NASA UAVSAR data from Zenodo\n\n2. **SAR visualization**: Displayed radar intensity and Pauli RGB\n\n3. **Covariance estimation**: Computed 3x3 SPD matrices from\n   polarimetric scattering vectors\n\n4. **Riemannian clustering**: Generated pseudo-labels using K-Means\n   with the Riemannian metric\n\n5. **Classification comparison**: Compared MDM, Tangent Space + LR,\n   and SPDNet on real radar data\n\n6. **Classification maps**: Visualized spatial prediction results\n\n**Key takeaways:**\n\n- PolSAR covariance matrices are naturally SPD\n- Riemannian geometry captures meaningful terrain differences\n- SPDNet learns features directly on the SPD manifold\n- Classification maps reveal spatial patterns in the radar image\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}