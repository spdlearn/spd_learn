{
  "cells": [
    {
      "id": "1f68111b",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install -q spd_learn moabb braindecode scikit-learn matplotlib\n\n# For GPU support (recommended for faster training)\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Benchmarking SPD Learn Models with MOABB and Hydra\n\nThis tutorial demonstrates how to set up a comprehensive benchmarking\npipeline for SPD Learn models using MOABB datasets and Hydra for\nconfiguration management. We compare multiple geometric deep learning\narchitectures on motor imagery EEG classification.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nReproducible machine learning experiments require systematic\nconfiguration management. This tutorial shows how to:\n\n1. **Use Hydra** :cite:p:`hydra2019` for declarative experiment configuration\n2. **Benchmark multiple models** from SPD Learn (SPDNet\n   :cite:p:`huang2017riemannian`, TSMNet, etc.)\n3. **Leverage MOABB** :cite:p:`moabb2018` for standardized EEG dataset access\n4. **Implement proper cross-validation** for reliable performance estimates\n5. **Visualize and compare results** across models\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Hydra is a powerful framework for managing complex configurations.\n   While this tutorial shows inline configuration for simplicity,\n   in practice you would use YAML files for better organization.</p></div>\n\n.. important::\n\n   **Model-Specific Training Requirements**\n\n   Different SPD models have different training requirements:\n\n   - **SPDNet**: Works on covariance matrices, can use higher learning rates (1e-3)\n   - **TSMNet**: Works on raw signals with SPDBatchNormMeanVar\n     :cite:p:`kobler2022spd`, requires lower learning rate (1e-4) and more\n     epochs (100+) for stable SPD learning\n   - **EEGSPDNet**: Works on raw signals, also requires lower learning rate (1e-4)\n     and more epochs for the channel-specific convolutions to converge\n\n   This benchmark uses model-specific training configurations to ensure\n   each model achieves optimal performance.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\nWe import the necessary libraries for this benchmark:\n\n- **MOABB**: Standardized EEG datasets and paradigms\n- **Braindecode**: EEGClassifier wrapper for PyTorch models\n- **SPD Learn**: Geometric deep learning models\n- **Hydra/OmegaConf**: Configuration management\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport tempfile\nimport warnings\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom braindecode import EEGClassifier\nfrom einops.layers.torch import Rearrange\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.paradigms import MotorImagery\nfrom omegaconf import MISSING, OmegaConf\nfrom sklearn.metrics import (\n    accuracy_score,\n    balanced_accuracy_score,\n    confusion_matrix,\n)\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom skorch.callbacks import (\n    Checkpoint,\n    EarlyStopping,\n    EpochScoring,\n    GradientNormClipping,\n    LRScheduler,\n)\nfrom skorch.dataset import ValidSplit\nfrom torch import nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom spd_learn.models import EEGSPDNet, SPDNet, TensorCSPNet, TSMNet\n\n\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hydra Configuration with Dataclasses\n\nHydra uses structured configurations that can be defined as dataclasses.\nThis provides type safety and autocompletion while maintaining the\nflexibility of YAML-based configuration.\n\nWe define configuration schemas for:\n\n- **Model configurations**: Architecture-specific parameters\n- **Training configurations**: Optimizer, scheduler, and training settings\n- **Experiment configurations**: Dataset, paradigm, and evaluation settings\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@dataclass\nclass ModelConfig:\n    \"\"\"Base configuration for all models.\n\n    This dataclass defines the common parameters shared by all SPD Learn models.\n    Model-specific configurations inherit from this class and add their own\n    parameters.\n\n    Parameters\n    ----------\n    name : str\n        Name of the model (e.g., \"SPDNet\", \"TSMNet\", \"EEGSPDNet\").\n    n_chans : int\n        Number of input EEG channels.\n    n_outputs : int\n        Number of output classes for classification.\n    \"\"\"\n\n    name: str = MISSING\n    n_chans: int = MISSING\n    n_outputs: int = MISSING\n\n\n@dataclass\nclass SPDNetConfig(ModelConfig):\n    \"\"\"Configuration for SPDNet model.\n\n    SPDNet operates on covariance matrices and can use higher learning rates.\n    It performs a single BiMap + ReEig + LogEig transformation.\n\n    Parameters\n    ----------\n    input_type : str, default=\"raw\"\n        Type of input data. \"raw\" computes covariance internally,\n        \"cov\" expects pre-computed covariance matrices.\n    subspacedim : int, optional\n        Output dimension of BiMap layer. If None, uses n_chans.\n    threshold : float, default=1e-4\n        Eigenvalue threshold for ReEig layer to ensure numerical stability.\n    upper : bool, default=True\n        If True, use only upper triangular part in LogEig output.\n    \"\"\"\n\n    name: str = \"SPDNet\"\n    input_type: str = \"raw\"\n    subspacedim: Optional[int] = None\n    threshold: float = 1e-4\n    upper: bool = True\n\n\n@dataclass\nclass TSMNetConfig(ModelConfig):\n    \"\"\"Configuration for TSMNet model.\n\n    TSMNet (Tangent Space Mapping Network) combines convolutional feature\n    extraction with SPD processing and SPDBatchNormMeanVar :cite:p:`kobler2022spd` for\n    domain adaptation.\n\n    .. note::\n\n       TSMNet requires lower learning rates (1e-4) and more epochs (100+)\n       compared to SPDNet for stable training on the Riemannian manifold.\n\n    Parameters\n    ----------\n    n_temp_filters : int, default=8\n        Number of temporal convolution filters. More filters capture\n        richer temporal dynamics but increase computation.\n    temp_kernel_length : int, default=50\n        Length of temporal convolution kernel. At 250Hz, 50 samples = 200ms.\n    n_spatiotemp_filters : int, default=32\n        Number of spatiotemporal filters after the spatial convolution.\n    n_bimap_filters : int, default=16\n        Output dimension of the BiMap layer. Controls the SPD manifold dimension.\n    reeig_threshold : float, default=1e-4\n        Eigenvalue threshold for ReEig to prevent numerical instability.\n    \"\"\"\n\n    name: str = \"TSMNet\"\n    n_temp_filters: int = 8\n    temp_kernel_length: int = 50\n    n_spatiotemp_filters: int = 32\n    n_bimap_filters: int = 16\n    reeig_threshold: float = 1e-4\n\n\n@dataclass\nclass EEGSPDNetConfig(ModelConfig):\n    \"\"\"Configuration for EEGSPDNet model.\n\n    EEGSPDNet uses channel-specific convolutions followed by covariance pooling\n    and multiple BiMap layers for hierarchical SPD feature learning.\n\n    .. note::\n\n       EEGSPDNet requires lower learning rates (1e-4) and sufficient epochs\n       for the channel-specific convolutions to learn meaningful features.\n\n    Parameters\n    ----------\n    n_filters : int, default=4\n        Number of convolutional filters per channel. Total filters = n_filters * n_chans.\n    bimap_sizes : tuple, default=(2, 2)\n        Tuple of (scale_factor, n_layers). Creates n_layers BiMap layers,\n        each reducing dimension by scale_factor.\n    filter_time_length : int, default=25\n        Length of temporal filter. At 250Hz, 25 samples = 100ms.\n    spd_drop_prob : float, default=0.0\n        Dropout probability for SPDDropout layers. Set to 0 for stability.\n    final_layer_drop_prob : float, default=0.5\n        Standard dropout probability before the final classifier.\n    \"\"\"\n\n    name: str = \"EEGSPDNet\"\n    n_filters: int = 4\n    bimap_sizes: tuple = (2, 2)\n    filter_time_length: int = 25\n    spd_drop_prob: float = 0.0\n    final_layer_drop_prob: float = 0.5\n\n\n@dataclass\nclass TensorCSPNetConfig(ModelConfig):\n    \"\"\"Configuration for TensorCSPNet model.\n\n    TensorCSPNet is designed for filter bank paradigms, processing\n    multi-frequency covariance tensors.\n\n    .. note::\n\n       TensorCSPNet requires FilterBankMotorImagery paradigm for data loading.\n\n    Parameters\n    ----------\n    n_patches : int, default=4\n        Number of temporal patches for local covariance computation.\n    n_freqs : int, default=9\n        Number of frequency bands. Must match the filter bank configuration.\n    use_mlp : bool, default=False\n        If True, use MLP instead of TCN for classification.\n    tcn_channels : int, default=16\n        Number of channels in TCN blocks.\n    dims : tuple, default=(22, 36, 36, 22)\n        Dimensions for BiMap layers in the network.\n    \"\"\"\n\n    name: str = \"TensorCSPNet\"\n    n_patches: int = 4\n    n_freqs: int = 9\n    use_mlp: bool = False\n    tcn_channels: int = 16\n    dims: tuple = (22, 36, 36, 22)\n\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Configuration for training parameters.\n\n    This dataclass contains all hyperparameters related to the training process.\n    Note that model-specific overrides may be applied for optimal performance.\n\n    Parameters\n    ----------\n    batch_size : int, default=32\n        Number of samples per training batch.\n    max_epochs : int, default=150\n        Maximum number of training epochs.\n    learning_rate : float, default=1e-3\n        Initial learning rate for the optimizer.\n    weight_decay : float, default=1e-4\n        L2 regularization strength.\n    gradient_clip_value : float, default=1.0\n        Maximum gradient norm for gradient clipping. Essential for SPD networks.\n    early_stopping_patience : int, default=30\n        Number of epochs without improvement before stopping.\n    lr_patience : int, default=15\n        Number of epochs without improvement before reducing learning rate.\n    lr_factor : float, default=0.5\n        Factor by which to reduce learning rate on plateau.\n    min_lr : float, default=1e-6\n        Minimum learning rate after reductions.\n    validation_split : float, default=0.1\n        Fraction of training data to use for validation.\n    seed : int, default=42\n        Random seed for reproducibility.\n    \"\"\"\n\n    batch_size: int = 32\n    max_epochs: int = 150\n    learning_rate: float = 1e-3\n    weight_decay: float = 1e-4\n    gradient_clip_value: float = 1.0\n    early_stopping_patience: int = 30\n    lr_patience: int = 15\n    lr_factor: float = 0.5\n    min_lr: float = 1e-6\n    validation_split: float = 0.1\n    seed: int = 42\n\n\n@dataclass\nclass ModelTrainingOverrides:\n    \"\"\"Model-specific training parameter overrides.\n\n    Different SPD models require different training configurations for optimal\n    performance. This dataclass defines overrides for each model type.\n\n    Parameters\n    ----------\n    learning_rate : float, optional\n        Override learning rate for this model.\n    max_epochs : int, optional\n        Override maximum epochs for this model.\n    batch_size : int, optional\n        Override batch size for this model.\n    optimizer : str, default=\"AdamW\"\n        Optimizer to use (\"Adam\" or \"AdamW\").\n    \"\"\"\n\n    learning_rate: Optional[float] = None\n    max_epochs: Optional[int] = None\n    batch_size: Optional[int] = None\n    optimizer: str = \"AdamW\"\n\n\n@dataclass\nclass DataConfig:\n    \"\"\"Configuration for dataset and paradigm.\n\n    Parameters\n    ----------\n    dataset_name : str, default=\"BNCI2014_001\"\n        Name of the MOABB dataset to use.\n    subjects : List[int], default=[1]\n        List of subject IDs to include in the benchmark.\n    n_classes : int, default=4\n        Number of motor imagery classes.\n    paradigm : str, default=\"MotorImagery\"\n        Paradigm type (\"MotorImagery\" or \"FilterBankMotorImagery\").\n    filters : List[List[int]], optional\n        Filter bank specification for FilterBankMotorImagery.\n    resample : float, optional\n        Resampling frequency in Hz. None keeps original sampling rate.\n    fmin : float, default=4.0\n        Lower frequency bound for bandpass filter.\n    fmax : float, default=38.0\n        Upper frequency bound for bandpass filter.\n    \"\"\"\n\n    dataset_name: str = \"BNCI2014_001\"\n    subjects: List[int] = field(default_factory=lambda: [1])\n    n_classes: int = 4\n    paradigm: str = \"MotorImagery\"  # or \"FilterBankMotorImagery\"\n    filters: Optional[List[List[int]]] = None\n    resample: Optional[float] = None\n    fmin: float = 4.0\n    fmax: float = 38.0\n\n\n@dataclass\nclass ExperimentConfig:\n    \"\"\"Main experiment configuration.\n\n    This is the top-level configuration that combines all other configurations\n    and defines the overall experiment structure.\n\n    Parameters\n    ----------\n    training : TrainingConfig\n        Training hyperparameters.\n    data : DataConfig\n        Dataset and paradigm configuration.\n    models : List[str], default=[\"SPDNet\", \"TSMNet\", \"EEGSPDNet\"]\n        List of model names to benchmark.\n    model_training_overrides : Dict[str, ModelTrainingOverrides], optional\n        Model-specific training parameter overrides.\n    n_folds : int, default=5\n        Number of cross-validation folds.\n    use_session_split : bool, default=True\n        If True, use session-based split (train on session 0, test on session 1).\n    device : str, default=\"auto\"\n        Device for training (\"auto\", \"cpu\", or \"cuda\").\n    checkpoint_dir : str, optional\n        Directory to save model checkpoints. If None, uses a temp directory.\n    \"\"\"\n\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n    data: DataConfig = field(default_factory=DataConfig)\n    models: List[str] = field(default_factory=lambda: [\"SPDNet\", \"TSMNet\", \"EEGSPDNet\"])\n    model_training_overrides: Dict[str, Any] = field(default_factory=dict)\n    n_folds: int = 5\n    use_session_split: bool = True\n    device: str = \"auto\"\n    checkpoint_dir: Optional[str] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Factory\n\nWe create a factory that generates model instances from configurations.\nThis pattern allows easy switching between models via configuration.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_model(\n    model_name: str,\n    n_chans: int,\n    n_outputs: int,\n    **kwargs: Any,\n) -> nn.Module:\n    \"\"\"Create a model instance from configuration.\n\n    This factory function instantiates the appropriate SPD Learn model\n    based on the provided name and configuration parameters.\n\n    Parameters\n    ----------\n    model_name : str\n        Name of the model to create. Supported: \"SPDNet\", \"TSMNet\",\n        \"EEGSPDNet\", \"TensorCSPNet\".\n    n_chans : int\n        Number of input EEG channels.\n    n_outputs : int\n        Number of output classes.\n    **kwargs : Any\n        Additional model-specific parameters. See individual model\n        configurations for available parameters.\n\n    Returns\n    -------\n    nn.Module\n        Instantiated PyTorch model ready for training.\n\n    Raises\n    ------\n    ValueError\n        If an unknown model name is provided.\n\n    Examples\n    --------\n    >>> model = create_model(\"SPDNet\", n_chans=22, n_outputs=4)\n    >>> model = create_model(\"TSMNet\", n_chans=22, n_outputs=4, n_temp_filters=8)\n    \"\"\"\n    if model_name == \"SPDNet\":\n        return SPDNet(\n            n_chans=n_chans,\n            n_outputs=n_outputs,\n            input_type=kwargs.get(\"input_type\", \"raw\"),\n            subspacedim=kwargs.get(\"subspacedim\", n_chans),\n            threshold=kwargs.get(\"threshold\", 1e-4),\n            upper=kwargs.get(\"upper\", True),\n        )\n    elif model_name == \"TSMNet\":\n        return TSMNet(\n            n_chans=n_chans,\n            n_outputs=n_outputs,\n            n_temp_filters=kwargs.get(\"n_temp_filters\", 8),\n            temp_kernel_length=kwargs.get(\"temp_kernel_length\", 50),\n            n_spatiotemp_filters=kwargs.get(\"n_spatiotemp_filters\", 32),\n            n_bimap_filters=kwargs.get(\"n_bimap_filters\", 16),\n            reeig_threshold=kwargs.get(\"reeig_threshold\", 1e-4),\n        )\n    elif model_name == \"EEGSPDNet\":\n        return EEGSPDNet(\n            n_chans=n_chans,\n            n_outputs=n_outputs,\n            n_filters=kwargs.get(\"n_filters\", 4),\n            bimap_sizes=kwargs.get(\"bimap_sizes\", (2, 2)),\n            filter_time_length=kwargs.get(\"filter_time_length\", 25),\n            spd_drop_prob=kwargs.get(\"spd_drop_prob\", 0.0),\n            final_layer_drop_prob=kwargs.get(\"final_layer_drop_prob\", 0.5),\n        )\n    elif model_name == \"TensorCSPNet\":\n        # TensorCSPNet requires special input format handling\n        n_freqs = kwargs.get(\"n_freqs\", 9)\n        model = nn.Sequential(\n            Rearrange(\"b c t f -> b f c t\"),\n            TensorCSPNet(\n                n_chans=n_chans,\n                n_outputs=n_outputs,\n                n_patches=kwargs.get(\"n_patches\", 4),\n                n_freqs=n_freqs,\n                use_mlp=kwargs.get(\"use_mlp\", False),\n                tcn_channels=kwargs.get(\"tcn_channels\", 16),\n                dims=kwargs.get(\"dims\", (n_chans, 36, 36, n_chans)),\n            ),\n        )\n        return model\n    else:\n        raise ValueError(\n            f\"Unknown model: {model_name}. \"\n            f\"Supported models: SPDNet, TSMNet, EEGSPDNet, TensorCSPNet\"\n        )\n\n\ndef get_default_model_training_overrides() -> Dict[str, Dict[str, Any]]:\n    \"\"\"Get default training parameter overrides for each model.\n\n    Different SPD models require different training configurations:\n\n    - **SPDNet**: Works on covariances, can use higher learning rates\n    - **TSMNet**: Needs lower learning rate and more epochs for SPDBatchNormMeanVar\n    - **EEGSPDNet**: Needs lower learning rate for channel-specific convolutions\n\n    Returns\n    -------\n    Dict[str, Dict[str, Any]]\n        Dictionary mapping model names to their training overrides.\n\n    Examples\n    --------\n    >>> overrides = get_default_model_training_overrides()\n    >>> overrides[\"TSMNet\"][\"learning_rate\"]\n    0.0001\n    \"\"\"\n    return {\n        \"SPDNet\": {\n            \"learning_rate\": 1e-3,\n            \"max_epochs\": 20,  # Reduced from 100 for faster documentation build\n            \"optimizer\": \"AdamW\",\n        },\n        \"TSMNet\": {\n            # TSMNet requires lower LR for stable SPD learning\n            # Reference: plot_tsmnet_domain_adaptation.py\n            \"learning_rate\": 1e-4,\n            \"max_epochs\": 30,  # Reduced from 150 for faster documentation build\n            \"optimizer\": \"Adam\",\n        },\n        \"EEGSPDNet\": {\n            # EEGSPDNet requires lower LR for channel-specific convolutions\n            # Reference: plot_eegspdnet.py\n            \"learning_rate\": 1e-4,\n            \"max_epochs\": 30,  # Reduced from 150 for faster documentation build\n            \"optimizer\": \"Adam\",\n        },\n        \"TensorCSPNet\": {\n            \"learning_rate\": 1e-3,\n            \"max_epochs\": 20,  # Reduced from 100 for faster documentation build\n            \"optimizer\": \"AdamW\",\n        },\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Hydra YAML Configuration\n\nIn a production setting, you would store configurations in YAML files.\nHere's an example of what such a configuration file might look like:\n\n```yaml\n# config/experiment/benchmark.yaml\ndefaults:\n  - _self_\n  - training: default\n  - data: bnci2014_001\n\nmodels:\n  - SPDNet\n  - TSMNet\n  - EEGSPDNet\n\nn_folds: 5\nuse_session_split: true\ndevice: auto\n\n# Model-specific training overrides\n# These are CRITICAL for TSMNet and EEGSPDNet to train properly\nmodel_training_overrides:\n  SPDNet:\n    learning_rate: 1e-3\n    max_epochs: 100\n    optimizer: AdamW\n  TSMNet:\n    learning_rate: 1e-4  # Lower LR for stable SPD learning\n    max_epochs: 150      # More epochs needed\n    optimizer: Adam\n  EEGSPDNet:\n    learning_rate: 1e-4  # Lower LR for channel convolutions\n    max_epochs: 150\n    optimizer: Adam\n\n# Override model hyperparameters\nmodel_params:\n  SPDNet:\n    subspacedim: null  # Use n_chans\n    threshold: 1e-4\n  TSMNet:\n    n_temp_filters: 8\n    temp_kernel_length: 50\n    n_spatiotemp_filters: 32\n    n_bimap_filters: 16\n  EEGSPDNet:\n    n_filters: 4\n    bimap_sizes: [2, 2]\n```\n```yaml\n# config/training/default.yaml\nbatch_size: 32\nmax_epochs: 150\nlearning_rate: 1e-3\nweight_decay: 1e-4\ngradient_clip_value: 1.0\nearly_stopping_patience: 30\nlr_patience: 15\nlr_factor: 0.5\nmin_lr: 1e-6\nvalidation_split: 0.1\nseed: 42\n```\n```yaml\n# config/data/bnci2014_001.yaml\ndataset_name: BNCI2014_001\nsubjects: [1, 2]\nn_classes: 4\nparadigm: MotorImagery\nfmin: 4.0\nfmax: 38.0\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up the Benchmark Configuration\n\nWe create a configuration using OmegaConf, which provides the same\nfunctionality as Hydra YAML files but defined programmatically.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create experiment configuration\nconfig = OmegaConf.structured(\n    ExperimentConfig(\n        training=TrainingConfig(\n            batch_size=32,\n            max_epochs=30,  # Reduced from 150 for faster documentation build\n            learning_rate=1e-3,\n            weight_decay=1e-4,\n            gradient_clip_value=1.0,\n            early_stopping_patience=30,\n            lr_patience=15,\n            lr_factor=0.5,\n            validation_split=0.1,\n            seed=42,\n        ),\n        data=DataConfig(\n            dataset_name=\"BNCI2014_001\",\n            subjects=[1],  # Single subject for faster demonstration\n            n_classes=4,\n            paradigm=\"MotorImagery\",\n            fmin=4.0,\n            fmax=38.0,\n        ),\n        models=[\"SPDNet\", \"TSMNet\", \"EEGSPDNet\"],\n        n_folds=1,  # Reduced from 3 for faster documentation build\n        use_session_split=True,\n        device=\"auto\",\n    )\n)\n\nprint(\"Experiment Configuration:\")\nprint(OmegaConf.to_yaml(config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Dataset\n\nWe use MOABB to load the dataset with the configured parameters.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Determine device\ndevice = (\n    \"cuda\"\n    if config.device == \"auto\" and torch.cuda.is_available()\n    else config.device\n    if config.device != \"auto\"\n    else \"cpu\"\n)\nprint(f\"\\nUsing device: {device}\")\n\n# Set random seeds for reproducibility\ntorch.manual_seed(config.training.seed)\nnp.random.seed(config.training.seed)\n\n# Load dataset\ndataset = BNCI2014_001()\nparadigm = MotorImagery(\n    n_classes=config.data.n_classes,\n    fmin=config.data.fmin,\n    fmax=config.data.fmax,\n)\n\n# Cache configuration for faster repeated runs\n# Note: Cross-platform compatible cache configuration\n# Set use=False if you encounter caching issues with older MOABB versions\n# or on systems where the cache directory is not accessible\ncache_config = dict(\n    save_raw=False,\n    save_epochs=False,\n    save_array=True,\n    use=False,  # Disable cache to avoid preload issues on some systems\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n)\n\nprint(f\"\\nLoading dataset: {config.data.dataset_name}\")\nprint(f\"Subjects: {config.data.subjects}\")\nprint(f\"Paradigm: {config.data.paradigm}\")\n\nX, labels, meta = paradigm.get_data(\n    dataset=dataset,\n    subjects=list(config.data.subjects),  # Convert OmegaConf list to Python list\n    cache_config=cache_config,\n)\n\n# Encode labels\nle = LabelEncoder()\ny = le.fit_transform(labels)\n\nn_chans = X.shape[1]\nn_outputs = len(le.classes_)\n\nprint(f\"\\nData shape: {X.shape}\")\nprint(f\"Classes: {le.classes_}\")\nprint(f\"Number of channels: {n_chans}\")\nprint(f\"Number of classes: {n_outputs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the Benchmark Pipeline\n\nWe define a benchmarking class that encapsulates the evaluation logic.\nThis makes it easy to run experiments with different configurations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SPDLearnBenchmark:\n    \"\"\"Benchmark pipeline for SPD Learn models.\n\n    This class provides a structured way to evaluate multiple models\n    on EEG datasets using cross-validation with proper training configurations\n    for each model type.\n\n    The benchmark supports:\n\n    - Session-based splits (train on session 0, test on session 1)\n    - K-fold cross-validation\n    - Model-specific training configurations\n    - Early stopping and learning rate scheduling\n    - Model checkpointing\n    - Per-class accuracy breakdown\n\n    Parameters\n    ----------\n    config : ExperimentConfig\n        Experiment configuration containing training, data, and model settings.\n    X : np.ndarray\n        Input data of shape (n_samples, n_channels, n_times).\n    y : np.ndarray\n        Labels of shape (n_samples,).\n    meta : pd.DataFrame\n        Metadata containing session and subject information.\n    device : str, default=\"cpu\"\n        Device to use for training (\"cpu\" or \"cuda\").\n    label_encoder : LabelEncoder, optional\n        Fitted label encoder for class names.\n\n    Attributes\n    ----------\n    results : List[Dict[str, Any]]\n        List of evaluation results for each model.\n\n    Examples\n    --------\n    >>> benchmark = SPDLearnBenchmark(config, X, y, meta, device=\"cuda\")\n    >>> results_df = benchmark.run_benchmark(model_configs)\n    >>> print(results_df[[\"Model\", \"Accuracy\", \"Balanced Accuracy\"]])\n    \"\"\"\n\n    def __init__(\n        self,\n        config: ExperimentConfig,\n        X: np.ndarray,\n        y: np.ndarray,\n        meta: pd.DataFrame,\n        device: str = \"cpu\",\n        label_encoder: Optional[LabelEncoder] = None,\n    ) -> None:\n        \"\"\"Initialize the benchmark pipeline.\"\"\"\n        self.config = config\n        self.X = X\n        self.y = y\n        self.meta = meta\n        self.device = device\n        self.label_encoder = label_encoder\n        self.results: List[Dict[str, Any]] = []\n        self._checkpoint_dir = config.checkpoint_dir or tempfile.mkdtemp()\n\n        # Get default training overrides and merge with config overrides\n        self.model_training_overrides = get_default_model_training_overrides()\n        if (\n            hasattr(config, \"model_training_overrides\")\n            and config.model_training_overrides\n        ):\n            for model_name, overrides in config.model_training_overrides.items():\n                if model_name in self.model_training_overrides:\n                    self.model_training_overrides[model_name].update(overrides)\n                else:\n                    self.model_training_overrides[model_name] = overrides\n\n    def _get_optimizer_class(self, optimizer_name: str) -> type:\n        \"\"\"Get the optimizer class from its name.\n\n        Parameters\n        ----------\n        optimizer_name : str\n            Name of the optimizer (\"Adam\" or \"AdamW\").\n\n        Returns\n        -------\n        type\n            PyTorch optimizer class.\n\n        Raises\n        ------\n        ValueError\n            If an unknown optimizer name is provided.\n        \"\"\"\n        optimizers = {\n            \"Adam\": torch.optim.Adam,\n            \"AdamW\": torch.optim.AdamW,\n            \"SGD\": torch.optim.SGD,\n        }\n        if optimizer_name not in optimizers:\n            raise ValueError(\n                f\"Unknown optimizer: {optimizer_name}. \"\n                f\"Supported: {list(optimizers.keys())}\"\n            )\n        return optimizers[optimizer_name]\n\n    def create_classifier(\n        self,\n        model: nn.Module,\n        model_name: str,\n        checkpoint_path: Optional[str] = None,\n    ) -> EEGClassifier:\n        \"\"\"Create an EEGClassifier with model-specific training parameters.\n\n        This method applies model-specific training overrides to ensure\n        optimal performance for each model type.\n\n        Parameters\n        ----------\n        model : nn.Module\n            PyTorch model to wrap.\n        model_name : str\n            Name of the model (used to look up training overrides).\n        checkpoint_path : str, optional\n            Path to save model checkpoints.\n\n        Returns\n        -------\n        EEGClassifier\n            Configured classifier ready for training.\n        \"\"\"\n        # Get model-specific training overrides\n        overrides = self.model_training_overrides.get(model_name, {})\n\n        # Apply overrides or use defaults\n        learning_rate = overrides.get(\n            \"learning_rate\", self.config.training.learning_rate\n        )\n        max_epochs = overrides.get(\"max_epochs\", self.config.training.max_epochs)\n        batch_size = overrides.get(\"batch_size\", self.config.training.batch_size)\n        optimizer_name = overrides.get(\"optimizer\", \"AdamW\")\n        optimizer_class = self._get_optimizer_class(optimizer_name)\n\n        # Build callbacks\n        callbacks = [\n            (\n                \"train_acc\",\n                EpochScoring(\n                    \"accuracy\",\n                    lower_is_better=False,\n                    on_train=True,\n                    name=\"train_acc\",\n                ),\n            ),\n            (\n                \"gradient_clip\",\n                GradientNormClipping(\n                    gradient_clip_value=self.config.training.gradient_clip_value\n                ),\n            ),\n            # Learning rate scheduler - reduce on plateau\n            (\n                \"lr_scheduler\",\n                LRScheduler(\n                    policy=ReduceLROnPlateau,\n                    mode=\"min\",\n                    factor=self.config.training.lr_factor,\n                    patience=self.config.training.lr_patience,\n                    min_lr=self.config.training.min_lr,\n                    monitor=\"valid_loss\",\n                ),\n            ),\n            # Early stopping\n            (\n                \"early_stopping\",\n                EarlyStopping(\n                    monitor=\"valid_loss\",\n                    patience=self.config.training.early_stopping_patience,\n                    threshold=1e-4,\n                    threshold_mode=\"rel\",\n                    lower_is_better=True,\n                ),\n            ),\n        ]\n\n        # Add checkpointing if path is provided\n        if checkpoint_path:\n            callbacks.append(\n                (\n                    \"checkpoint\",\n                    Checkpoint(\n                        monitor=\"valid_loss_best\",\n                        f_pickle=None,\n                        dirname=os.path.dirname(checkpoint_path),\n                        f_params=os.path.basename(checkpoint_path),\n                    ),\n                )\n            )\n\n        return EEGClassifier(\n            model,\n            criterion=torch.nn.CrossEntropyLoss,\n            optimizer=optimizer_class,\n            optimizer__lr=learning_rate,\n            optimizer__weight_decay=self.config.training.weight_decay,\n            train_split=ValidSplit(\n                self.config.training.validation_split,\n                stratified=True,\n                random_state=self.config.training.seed,\n            ),\n            batch_size=batch_size,\n            max_epochs=max_epochs,\n            callbacks=callbacks,\n            device=self.device,\n            verbose=0,  # Reduce verbosity for benchmark\n        )\n\n    def _compute_per_class_accuracy(\n        self, y_true: np.ndarray, y_pred: np.ndarray\n    ) -> Dict[str, float]:\n        \"\"\"Compute per-class accuracy.\n\n        Parameters\n        ----------\n        y_true : np.ndarray\n            True labels.\n        y_pred : np.ndarray\n            Predicted labels.\n\n        Returns\n        -------\n        Dict[str, float]\n            Dictionary mapping class names to their accuracies.\n        \"\"\"\n        per_class_acc = {}\n        unique_classes = np.unique(y_true)\n\n        for cls in unique_classes:\n            mask = y_true == cls\n            if mask.sum() > 0:\n                cls_acc = (y_pred[mask] == y_true[mask]).mean()\n                # Get class name if label encoder is available\n                if self.label_encoder is not None:\n                    cls_name = self.label_encoder.classes_[cls]\n                else:\n                    cls_name = str(cls)\n                per_class_acc[cls_name] = cls_acc\n\n        return per_class_acc\n\n    def evaluate_model(\n        self,\n        model_name: str,\n        model_params: Optional[Dict[str, Any]] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Evaluate a single model using cross-validation.\n\n        This method handles:\n\n        - Creating fresh model instances for each fold\n        - Applying model-specific training configurations\n        - Computing accuracy, balanced accuracy, and per-class metrics\n        - Saving training history and confusion matrices\n\n        Parameters\n        ----------\n        model_name : str\n            Name of the model to evaluate.\n        model_params : dict, optional\n            Additional model parameters to override defaults.\n\n        Returns\n        -------\n        dict\n            Dictionary containing evaluation results including:\n\n            - model: Model name\n            - mean_accuracy: Mean accuracy across folds\n            - std_accuracy: Standard deviation of accuracy\n            - mean_balanced_accuracy: Mean balanced accuracy\n            - std_balanced_accuracy: Standard deviation of balanced accuracy\n            - per_class_accuracy: Per-class accuracy breakdown\n            - fold_results: Detailed results for each fold\n            - model_params: Parameters used for the model\n            - training_overrides: Training parameters used\n\n        Raises\n        ------\n        RuntimeError\n            If training fails for all folds.\n        \"\"\"\n        model_params = model_params or {}\n        n_chans = self.X.shape[1]\n\n        # Get training overrides for display\n        overrides = self.model_training_overrides.get(model_name, {})\n        lr = overrides.get(\"learning_rate\", self.config.training.learning_rate)\n        epochs = overrides.get(\"max_epochs\", self.config.training.max_epochs)\n        optimizer = overrides.get(\"optimizer\", \"AdamW\")\n\n        print(f\"\\n{'=' * 60}\")\n        print(f\"Evaluating: {model_name}\")\n        print(f\"{'=' * 60}\")\n        print(f\"Training config: lr={lr}, max_epochs={epochs}, optimizer={optimizer}\")\n\n        fold_results = []\n        all_y_true = []\n        all_y_pred = []\n        failed_folds = []\n\n        if self.config.use_session_split:\n            # Use session-based split (train on session 0, test on session 1)\n            train_idx = self.meta.query(\"session == '0train'\").index.to_numpy()\n            test_idx = self.meta.query(\"session == '1test'\").index.to_numpy()\n\n            try:\n                # Create fresh model\n                model = create_model(\n                    model_name,\n                    n_chans=n_chans,\n                    n_outputs=n_outputs,\n                    **model_params,\n                )\n\n                checkpoint_path = os.path.join(\n                    self._checkpoint_dir, f\"{model_name}_fold0_best.pt\"\n                )\n                clf = self.create_classifier(model, model_name, checkpoint_path)\n\n                print(f\"Training on {len(train_idx)} samples...\")\n                clf.fit(self.X[train_idx], self.y[train_idx])\n\n                # Get actual epochs trained (may stop early)\n                actual_epochs = len(clf.history)\n                print(f\"Training completed in {actual_epochs} epochs\")\n\n                # Evaluate\n                y_pred = clf.predict(self.X[test_idx])\n                acc = accuracy_score(self.y[test_idx], y_pred)\n                bal_acc = balanced_accuracy_score(self.y[test_idx], y_pred)\n                per_class_acc = self._compute_per_class_accuracy(\n                    self.y[test_idx], y_pred\n                )\n\n                # Store predictions for confusion matrix\n                all_y_true.extend(self.y[test_idx])\n                all_y_pred.extend(y_pred)\n\n                fold_results.append(\n                    {\n                        \"fold\": 0,\n                        \"accuracy\": acc,\n                        \"balanced_accuracy\": bal_acc,\n                        \"per_class_accuracy\": per_class_acc,\n                        \"n_train\": len(train_idx),\n                        \"n_test\": len(test_idx),\n                        \"actual_epochs\": actual_epochs,\n                        \"history\": clf.history,\n                    }\n                )\n\n                print(f\"  Accuracy: {acc:.4f}, Balanced Acc: {bal_acc:.4f}\")\n                print(f\"  Per-class: {per_class_acc}\")\n\n            except Exception as e:\n                print(f\"  ERROR: Training failed - {str(e)}\")\n                failed_folds.append((0, str(e)))\n\n        else:\n            # Use k-fold cross-validation\n            skf = StratifiedKFold(\n                n_splits=self.config.n_folds,\n                shuffle=True,\n                random_state=self.config.training.seed,\n            )\n\n            for fold_idx, (train_idx, test_idx) in enumerate(skf.split(self.X, self.y)):\n                print(f\"\\nFold {fold_idx + 1}/{self.config.n_folds}\")\n\n                try:\n                    # Create fresh model for each fold\n                    model = create_model(\n                        model_name,\n                        n_chans=n_chans,\n                        n_outputs=n_outputs,\n                        **model_params,\n                    )\n\n                    checkpoint_path = os.path.join(\n                        self._checkpoint_dir,\n                        f\"{model_name}_fold{fold_idx}_best.pt\",\n                    )\n                    clf = self.create_classifier(model, model_name, checkpoint_path)\n\n                    print(f\"  Training on {len(train_idx)} samples...\")\n                    clf.fit(self.X[train_idx], self.y[train_idx])\n\n                    # Get actual epochs trained\n                    actual_epochs = len(clf.history)\n                    print(f\"  Completed in {actual_epochs} epochs\")\n\n                    # Evaluate\n                    y_pred = clf.predict(self.X[test_idx])\n                    acc = accuracy_score(self.y[test_idx], y_pred)\n                    bal_acc = balanced_accuracy_score(self.y[test_idx], y_pred)\n                    per_class_acc = self._compute_per_class_accuracy(\n                        self.y[test_idx], y_pred\n                    )\n\n                    # Store predictions for confusion matrix\n                    all_y_true.extend(self.y[test_idx])\n                    all_y_pred.extend(y_pred)\n\n                    fold_results.append(\n                        {\n                            \"fold\": fold_idx,\n                            \"accuracy\": acc,\n                            \"balanced_accuracy\": bal_acc,\n                            \"per_class_accuracy\": per_class_acc,\n                            \"n_train\": len(train_idx),\n                            \"n_test\": len(test_idx),\n                            \"actual_epochs\": actual_epochs,\n                            \"history\": clf.history,\n                        }\n                    )\n\n                    print(f\"  Accuracy: {acc:.4f}, Balanced Acc: {bal_acc:.4f}\")\n\n                except Exception as e:\n                    print(f\"  ERROR: Fold {fold_idx + 1} failed - {str(e)}\")\n                    failed_folds.append((fold_idx, str(e)))\n\n        # Check if any folds succeeded\n        if not fold_results:\n            print(f\"\\nWARNING: All folds failed for {model_name}\")\n            result = {\n                \"model\": model_name,\n                \"mean_accuracy\": 0.0,\n                \"std_accuracy\": 0.0,\n                \"mean_balanced_accuracy\": 0.0,\n                \"std_balanced_accuracy\": 0.0,\n                \"per_class_accuracy\": {},\n                \"fold_results\": [],\n                \"model_params\": model_params,\n                \"training_overrides\": overrides,\n                \"failed_folds\": failed_folds,\n                \"confusion_matrix\": None,\n            }\n            self.results.append(result)\n            return result\n\n        # Aggregate results\n        # Compute aggregate per-class accuracy\n        agg_per_class_acc = {}\n        for fold_result in fold_results:\n            for cls_name, acc in fold_result[\"per_class_accuracy\"].items():\n                if cls_name not in agg_per_class_acc:\n                    agg_per_class_acc[cls_name] = []\n                agg_per_class_acc[cls_name].append(acc)\n\n        mean_per_class_acc = {\n            cls: np.mean(accs) for cls, accs in agg_per_class_acc.items()\n        }\n\n        # Compute confusion matrix\n        cm = None\n        if all_y_true and all_y_pred:\n            cm = confusion_matrix(all_y_true, all_y_pred)\n\n        result = {\n            \"model\": model_name,\n            \"mean_accuracy\": np.mean([r[\"accuracy\"] for r in fold_results]),\n            \"std_accuracy\": np.std([r[\"accuracy\"] for r in fold_results]),\n            \"mean_balanced_accuracy\": np.mean(\n                [r[\"balanced_accuracy\"] for r in fold_results]\n            ),\n            \"std_balanced_accuracy\": np.std(\n                [r[\"balanced_accuracy\"] for r in fold_results]\n            ),\n            \"per_class_accuracy\": mean_per_class_acc,\n            \"fold_results\": fold_results,\n            \"model_params\": model_params,\n            \"training_overrides\": overrides,\n            \"failed_folds\": failed_folds,\n            \"confusion_matrix\": cm,\n        }\n\n        self.results.append(result)\n        return result\n\n    def run_benchmark(\n        self, model_configs: Optional[Dict[str, Dict[str, Any]]] = None\n    ) -> pd.DataFrame:\n        \"\"\"Run benchmark on all configured models.\n\n        Parameters\n        ----------\n        model_configs : dict, optional\n            Dictionary mapping model names to their architecture parameters.\n            Training parameters are handled separately via model_training_overrides.\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame containing benchmark results for all models.\n\n        Examples\n        --------\n        >>> model_configs = {\n        ...     \"SPDNet\": {\"subspacedim\": 22},\n        ...     \"TSMNet\": {\"n_temp_filters\": 8},\n        ... }\n        >>> results_df = benchmark.run_benchmark(model_configs)\n        \"\"\"\n        model_configs = model_configs or {}\n\n        for model_name in self.config.models:\n            params = model_configs.get(model_name, {})\n            try:\n                self.evaluate_model(model_name, params)\n            except Exception as e:\n                print(f\"\\nERROR: Failed to evaluate {model_name}: {str(e)}\")\n                # Add a placeholder result\n                self.results.append(\n                    {\n                        \"model\": model_name,\n                        \"mean_accuracy\": 0.0,\n                        \"std_accuracy\": 0.0,\n                        \"mean_balanced_accuracy\": 0.0,\n                        \"std_balanced_accuracy\": 0.0,\n                        \"per_class_accuracy\": {},\n                        \"fold_results\": [],\n                        \"model_params\": params,\n                        \"training_overrides\": {},\n                        \"failed_folds\": [(0, str(e))],\n                        \"confusion_matrix\": None,\n                    }\n                )\n\n        return self.get_results_dataframe()\n\n    def get_results_dataframe(self) -> pd.DataFrame:\n        \"\"\"Get results as a pandas DataFrame.\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame with benchmark results including accuracy metrics,\n            training configuration, and per-class breakdown.\n        \"\"\"\n        records = []\n        for r in self.results:\n            # Format per-class accuracy\n            per_class_str = \", \".join(\n                [f\"{cls}: {acc:.2f}\" for cls, acc in r[\"per_class_accuracy\"].items()]\n            )\n\n            # Get training info\n            overrides = r.get(\"training_overrides\", {})\n            lr = overrides.get(\"learning_rate\", self.config.training.learning_rate)\n            optimizer = overrides.get(\"optimizer\", \"AdamW\")\n\n            records.append(\n                {\n                    \"Model\": r[\"model\"],\n                    \"Accuracy\": f\"{r['mean_accuracy']:.4f} +/- {r['std_accuracy']:.4f}\",\n                    \"Balanced Accuracy\": f\"{r['mean_balanced_accuracy']:.4f} +/- {r['std_balanced_accuracy']:.4f}\",\n                    \"Mean Acc\": r[\"mean_accuracy\"],\n                    \"Std Acc\": r[\"std_accuracy\"],\n                    \"Per-Class Acc\": per_class_str,\n                    \"LR\": lr,\n                    \"Optimizer\": optimizer,\n                    \"Failed Folds\": len(r.get(\"failed_folds\", [])),\n                }\n            )\n        return pd.DataFrame(records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the Benchmark\n\nNow we run the benchmark with our configured models.\nNote the model-specific configurations for optimal performance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define model-specific architecture configurations\nmodel_configs = {\n    \"SPDNet\": {\n        \"subspacedim\": n_chans,\n        \"threshold\": 1e-4,\n    },\n    \"TSMNet\": {\n        # Architecture parameters from plot_tsmnet_domain_adaptation.py\n        \"n_temp_filters\": 8,\n        \"temp_kernel_length\": 50,  # 200ms at 250Hz\n        \"n_spatiotemp_filters\": 32,\n        \"n_bimap_filters\": 16,\n    },\n    \"EEGSPDNet\": {\n        # Architecture parameters from plot_eegspdnet.py\n        \"n_filters\": 4,\n        \"bimap_sizes\": (2, 2),\n        \"filter_time_length\": 25,  # 100ms at 250Hz\n        \"spd_drop_prob\": 0.0,  # Disable SPD dropout for stability\n        \"final_layer_drop_prob\": 0.5,\n    },\n}\n\n# Create benchmark instance\nbenchmark = SPDLearnBenchmark(\n    config=config,\n    X=X,\n    y=y,\n    meta=meta,\n    device=device,\n    label_encoder=le,\n)\n\n# Run benchmark\nresults_df = benchmark.run_benchmark(model_configs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n\nWe display the benchmark results in a formatted table with per-class\naccuracy breakdown.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\nprint(\"Benchmark Results Summary\")\nprint(\"=\" * 80)\nprint(\n    results_df[[\"Model\", \"Accuracy\", \"Balanced Accuracy\", \"LR\", \"Optimizer\"]].to_string(\n        index=False\n    )\n)\n\nprint(\"\\n\" + \"-\" * 80)\nprint(\"Per-Class Accuracy Breakdown\")\nprint(\"-\" * 80)\nfor r in benchmark.results:\n    print(f\"\\n{r['model']}:\")\n    for cls_name, acc in r[\"per_class_accuracy\"].items():\n        print(f\"  {cls_name}: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Benchmark Results\n\nWe create visualizations to compare model performance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Extract data for plotting\nmodels = [r[\"model\"] for r in benchmark.results]\nmean_accs = [r[\"mean_accuracy\"] for r in benchmark.results]\nstd_accs = [r[\"std_accuracy\"] for r in benchmark.results]\nmean_bal_accs = [r[\"mean_balanced_accuracy\"] for r in benchmark.results]\nstd_bal_accs = [r[\"std_balanced_accuracy\"] for r in benchmark.results]\n\n# Plot 1: Accuracy comparison with error bars\nax1 = axes[0]\nx_pos = np.arange(len(models))\nbar_width = 0.35\n\nbars1 = ax1.bar(\n    x_pos - bar_width / 2,\n    mean_accs,\n    bar_width,\n    yerr=std_accs,\n    label=\"Accuracy\",\n    color=\"#3498db\",\n    edgecolor=\"black\",\n    capsize=5,\n)\nbars2 = ax1.bar(\n    x_pos + bar_width / 2,\n    mean_bal_accs,\n    bar_width,\n    yerr=std_bal_accs,\n    label=\"Balanced Accuracy\",\n    color=\"#2ecc71\",\n    edgecolor=\"black\",\n    capsize=5,\n)\n\nax1.axhline(y=0.25, color=\"red\", linestyle=\"--\", label=\"Chance (4 classes)\", alpha=0.7)\nax1.set_xlabel(\"Model\", fontsize=12)\nax1.set_ylabel(\"Score\", fontsize=12)\nax1.set_title(\"Model Performance Comparison\", fontsize=14)\nax1.set_xticks(x_pos)\nax1.set_xticklabels(models, rotation=15, ha=\"right\")\nax1.set_ylim([0, 1])\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3, axis=\"y\")\n\n# Add value labels on bars\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.02,\n        f\"{height:.2f}\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=9,\n    )\n\n# Plot 2: Radar chart for multi-dimensional comparison\nax2 = axes[1]\n\n# Create ranking-based scores (normalized)\nmetrics = [\"Accuracy\", \"Bal. Accuracy\", \"Training Stability\"]\nn_metrics = len(metrics)\n\n# Calculate stability as inverse of std (lower std = more stable)\nmax_std = max(std_accs) if max(std_accs) > 0 else 1\nstability_scores = [1 - (s / max_std) if max_std > 0 else 1 for s in std_accs]\n\n# Create data for radar chart\nangles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()\nangles += angles[:1]  # Complete the loop\n\ncolors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#9b59b6\", \"#f39c12\"]\n\nfor idx, model in enumerate(models):\n    values = [mean_accs[idx], mean_bal_accs[idx], stability_scores[idx]]\n    values += values[:1]  # Complete the loop\n\n    ax2.plot(\n        angles,\n        values,\n        \"o-\",\n        linewidth=2,\n        label=model,\n        color=colors[idx % len(colors)],\n    )\n    ax2.fill(angles, values, alpha=0.1, color=colors[idx % len(colors)])\n\nax2.set_xticks(angles[:-1])\nax2.set_xticklabels(metrics, fontsize=10)\nax2.set_ylim([0, 1])\nax2.set_title(\"Multi-Metric Comparison\", fontsize=14)\nax2.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1), fontsize=10)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.suptitle(\"SPD Learn Model Benchmark Results\", fontsize=16, y=1.02)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-Fold Results Visualization\n\nWe can also visualize the per-fold results to understand variance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not config.use_session_split and len(benchmark.results[0][\"fold_results\"]) > 1:\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    for idx, result in enumerate(benchmark.results):\n        fold_accs = [r[\"accuracy\"] for r in result[\"fold_results\"]]\n        ax.plot(\n            range(1, len(fold_accs) + 1),\n            fold_accs,\n            \"o-\",\n            label=result[\"model\"],\n            linewidth=2,\n            markersize=8,\n        )\n\n    ax.set_xlabel(\"Fold\", fontsize=12)\n    ax.set_ylabel(\"Accuracy\", fontsize=12)\n    ax.set_title(\"Per-Fold Accuracy\", fontsize=14)\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.set_ylim([0, 1])\n\n    plt.tight_layout()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Hydra from Command Line\n\nIn a production environment, you would use Hydra's command-line\ninterface to manage configurations. Here's how you would structure\nyour project:\n\n```text\nproject/\n+-- config/\n|   +-- config.yaml           # Main config\n|   +-- training/\n|   |   +-- default.yaml\n|   |   +-- fast.yaml\n|   +-- data/\n|   |   +-- bnci2014_001.yaml\n|   |   +-- bnci2014_004.yaml\n|   +-- model/\n|       +-- spdnet.yaml\n|       +-- tsmnet.yaml\n|       +-- eegspdnet.yaml\n+-- benchmark.py              # Main script\n```\nYou would then run experiments like:\n\n```bash\n# Default configuration\npython benchmark.py\n\n# Override training parameters\npython benchmark.py training.max_epochs=200 training.learning_rate=1e-4\n\n# Use different dataset\npython benchmark.py data=bnci2014_004\n\n# Run multiple experiments with multirun\npython benchmark.py -m training.learning_rate=1e-3,1e-4,1e-5\n```\nExample Hydra main script:\n\n```python\nimport hydra\nfrom omegaconf import DictConfig\n\n@hydra.main(version_base=None, config_path=\"config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> float:\n    # Your benchmark code here\n    benchmark = SPDLearnBenchmark(cfg, X, y, meta, device)\n    results = benchmark.run_benchmark()\n    return results[\"mean_accuracy\"].mean()\n\nif __name__ == \"__main__\":\n    main()\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Configuration: Filter Bank Models\n\nFor filter bank models like TensorCSPNet, we need to use\nFilterBankMotorImagery paradigm. Here's how to configure this:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Filter Bank Configuration Example\")\nprint(\"=\" * 60)\n\n# Define filter bank configuration\nfilterbank_config = OmegaConf.create(\n    {\n        \"paradigm\": \"FilterBankMotorImagery\",\n        \"filters\": [\n            [4, 8],\n            [8, 12],\n            [12, 16],\n            [16, 20],\n            [20, 24],\n            [24, 28],\n            [28, 32],\n            [32, 36],\n            [36, 40],\n        ],\n        \"model\": {\n            \"name\": \"TensorCSPNet\",\n            \"n_patches\": 4,\n            \"n_freqs\": 9,\n            \"use_mlp\": False,\n            \"tcn_channels\": 16,\n        },\n    }\n)\n\nprint(\"Filter Bank Configuration:\")\nprint(OmegaConf.to_yaml(filterbank_config))\n\n# Note: To actually run TensorCSPNet, you would use:\n#\n# from moabb.paradigms import FilterBankMotorImagery\n#\n# fb_paradigm = FilterBankMotorImagery(\n#     n_classes=4,\n#     filters=filterbank_config.filters,\n# )\n# X_fb, labels_fb, meta_fb = fb_paradigm.get_data(...)\n#\n# Then create the model with:\n# model = create_model(\"TensorCSPNet\", n_chans=n_chans, n_outputs=n_outputs,\n#                      n_freqs=len(filterbank_config.filters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extending the Benchmark with New Models\n\nTo add a new model to this benchmark framework, follow these steps:\n\n1. **Define a configuration dataclass** for your model:\n\n```python\n@dataclass\nclass MyNewModelConfig(ModelConfig):\n    \"\"\"Configuration for MyNewModel.\"\"\"\n    name: str = \"MyNewModel\"\n    param1: int = 10\n    param2: float = 0.1\n```\n2. **Add the model to create_model()** factory function:\n\n```python\ndef create_model(model_name, n_chans, n_outputs, **kwargs):\n    # ... existing models ...\n    elif model_name == \"MyNewModel\":\n        return MyNewModel(\n            n_chans=n_chans,\n            n_outputs=n_outputs,\n            param1=kwargs.get(\"param1\", 10),\n            param2=kwargs.get(\"param2\", 0.1),\n        )\n```\n3. **Add training overrides** if your model needs special training:\n\n```python\ndef get_default_model_training_overrides():\n    return {\n        # ... existing models ...\n        \"MyNewModel\": {\n            \"learning_rate\": 5e-4,\n            \"max_epochs\": 120,\n            \"optimizer\": \"AdamW\",\n        },\n    }\n```\n4. **Add the model to the experiment configuration**:\n\n```python\nconfig = OmegaConf.structured(\n    ExperimentConfig(\n        models=[\"SPDNet\", \"TSMNet\", \"EEGSPDNet\", \"MyNewModel\"],\n        # ...\n    )\n)\n```\n5. **Provide model architecture parameters**:\n\n```python\nmodel_configs = {\n    # ... existing models ...\n    \"MyNewModel\": {\n        \"param1\": 20,\n        \"param2\": 0.05,\n    },\n}\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this tutorial, we demonstrated how to:\n\n1. **Define structured configurations** using dataclasses and OmegaConf\n2. **Create a model factory** for easy model instantiation\n3. **Build a benchmarking pipeline** with proper cross-validation\n4. **Use model-specific training configurations** for optimal performance\n5. **Implement early stopping and learning rate scheduling**\n6. **Evaluate multiple SPD Learn models** (SPDNet, TSMNet, EEGSPDNet)\n7. **Visualize and compare results** across models with per-class breakdown\n8. **Structure a Hydra-based project** for production use\n\nKey takeaways:\n\n- **Hydra and OmegaConf** provide powerful configuration management\n- **Structured configs** enable type safety and easy modification\n- **Model-specific training** is essential - TSMNet/EEGSPDNet need\n  lower learning rates (1e-4) and more epochs than SPDNet\n- **Proper cross-validation** is essential for reliable benchmarks\n- **SPD Learn models** :cite:p:`wilson2025deep` offer different trade-offs for EEG classification\n\nFor production benchmarks, consider:\n\n- Using more subjects and epochs\n- Implementing hyperparameter tuning with Optuna\n- Adding more evaluation metrics (F1, Cohen's kappa)\n- Using Hydra's multirun for hyperparameter sweeps\n- Logging with MLflow or Weights & Biases\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}