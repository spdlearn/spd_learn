{
  "cells": [
    {
      "id": "3f158f69",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install -q spd_learn moabb braindecode scikit-learn matplotlib\n\n# For GPU support (recommended for faster training)\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Building Blocks of SPD Neural Networks\n\nThis tutorial provides a comprehensive walkthrough of the fundamental building\nblocks used in SPD (Symmetric Positive Definite) neural networks. We explore\nthe mathematical foundations and geometric intuitions behind each layer.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to SPD Neural Networks\n\nTraditional neural networks operate on vectors in Euclidean space.\nHowever, many real-world signals (EEG, fMRI, radar) are better represented\nas covariance matrices, which lie on a curved Riemannian manifold.\n\nThe SPDNet :cite:p:`huang2017riemannian` pipeline transforms raw signals\nthrough a series of geometry-aware operations:\n\n```text\nRaw Signal --> Covariance --> BiMap --> ReEig --> LogEig --> Linear\n(n_chans, n_times)   (n, n)    (m, m)    (m, m)   (m*(m+1)/2)   (n_classes)\n```\nEach layer respects the geometric structure of SPD matrices, ensuring\nthat intermediate representations remain valid covariance matrices.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom spd_learn.modules import BiMap, CovLayer, LogEig, ReEig, Shrinkage, TraceNorm\n\n\n# For reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set default figure size\nplt.rcParams[\"figure.figsize\"] = (12, 4)\nplt.rcParams[\"figure.dpi\"] = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The SPD Manifold: A Quick Primer\n\nA Symmetric Positive Definite (SPD) matrix is a symmetric matrix with\nall positive eigenvalues. The space of all $n \\times n$ SPD matrices,\ndenoted $\\mathcal{S}^n_{++}$, forms a Riemannian manifold.\n\nKey properties:\n\n- **Not a vector space**: The sum of two SPD matrices is SPD, but\n  scalar multiplication can break positive definiteness\n- **Curved geometry**: Straight lines (geodesics) curve through the space\n- **Cone structure**: SPD matrices form an open convex cone\n\nWe can visualize 2x2 SPD matrices as ellipses:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_spd_as_ellipse(spd_matrix, ax, center=(0, 0), color=\"blue\", alpha=0.5):\n    \"\"\"Visualize a 2x2 SPD matrix as an ellipse.\"\"\"\n    eigvals, eigvecs = np.linalg.eigh(spd_matrix)\n    width = 2 * np.sqrt(eigvals[1])\n    height = 2 * np.sqrt(eigvals[0])\n    angle = np.degrees(np.arctan2(eigvecs[1, 1], eigvecs[0, 1]))\n\n    from matplotlib.patches import Ellipse\n\n    ellipse = Ellipse(\n        center,\n        width,\n        height,\n        angle=angle,\n        alpha=alpha,\n        facecolor=color,\n        edgecolor=\"black\",\n        linewidth=2,\n    )\n    ax.add_patch(ellipse)\n    return ellipse\n\n\n# Create some example SPD matrices using PyTorch\nspd_1 = torch.tensor([[2.0, 0.5], [0.5, 1.0]], dtype=torch.float64)\nspd_2 = torch.tensor([[1.0, -0.3], [-0.3, 1.5]], dtype=torch.float64)\nspd_3 = torch.tensor([[3.0, 1.0], [1.0, 2.0]], dtype=torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.set_aspect(\"equal\")\nax.grid(True, alpha=0.3)\nax.axhline(y=0, color=\"k\", linewidth=0.5)\nax.axvline(x=0, color=\"k\", linewidth=0.5)\n\n# Convert to numpy for plotting\nvisualize_spd_as_ellipse(spd_1.numpy(), ax, center=(-1.5, 1.5), color=\"#3498db\")\nvisualize_spd_as_ellipse(spd_2.numpy(), ax, center=(1.5, 1.5), color=\"#e74c3c\")\nvisualize_spd_as_ellipse(spd_3.numpy(), ax, center=(0, -1.5), color=\"#2ecc71\")\n\nax.set_title(\"SPD Matrices Visualized as Ellipses\", fontsize=14, fontweight=\"bold\")\nax.text(-1.5, 2.8, \"SPD 1\", ha=\"center\", fontsize=11)\nax.text(1.5, 2.8, \"SPD 2\", ha=\"center\", fontsize=11)\nax.text(0, -3.0, \"SPD 3\", ha=\"center\", fontsize=11)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 1: Covariance Layer (CovLayer)\n\nThe first step in an SPD network is to compute covariance matrices\nfrom raw multivariate signals. Given a signal $X \\in \\mathbb{R}^{C \\times T}$\nwith C channels and T time samples, the sample covariance is:\n\n\\begin{align}\\Sigma = \\frac{1}{T-1} X X^T\\end{align}\n\nThis maps the signal from Euclidean space to the SPD manifold.\n\n**Input shape**: ``(batch, n_channels, n_times)``\n\n**Output shape**: ``(batch, n_channels, n_channels)``\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate synthetic multivariate time series\nbatch_size = 4\nn_channels = 8\nn_times = 100\n\n# Create correlated signals\nraw_signals = torch.randn(batch_size, n_channels, n_times)\n\n# Add some structure (channel correlations)\nmixing_matrix = torch.randn(n_channels, n_channels)\nraw_signals = torch.einsum(\"ij,bjt->bit\", mixing_matrix, raw_signals)\n\nprint(\"Input (raw signals):\")\nprint(f\"  Shape: {raw_signals.shape}\")\nprint(f\"  Min: {raw_signals.min():.3f}, Max: {raw_signals.max():.3f}\")\n\n# Apply CovLayer\ncov_layer = CovLayer()\ncovariances = cov_layer(raw_signals)\n\nprint(\"\\nOutput (covariance matrices):\")\nprint(f\"  Shape: {covariances.shape}\")\nprint(f\"  Symmetric: {torch.allclose(covariances, covariances.transpose(-2, -1))}\")\n\n# Check positive definiteness\neigvals = torch.linalg.eigvalsh(covariances)\nprint(f\"  Min eigenvalue: {eigvals.min():.6f} (should be > 0 for SPD)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Covariance Computation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Plot raw signal (first sample, first 3 channels)\nax1 = axes[0]\ntime = np.arange(n_times)\nfor i in range(3):\n    ax1.plot(time, raw_signals[0, i, :].numpy(), label=f\"Channel {i + 1}\", alpha=0.8)\nax1.set_xlabel(\"Time samples\")\nax1.set_ylabel(\"Amplitude\")\nax1.set_title(\"Raw Signal (3 channels)\", fontsize=12, fontweight=\"bold\")\nax1.legend(fontsize=9)\nax1.grid(True, alpha=0.3)\n\n# Plot covariance matrix as heatmap\nax2 = axes[1]\ncov_np = covariances[0].numpy()\nim = ax2.imshow(cov_np, cmap=\"RdBu_r\", aspect=\"auto\")\nax2.set_title(\"Covariance Matrix\", fontsize=12, fontweight=\"bold\")\nax2.set_xlabel(\"Channel\")\nax2.set_ylabel(\"Channel\")\nplt.colorbar(im, ax=ax2, shrink=0.8)\n\n# Plot eigenvalue spectrum\nax3 = axes[2]\neigvals_np = eigvals[0].numpy()\nax3.bar(range(n_channels), sorted(eigvals_np, reverse=True), color=\"#3498db\", alpha=0.8)\nax3.set_xlabel(\"Eigenvalue index\")\nax3.set_ylabel(\"Eigenvalue\")\nax3.set_title(\"Eigenvalue Spectrum\", fontsize=12, fontweight=\"bold\")\nax3.set_yscale(\"log\")\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 2: Regularization (Shrinkage & TraceNorm)\n\nSample covariance matrices can be ill-conditioned, especially when\nthe number of samples is small relative to the number of channels.\nRegularization improves numerical stability.\n\n**Shrinkage (Ledoit-Wolf)** :cite:p:`ledoit2004well`:\n\n\\begin{align}\\Sigma_{reg} = (1 - \\alpha) \\Sigma + \\alpha \\cdot \\mu \\cdot I\\end{align}\n\nwhere $\\alpha$ is the shrinkage coefficient and $\\mu$ is\nthe average eigenvalue (trace / n).\n\n**Trace Normalization:**\n\n\\begin{align}\\Sigma_{norm} = \\frac{\\Sigma}{\\text{trace}(\\Sigma)} + \\epsilon I\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Apply shrinkage regularization\nshrinkage = Shrinkage(n_chans=n_channels, init_shrinkage=0.5, learnable=True)\ncov_shrunk = shrinkage(covariances)\n\n# Apply trace normalization\ntrace_norm = TraceNorm(epsilon=1e-5)\ncov_normalized = trace_norm(covariances)\n\nprint(\"Regularization effects:\")\nprint(f\"  Original condition number: {torch.linalg.cond(covariances[0]):.1f}\")\nprint(f\"  After shrinkage: {torch.linalg.cond(cov_shrunk[0].detach()):.1f}\")\nprint(f\"  After trace norm: {torch.linalg.cond(cov_normalized[0]):.1f}\")\n\n# Visualize eigenvalue spectra\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\neigvals_orig = torch.linalg.eigvalsh(covariances[0]).numpy()\neigvals_shrunk = torch.linalg.eigvalsh(cov_shrunk[0].detach()).numpy()\neigvals_norm = torch.linalg.eigvalsh(cov_normalized[0]).numpy()\n\nfor ax, eigv, title, color in zip(\n    axes,\n    [eigvals_orig, eigvals_shrunk, eigvals_norm],\n    [\"Original\", \"After Shrinkage\", \"After Trace Norm\"],\n    [\"#3498db\", \"#e74c3c\", \"#2ecc71\"],\n):\n    ax.bar(range(n_channels), sorted(eigv, reverse=True), color=color, alpha=0.8)\n    ax.set_xlabel(\"Eigenvalue index\")\n    ax.set_ylabel(\"Eigenvalue\")\n    ax.set_title(title, fontsize=12, fontweight=\"bold\")\n    ax.set_yscale(\"log\")\n    ax.grid(True, alpha=0.3)\n    ax.axhline(y=min(eigv), color=\"red\", linestyle=\"--\", alpha=0.5)\n    ax.text(\n        n_channels - 1,\n        min(eigv) * 1.5,\n        f\"min={min(eigv):.2e}\",\n        ha=\"right\",\n        fontsize=9,\n        color=\"red\",\n    )\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 3: BiMap Layer\n\nThe BiMap (Bilinear Mapping) layer performs dimensionality reduction\nwhile preserving the SPD structure. It applies a congruence transformation:\n\n\\begin{align}Y = W^T X W\\end{align}\n\nwhere $W \\in \\mathbb{R}^{n \\times m}$ is constrained to lie on the\n**Stiefel manifold** ($W^T W = I$).\n\n**Key properties:**\n\n- If X is SPD, then Y is also SPD\n- Reduces dimension from n x n to m x m\n- W is orthogonal, preventing information collapse\n\n**Input shape**: ``(batch, n, n)``\n\n**Output shape**: ``(batch, m, m)``\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create BiMap layer: reduce from 8x8 to 4x4\nbimap = BiMap(in_features=n_channels, out_features=4, parametrized=True)\n\n# Apply BiMap\ncov_reduced = bimap(covariances)\n\nprint(\"BiMap transformation:\")\nprint(f\"  Input shape: {covariances.shape}\")\nprint(f\"  Output shape: {cov_reduced.shape}\")\nprint(f\"  Weight matrix W shape: {bimap.weight.shape}\")\n\n# Verify orthogonality of W\nW = bimap.weight[0]  # Get first (only) weight matrix\nWtW = W.T @ W\nprint(\"\\n  W^T W (should be identity):\")\nprint(f\"  {WtW.detach().numpy().round(4)}\")\n\n# Verify output is still SPD\neigvals_reduced = torch.linalg.eigvalsh(cov_reduced)\nprint(\n    f\"\\n  Output eigenvalues (all > 0): {eigvals_reduced[0].detach().numpy().round(4)}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the BiMap Transformation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\n# Input covariance\nax1 = axes[0]\nim1 = ax1.imshow(covariances[0].numpy(), cmap=\"RdBu_r\", aspect=\"auto\")\nax1.set_title(\"Input (8x8)\", fontsize=12, fontweight=\"bold\")\nax1.set_xlabel(\"Channel\")\nax1.set_ylabel(\"Channel\")\nplt.colorbar(im1, ax=ax1, shrink=0.8)\n\n# Weight matrix W\nax2 = axes[1]\nW_np = bimap.weight[0].detach().numpy()\nim2 = ax2.imshow(W_np, cmap=\"RdBu_r\", aspect=\"auto\")\nax2.set_title(\"W (8x4, Stiefel)\", fontsize=12, fontweight=\"bold\")\nax2.set_xlabel(\"Output dim\")\nax2.set_ylabel(\"Input dim\")\nplt.colorbar(im2, ax=ax2, shrink=0.8)\n\n# W^T W (should be identity)\nax3 = axes[2]\nWtW_np = (W.T @ W).detach().numpy()\nim3 = ax3.imshow(WtW_np, cmap=\"RdBu_r\", aspect=\"auto\", vmin=-0.1, vmax=1.1)\nax3.set_title(r\"$W^T W$ (Identity)\", fontsize=12, fontweight=\"bold\")\nplt.colorbar(im3, ax=ax3, shrink=0.8)\n\n# Output covariance\nax4 = axes[3]\nim4 = ax4.imshow(cov_reduced[0].detach().numpy(), cmap=\"RdBu_r\", aspect=\"auto\")\nax4.set_title(\"Output (4x4)\", fontsize=12, fontweight=\"bold\")\nax4.set_xlabel(\"Channel\")\nax4.set_ylabel(\"Channel\")\nplt.colorbar(im4, ax=ax4, shrink=0.8)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 4: ReEig Layer (Rectified Eigenvalues)\n\nThe ReEig layer introduces non-linearity while preserving the SPD property.\nIt applies a ReLU-like function to eigenvalues:\n\n\\begin{align}\\text{ReEig}(X) = U \\max(\\Lambda, \\epsilon) U^T\\end{align}\n\nwhere $X = U \\Lambda U^T$ is the eigendecomposition and\n$\\epsilon$ is a small threshold.\n\n**Geometric interpretation:**\n\n- Clamps small eigenvalues to $\\epsilon$\n- Prevents matrices from becoming singular\n- Analogous to ReLU in standard neural networks\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create ReEig layer\nreeig = ReEig(threshold=1e-4)\n\n# Create a matrix with some small eigenvalues for demonstration\ndemo_eigvals = torch.tensor([2.0, 0.5, 0.01, 0.001])\ndemo_eigvecs = torch.linalg.qr(torch.randn(4, 4))[0]\ndemo_spd = demo_eigvecs @ torch.diag(demo_eigvals) @ demo_eigvecs.T\ndemo_spd = demo_spd.unsqueeze(0)  # Add batch dimension\n\n# Apply ReEig\ndemo_rectified = reeig(demo_spd)\n\n# Compare eigenvalues\neigvals_before = torch.linalg.eigvalsh(demo_spd[0])\neigvals_after = torch.linalg.eigvalsh(demo_rectified[0])\n\nprint(\"ReEig transformation:\")\nprint(\"  Threshold: 1e-4\")\nprint(f\"  Eigenvalues before: {eigvals_before.numpy().round(6)}\")\nprint(f\"  Eigenvalues after:  {eigvals_after.numpy().round(6)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the ReEig Non-linearity\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot the ReEig function\nax1 = axes[0]\nx = np.linspace(0, 2.5, 200)\nepsilon = 0.3  # Larger threshold for visualization\ny_reeig = np.maximum(x, epsilon)\n\nax1.plot(x, x, \"k--\", alpha=0.4, label=\"Identity (y=x)\", linewidth=2)\nax1.plot(x, y_reeig, \"b-\", linewidth=3, label=f\"ReEig (eps={epsilon})\")\nax1.fill_between(\n    [0, epsilon], [epsilon, epsilon], [0, 0], color=\"red\", alpha=0.15, label=\"Clamped\"\n)\nax1.axhline(y=epsilon, color=\"red\", linestyle=\"--\", alpha=0.5)\nax1.axvline(x=epsilon, color=\"red\", linestyle=\"--\", alpha=0.5)\n\n# Mark the demo eigenvalues (scaled for visualization)\nscale = 1.0\nfor i, (ev_before, ev_after) in enumerate(\n    zip(eigvals_before.numpy() * scale, eigvals_after.numpy() * scale)\n):\n    if ev_before < 2.5:\n        ax1.scatter(\n            [ev_before], [max(ev_before, epsilon)], s=100, zorder=5, edgecolors=\"black\"\n        )\n        if ev_before < epsilon:\n            ax1.plot(\n                [ev_before, ev_before],\n                [ev_before, epsilon],\n                \"r:\",\n                linewidth=2,\n                alpha=0.7,\n            )\n\nax1.set_xlim(-0.1, 2.5)\nax1.set_ylim(-0.1, 2.5)\nax1.set_xlabel(\"Input eigenvalue\", fontsize=12)\nax1.set_ylabel(\"Output eigenvalue\", fontsize=12)\nax1.set_title(\"ReEig: Eigenvalue Rectification\", fontsize=13, fontweight=\"bold\")\nax1.legend(loc=\"lower right\", fontsize=10)\nax1.grid(True, alpha=0.3)\nax1.set_aspect(\"equal\")\n\n# Bar plot of eigenvalues\nax2 = axes[1]\nx_pos = np.arange(4)\nwidth = 0.35\n\nbars1 = ax2.bar(\n    x_pos - width / 2,\n    eigvals_before.numpy(),\n    width,\n    label=\"Before ReEig\",\n    color=\"#3498db\",\n    alpha=0.8,\n)\nbars2 = ax2.bar(\n    x_pos + width / 2,\n    eigvals_after.numpy(),\n    width,\n    label=\"After ReEig\",\n    color=\"#e74c3c\",\n    alpha=0.8,\n)\nax2.axhline(y=1e-4, color=\"green\", linestyle=\"--\", linewidth=2, label=\"Threshold\")\nax2.set_xlabel(\"Eigenvalue index\", fontsize=12)\nax2.set_ylabel(\"Eigenvalue\", fontsize=12)\nax2.set_title(\"Eigenvalue Comparison\", fontsize=13, fontweight=\"bold\")\nax2.set_yscale(\"log\")\nax2.set_xticks(x_pos)\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Block 5: LogEig Layer (Logarithmic Map)\n\nThe LogEig layer maps SPD matrices to the tangent space at the identity\nby applying the matrix logarithm:\n\n\\begin{align}\\log(X) = U \\log(\\Lambda) U^T\\end{align}\n\nwhere $X = U \\Lambda U^T$ is the eigendecomposition.\n\n**Geometric interpretation:**\n\n- Projects from curved manifold to flat tangent space\n- In tangent space, standard Euclidean operations apply\n- Output is a symmetric matrix, which can be vectorized\n\n**Key insight**: The logarithm \"flattens\" the SPD manifold, allowing\nus to use standard linear classifiers.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create LogEig layer with vectorization\nlogeig = LogEig(upper=True)\n\n# Use our reduced covariances\nlog_matrices = logeig(cov_reduced.detach())\n\nprint(\"LogEig transformation:\")\nprint(f\"  Input shape: {cov_reduced.shape}\")\nprint(f\"  Output shape: {log_matrices.shape}\")\nprint(f\"  Output dimension formula: n*(n+1)/2 = 4*5/2 = {4 * 5 // 2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Matrix Logarithm\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Input SPD matrix\nax1 = axes[0]\nspd_input = cov_reduced[0].detach().numpy()\nim1 = ax1.imshow(spd_input, cmap=\"RdBu_r\", aspect=\"auto\")\nax1.set_title(\"Input SPD Matrix\", fontsize=12, fontweight=\"bold\")\nplt.colorbar(im1, ax=ax1, shrink=0.8)\n\n# Compute full matrix log (without vectorization) for visualization\nlogeig_full = LogEig(upper=False, flatten=False)\nlog_full = logeig_full(cov_reduced.detach())\n\n# Matrix logarithm\nax2 = axes[1]\nlog_matrix = log_full[0].numpy()\nim2 = ax2.imshow(log_matrix, cmap=\"RdBu_r\", aspect=\"auto\")\nax2.set_title(r\"$\\log(X)$ (tangent space)\", fontsize=12, fontweight=\"bold\")\nplt.colorbar(im2, ax=ax2, shrink=0.8)\n\n# Vectorized output\nax3 = axes[2]\nvec_output = log_matrices[0].numpy()\nax3.bar(range(len(vec_output)), vec_output, color=\"#2ecc71\", alpha=0.8)\nax3.set_xlabel(\"Vector index\", fontsize=11)\nax3.set_ylabel(\"Value\", fontsize=11)\nax3.set_title(\"Vectorized Output (upper triangular)\", fontsize=12, fontweight=\"bold\")\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete SPDNet Pipeline\n\nNow let's put it all together and trace the shape transformations\nthrough a complete SPDNet pipeline.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SimpleSPDNet(nn.Module):\n    \"\"\"A simple SPD network for demonstration.\"\"\"\n\n    def __init__(self, n_channels, n_classes):\n        super().__init__()\n\n        # Layer 1: Covariance computation\n        self.cov = CovLayer()\n\n        # Layer 2: Regularization\n        self.shrinkage = Shrinkage(n_chans=n_channels, init_shrinkage=0.1)\n\n        # Layer 3: BiMap (reduce dimension)\n        self.bimap1 = BiMap(in_features=n_channels, out_features=n_channels // 2)\n\n        # Layer 4: ReEig (non-linearity)\n        self.reeig1 = ReEig()\n\n        # Layer 5: Another BiMap\n        self.bimap2 = BiMap(in_features=n_channels // 2, out_features=n_channels // 4)\n\n        # Layer 6: Another ReEig\n        self.reeig2 = ReEig()\n\n        # Layer 7: LogEig (project to tangent space)\n        self.logeig = LogEig(upper=True)\n\n        # Layer 8: Linear classifier\n        tangent_dim = (n_channels // 4) * (n_channels // 4 + 1) // 2\n        self.classifier = nn.Linear(tangent_dim, n_classes)\n\n    def forward(self, x, return_intermediates=False):\n        intermediates = {}\n\n        # Raw signal -> Covariance\n        x = self.cov(x)\n        intermediates[\"cov\"] = x.clone()\n\n        # Shrinkage regularization\n        x = self.shrinkage(x)\n        intermediates[\"shrinkage\"] = x.clone()\n\n        # BiMap + ReEig block 1\n        x = self.bimap1(x)\n        intermediates[\"bimap1\"] = x.clone()\n        x = self.reeig1(x)\n        intermediates[\"reeig1\"] = x.clone()\n\n        # BiMap + ReEig block 2\n        x = self.bimap2(x)\n        intermediates[\"bimap2\"] = x.clone()\n        x = self.reeig2(x)\n        intermediates[\"reeig2\"] = x.clone()\n\n        # LogEig (to tangent space)\n        x = self.logeig(x)\n        intermediates[\"logeig\"] = x.clone()\n\n        # Linear classifier\n        x = self.classifier(x)\n        intermediates[\"output\"] = x.clone()\n\n        if return_intermediates:\n            return x, intermediates\n        return x\n\n\n# Create and use the network\nn_channels = 16\nn_classes = 4\nmodel = SimpleSPDNet(n_channels=n_channels, n_classes=n_classes)\n\n# Generate input\nbatch_size = 8\nn_times = 200\nraw_input = torch.randn(batch_size, n_channels, n_times)\n\n# Forward pass with intermediates\noutput, intermediates = model(raw_input, return_intermediates=True)\n\n# Print shape transformations\nprint(\"Shape Transformations Through SPDNet\")\nprint(\"=\" * 50)\nprint(f\"Input (raw signal):     {raw_input.shape}\")\nprint(f\"After CovLayer:         {intermediates['cov'].shape}\")\nprint(f\"After Shrinkage:        {intermediates['shrinkage'].shape}\")\nprint(f\"After BiMap1 (16->8):   {intermediates['bimap1'].shape}\")\nprint(f\"After ReEig1:           {intermediates['reeig1'].shape}\")\nprint(f\"After BiMap2 (8->4):    {intermediates['bimap2'].shape}\")\nprint(f\"After ReEig2:           {intermediates['reeig2'].shape}\")\nprint(f\"After LogEig:           {intermediates['logeig'].shape}\")\nprint(f\"Output (logits):        {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Full Pipeline\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\n# Row 1: Matrix representations\nax1 = axes[0, 0]\nax1.plot(raw_input[0, :3, :].T.numpy(), alpha=0.7)\nax1.set_title(\"Raw Signal\", fontsize=11, fontweight=\"bold\")\nax1.set_xlabel(\"Time\")\nax1.set_ylabel(\"Amplitude\")\n\nax2 = axes[0, 1]\nim2 = ax2.imshow(intermediates[\"cov\"][0].detach().numpy(), cmap=\"RdBu_r\", aspect=\"auto\")\nax2.set_title(\"Covariance (16x16)\", fontsize=11, fontweight=\"bold\")\n\nax3 = axes[0, 2]\nim3 = ax3.imshow(\n    intermediates[\"bimap1\"][0].detach().numpy(), cmap=\"RdBu_r\", aspect=\"auto\"\n)\nax3.set_title(\"After BiMap1 (8x8)\", fontsize=11, fontweight=\"bold\")\n\nax4 = axes[0, 3]\nim4 = ax4.imshow(\n    intermediates[\"bimap2\"][0].detach().numpy(), cmap=\"RdBu_r\", aspect=\"auto\"\n)\nax4.set_title(\"After BiMap2 (4x4)\", fontsize=11, fontweight=\"bold\")\n\n# Row 2: Eigenvalue spectra and output\nax5 = axes[1, 0]\neigvals_cov = torch.linalg.eigvalsh(intermediates[\"cov\"][0]).numpy()\nax5.bar(range(len(eigvals_cov)), sorted(eigvals_cov, reverse=True), color=\"#3498db\")\nax5.set_title(\"Cov Eigenvalues\", fontsize=11, fontweight=\"bold\")\nax5.set_yscale(\"log\")\n\nax6 = axes[1, 1]\neigvals_bimap1 = torch.linalg.eigvalsh(intermediates[\"reeig1\"][0]).detach().numpy()\nax6.bar(\n    range(len(eigvals_bimap1)), sorted(eigvals_bimap1, reverse=True), color=\"#e74c3c\"\n)\nax6.set_title(\"After ReEig1\", fontsize=11, fontweight=\"bold\")\nax6.set_yscale(\"log\")\n\nax7 = axes[1, 2]\neigvals_bimap2 = torch.linalg.eigvalsh(intermediates[\"reeig2\"][0]).detach().numpy()\nax7.bar(\n    range(len(eigvals_bimap2)), sorted(eigvals_bimap2, reverse=True), color=\"#2ecc71\"\n)\nax7.set_title(\"After ReEig2\", fontsize=11, fontweight=\"bold\")\nax7.set_yscale(\"log\")\n\nax8 = axes[1, 3]\nlogeig_vec = intermediates[\"logeig\"][0].detach().numpy()\nax8.bar(range(len(logeig_vec)), logeig_vec, color=\"#9b59b6\")\nax8.set_title(\"LogEig (tangent space)\", fontsize=11, fontweight=\"bold\")\nax8.set_xlabel(\"Dimension\")\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Composing Custom Architectures\n\nThe modular design of SPD Learn allows flexible architecture composition.\nHere are some common patterns:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DeepSPDNet(nn.Module):\n    \"\"\"Deeper SPD network with multiple BiMap+ReEig blocks.\"\"\"\n\n    def __init__(self, n_channels, hidden_dims, n_classes):\n        super().__init__()\n\n        self.cov = CovLayer()\n        self.shrinkage = Shrinkage(n_chans=n_channels)\n\n        # Build BiMap+ReEig blocks\n        self.blocks = nn.ModuleList()\n        dims = [n_channels] + list(hidden_dims)\n        for i in range(len(hidden_dims)):\n            self.blocks.append(\n                nn.Sequential(\n                    BiMap(in_features=dims[i], out_features=dims[i + 1]),\n                    ReEig(),\n                )\n            )\n\n        self.logeig = LogEig(upper=True)\n        final_dim = hidden_dims[-1] * (hidden_dims[-1] + 1) // 2\n        self.classifier = nn.Linear(final_dim, n_classes)\n\n    def forward(self, x):\n        x = self.cov(x)\n        x = self.shrinkage(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.logeig(x)\n        return self.classifier(x)\n\n\n# Create a deeper network\ndeep_model = DeepSPDNet(\n    n_channels=22,  # EEG with 22 channels\n    hidden_dims=[16, 8, 4],  # Progressive reduction\n    n_classes=4,  # 4-class motor imagery\n)\n\n# Count parameters\nn_params = sum(p.numel() for p in deep_model.parameters())\nprint(\"\\nDeepSPDNet Architecture:\")\nprint(\"  Input channels: 22\")\nprint(\"  Hidden dimensions: 22 -> 16 -> 8 -> 4\")\nprint(f\"  Output features: {4 * 5 // 2} (vectorized)\")\nprint(f\"  Total parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: The SPD Learning Pipeline\n\nThis tutorial covered the fundamental building blocks of SPD neural networks:\n\n1. **CovLayer**: Transforms raw signals to SPD covariance matrices\n\n2. **Shrinkage/TraceNorm**: Regularizes covariance matrices for stability\n\n3. **BiMap**: Performs geometry-preserving dimensionality reduction\n   using orthogonal projections\n\n4. **ReEig**: Introduces non-linearity by rectifying eigenvalues,\n   analogous to ReLU in standard networks\n\n5. **LogEig**: Maps SPD matrices to tangent space where Euclidean\n   operations apply, enabling standard linear classification\n\n**Key insights**:\n\n- All operations preserve SPD structure until LogEig\n- BiMap uses Stiefel manifold constraints for stable training\n  :cite:p:`brooks2019riemannian`\n- The pipeline gradually reduces dimensionality while preserving information\n- LogEig \"flattens\" the manifold for classification\n\nFor more advanced architectures, see the EEGSPDNet and TSMNet tutorials.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}